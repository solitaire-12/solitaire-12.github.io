{"meta":{"title":"织秋个人博客","subtitle":"副标题","description":"座右铭","author":"solitaire-12","url":"https://solitaire-12.github.io","root":"/"},"pages":[{"title":"","date":"2024-03-14T09:19:27.963Z","updated":"2024-03-14T09:19:27.963Z","comments":true,"path":"404.html","permalink":"https://solitaire-12.github.io/404.html","excerpt":"","text":""},{"title":"about","date":"2024-03-19T04:54:36.000Z","updated":"2024-06-14T04:51:37.794Z","comments":true,"path":"about/index.html","permalink":"https://solitaire-12.github.io/about/index.html","excerpt":"","text":"2024-06-14 补充 数据结构-图 2024-03-25 上传笔记 Redis解决数学公式重复显示问题 2024-03-19 切换主题 Volantis 2024-03-18 同步笔记 服务器环境配置笔记JVMJava8 2024-03-15 切换主题 Async 2024-03-14 博客建立 简单说一下为啥建立这个博客吧。原先用gitee存放自己的笔记，每次写完之后push一下就好了，也算方便。随着笔记越来越多，想要在这种方式下找到自己想要的内容，简直噩梦。也尝试过Notion，不过Notion会把鸡蛋放一个篮子里。思来想去，搞了这个博客。","author":"织秋"},{"title":"frineds","date":"2024-03-19T04:55:03.000Z","updated":"2024-03-19T04:55:03.563Z","comments":true,"path":"frineds/index.html","permalink":"https://solitaire-12.github.io/frineds/index.html","excerpt":"","text":""},{"title":"分类","date":"2024-03-14T07:40:18.000Z","updated":"2024-03-14T08:58:24.939Z","comments":true,"path":"categories/index.html","permalink":"https://solitaire-12.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2024-03-14T08:59:43.000Z","updated":"2024-03-14T09:00:13.560Z","comments":true,"path":"links/index.html","permalink":"https://solitaire-12.github.io/links/index.html","excerpt":"","text":""},{"title":"tags","date":"2024-03-14T08:59:06.000Z","updated":"2024-03-14T08:59:23.315Z","comments":true,"path":"tags/index.html","permalink":"https://solitaire-12.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Nginx|配置解析","slug":"nginx/nginx","date":"2024-05-22T01:10:00.000Z","updated":"2024-05-22T02:46:08.726Z","comments":true,"path":"2024/05/22/nginx/nginx/","permalink":"https://solitaire-12.github.io/2024/05/22/nginx/nginx/","excerpt":"","text":"核心配置指令 配置目录 Nginx配置文件在conf目录下，其中默认目录结构如下： fastcgi.conf：为了规范配置指令 SCRIPT_FILENAME 的用法，引入 FastCGI 变量传递配置； fastcgi_params：Nginx 在配置 FastCGI 代理服务时会根据 fastcgi_params 文件的配置向 FastCGI 服务器传递变量，该配置文件现已由 fastcgi.conf 代替； koi-utf | koi-win | win-utf：koi8-r编码转换的映射文件。 mime.types：MIME类型映射表，Nginx 会根据服务端文件后缀名在映射关系中获取所属文件类型，将文件类型添加到 HTTP 消息头字段&quot;Content-Type&quot;中。 nginx.conf：Nginx默认的配置文件。 scgi_params：Nginx 在配置 SCGI 代理服务时会根据 scgi_params 文件的配置向 SCGI 服务器传递变量； uwsgi_params：Nginx 在配置 uWSGI 代理服务时会根据 uwsgi_params 文件的配置向 uWSGI 服务器传递变量； Linux环境下的conf目录中，存在 .default拓展名的文件，是Nginx配置文件的样例文件。 配置文件结构 域名称 域类型 说明 main 全局域 根据别指令区域。该区域的配置指令是全局有效的，该指令为隐性显示。nginx.conf 的整个文件内容都写在该指令域中 events 指令域 事件驱动相关的配置指令域 http 指令域 http核心配置指令域，包含客户端完整 HTTP 请求过程中每个过程的处理方法的配置指令 upstream 指令域 用于定义被代理服务器组的指令区域，也称&quot;上游服务器&quot; server 指令域 Nginx 用来定义服务 IP、绑定端口及服务相关的指令区域 location 指令域 对用户 URI 进行访问路由处理的指令区域 stream 指令域 Nginx 对 TCP 协议实现代理的配置指令域 types 指令域 定义被请求文件扩展名与 MIME 类型映射表的指令区域 if 指令域 按照选择条件判断为真时使用的配置指令域 1234567891011121314151617181920212223242526272829303132# nginx.conf 默认配置文件#user nobody;worker_processes 1; # 只启动一个工作进程events &#123; worker_connections 1024; # 每个工作进程的最大连接为1024&#125;http &#123; include mime.types; # 引入MIME类型映射表文件 default_type application/octet-stream; # 全局默认映射类型为application/octet-stream #log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; # &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; # &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; #access_log logs/access.log main; sendfile on; # 启用零复制机制 keepalive_timeout 65; # 保持连接超时时间为65s server &#123; listen 80; # 监听80端口的网络连接请求 server_name localhost; # 虚拟主机名为localhost #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; Nginx 教程 (w3ccoo.com)","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://solitaire-12.github.io/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://solitaire-12.github.io/tags/Nginx/"}]},{"title":"MySQL|进阶","slug":"data/mysql-进阶","date":"2024-04-22T01:00:00.000Z","updated":"2024-05-10T08:52:06.056Z","comments":true,"path":"2024/04/22/data/mysql-进阶/","permalink":"https://solitaire-12.github.io/2024/04/22/data/mysql-%E8%BF%9B%E9%98%B6/","excerpt":"","text":"⭐索引 索引是一个单独的、存储在磁盘上的数据库结构，包含着对数据表里所有记录的引用指针。 索引是在存储引擎中实现的，因此，每种存储引擎的索引都不一定完全相同，并且每种存储引擎也不一定支持所有索引类型。根据存储引擎定义每个表的最大索引数和最大索引长度。所有存储引擎支持每个表至少16个索引，总索引长度至少为256字节 分类 普通索引和唯一索引 index(column_name)：普通索引是MySQL中的基本索引类型，允许在定义索引的列中插入重复值和空值。 unique index uniquIdx(column_name)：唯一索引要求索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。主键索引是一种特殊的唯一索引，不允许有空值。如果是组合索引，则列值的组合必须唯一。 单列索引和组合索引 index(c1)：单列索引即一个索引只包含单个列，一个表可以有多个单列索引。 index(c1,c2...)：组合索引是指在表的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用。使用组合索引时遵循最左前缀集合。利用索引中最左边的列集来匹配行，这样的列集称为最左前缀。例如，由id、name和age 3个字段构成的索引，索引行中按id、name、age的顺序存放，索引可以搜索（id,name, age）、（id, name）或者id字段组合。如果列不构成索引最左面的前缀，那么MySQL不能使用局部索引，如（age）或者（name,age）组合则不能使用索引查询。 全文索引 fulltext index(xx)全文索引类型为fulltext，在定义索引的列上支持值的全文查找，允许在这些索引列中插入重复值和空值。全文索引可以在char、varchar或者text类型的列上创建。MySQL中只有MyISAM存储引擎支持全文索引。索引总是对整个列进行，不支持局部（前缀）索引。 空间索引 spatial index(xx)：空间索引是对空间数据类型的字段建立的索引，MySQL中的空间数据类型有4种，分别是geometry、point、linestring和polygon。MySQL使用spatial关键字进行扩展，使得能够用创建正规索引类似的语法创建空间索引。创建空间索引的列，必须将其声明为not null，空间索引只能在存储引擎为MyISAM的表中创建。 🌟索引的设计原则： 索引并非越多越好。一张表中如果有大量的索引，不仅占用磁盘空间，还会影响insert / delete / update等语句的性能，因为表数据改动时，索引也会进行调整和更新。 避免对经常更新的表设置过多的索引，且索引中的列尽可能少。应该经常用于查询的字段创建索引，但要避免添加不必要的字段。 数据量小的表最好不要使用索引。 在条件表达式中经常用到的列（且该列不同值较多）上添加索引。 当唯一性是某种数据本身的特征时，指定唯一索引。 在频繁进行排序或分组（即进行group by或order by操作）的列上建立索引。如果待排序的列有多个，可以在这些列上建立组合索引。 创建索引 1create table table_name [col_name data_type] [unique | fulltext | spatial] [index | key] [index_name] (col_name [length]) [ASC | DESC] [unique | fulltext | spatial]，可选参数，分别表示唯一索引、全文索引和空间索引 [index | key] 两者作用相同，用来创建索引。 [index_name] ，索引名称，可选参数 (col_name [length]) ，MySQL默认col_name为索引值；length为索引长度，可选参数。 [ASC | DESC]指定升序或者降序的索引值存储。 已有字段添加索引 12alter table table_name add [unique|fulltext|spatial] [index|key][index_name] (col_name[length],…) [asc | desc] Table表示创建索引的表。 Non_unique表示索引非唯一。1代表是非唯一索引，0代表唯一索引。 Key_name表示索引的名称。 Seq_in_index表示该字段在索引中的位置。单列索引该值为1，组合索引为每个字段在索引定义中的顺序。 Column_name表示定义索引的列字段。 Sub_part表示索引的长度。 Null表示该字段是否能为空值。 Index_type表示索引类型。 1234# create 方式添加索引create [unique|fulltext|spatial] index [index_name] on table_name (col_name[length],..) [asc | desc]## 例子create index TextIdx on tb1(b); 删除索引 1234# 1alter table table_name drop index index_name;# 2drop index index_name on table_name; ⚠️删除表中的列时，如果要删除的列为索引的组成部分，则该列也会从索引中删除。如果组成索引的所有列都被删除，则整个索引将被删除。 🌟explain分析是否命中索引 1explain &lt;sql语句&gt; select_type表示查询类型。SIMPLE代表简单的SELECT查询。其他情况有PRIMARY、UNION、SUBQUERY。 table表示数据库读取的数据表的名字，数据库读取的数据表的名字。 type表示本数据表与其他数据表之间的关联关系，可能的取值有system、const、eq_ref、ref、range、index和All。 possible_keys表示MySQL在搜索数据记录时可选用的各个索引。 key表示MySQL实际选用的索引 key_len行给出索引按字节计算的长度，key_len数值越小，表示越快。 ref行给出了关联关系中另一个数据表里的数据列名 rows行是MySQL在执行这个查询时预计会从这个数据表里读出的数据行的个数。 Extra行提供了与关联操作有关的信息。 存储过程 todo 视图 todo 触发器 todo 权限 1234# 登录MySQL服务器mysql [-h [主机ip，默认localhost]] -u&lt;username&gt; -p[password] [-P[post，默认3306]] [database_name] [-e &quot;sql语句&quot;]# 退出exit 用户管理 创建普通用户 create user语句创建新用户 1create user &#x27;userName&#x27;@&#x27;localhost&#x27; [identified by [PASSWORD] &#x27;password&#x27;] | [identified with auth_plugin [as &#x27;auth_string&#x27;]] identified by 表示用来设置用户的密码；PASSWORD表示使用哈希值设置密码，该参数可选；password表示用户登录时使用的普通明文密码。不需要密码就不用这段。 identified with语句为用户指定一个身份验证插件；auth_plugin是插件的名称，插件的名称可以是一个带单引号的字符串或者带双引号的字符串；auth_string是可选的字符串参数，该参数将传递给身份验证插件，由该插件解释该参数的意义。 直接操作user表 不推荐使用，因为user表中有很多没有默认值的字段，缺失无法添加。 1insert into user(host,user,authentication_string) valuses(&#x27;host&#x27;,&#x27;userName&#x27;,MD5(&#x27;passward&#x27;)); 删除普通用户 drop user语句删除用户 1234# 删除某个用户drop user &#x27;username&#x27;@&#x27;host&#x27;;# 删除来自所有授权表的帐户权限记录drop user; 直接操作user表 1delete from user where host=&#x27;localhost&#x27; and user=&#x27;customer1&#x27;; 修改密码 root用户修改自己的密码 1update user set authentication_string = MD5(&#x27;password&#x27;) where user = &#x27;root&#x27; and host=&#x27;localhost&#x27;; root用户修改普通用户密码 12set password for &#x27;username&#x27;@&#x27;localhost&#x27; = &#x27;password&#x27;;update user set authentication_string=MD5(&#x27;password&#x27;) where user=&#x27;userName&#x27; and host=&#x27;hostName&#x27;; 权限管理 权限 user表中对应的列 权限的范围 create create_priv 数据库、表、索引 drop drop_priv 数据库、表、索引 grant option grant_priv 数据库、表、存储过程 references references_priv 数据库、表 event event_priv 数据库 alter alter_priv 数据库 delete delete_priv 表 index index_priv 表 insert insert_priv 表 select select_priv 表、列 update update_priv 表、列 create temporary tables create_tmp_table_priv 表 lock tables lock_tables_priv 表 trigger tigger_priv 表 create view create_view_priv 视图 show view show_view_priv 视图 alter routine alter_routine_priv 存储过程、函数 create routine create_routine_priv 存储过程、函数 execute execute_priv 存储过程、函数 file file_priv 访问服务器上的文件 create tablespace create_tablespace_priv 服务器管理 create uesr create_user_priv 服务器管理 process process_priv 存储过程、函数 reload reload_priv 访问服务器上的文件 replication client repl_client_priv 服务器管理 replication slave repl_slave_priv 服务器管理 show databases show_db_priv 服务器管理 shutdown shutdown_priv 服务器管理 super super_priv 服务器管理 reload命令告诉服务器将授权表重新读入内存； flush-hosts , flush-logs , flush-privileges , flush-status , flush-tables , flush-threads , refresh , reload flush-privileges是reload的同义词；refresh命令清空所有表并关闭/打开记录文件；其他flush-xxx命令执行类似refresh的功能，但是范围更有限，并且在某些情况下可能更好用。例如，如果只是想清空记录文件，flush-logs是比refresh更好的选择。 shutdown命令关掉服务器。只能从MySQLadmin发出命令。 process权限拥有processlist命令显示在服务器内执行的线程的信息（其他账户相关的客户端执行的语句）。kill命令杀死服务器线程。用户总是能显示或杀死自己的线程，但是需要PROCESS权限来显示或杀死其他用户和SUPER权限启动的线程。 spuer权限拥有kill命令能用来终止其他用户或更改服务器的操作方式。 授权 授权权限分级： 全局层级 全局权限适用于一个给定服务器中的所有数据库。这些权限存储在mysql.user表中。grant all on *.*和revoke all on *.*只授予和撤销全局权限。 数据库层级 数据库权限适用于一个给定数据库中的所有目标。这些权限存储在mysql.db和mysql.host表中。grant all on db_name.和revoke all on db_name.*只授予和撤销数据库权限。 表层级 表权限适用于一个给定表中的所有列。这些权限存储在mysql.talbes_priv表中。grant all on db_name.tbl_name和revoke all on db_name.tbl_name只授予和撤销表权限。 列层级 列权限适用于一个给定表中的单一列。这些权限存储在mysql.columns_priv表中。当使用revoke时，必须指定与被授权列相同的列。 子程序层级 create routine、alter routine、execute、grant权限适用于已存储的子程序。这些权限可以被授予为全局层级和数据库层级。而且，除了create routine外，这些权限可以被授予子程序层级，并存储在mysql.procs_priv表中。 在MySQL中，必须是拥有grant权限的用户才可以进行grant。要使用grant或revoke，必须拥有grant option权限，并且必须用于正在授权或撤销的权限。grant的语法如下： 1grant priv_type [(coloumns)] [,priv_type [(coloums)]] ... on [object_type] table1,table2... to user [with grant option] object_type = table | function | procedure priv_type表示权限类型；[(coloumns)]表示权限作用于那些列上，不指定该参数，表示用于整个表。 object_type指定授权作用的对象类型包括table（表）、function（函数）和procedure（存储过程）。当旧版本升级到高版本，要使用该子句，升级权限表。 user表示用户帐户，有用户名和主机名构成，形式是'username'@'hostname' identified by表示设置密码 with关键字后可以跟随一个或多个with_option参数。1. grant option：被授权的用户可以将这些权限赋予别的用户2. max_queries_per_hour count：设置每个小时可以执行count次查询。3. max_updates_per_hour count：设置每小时可执行count从更新。4. max_connections_per_hour count：设置每小时可建立count个连接。5. max_user_connections count：设置单个用户可以同时建立count个连接。 取消授权 1revoke all privileges, grant option from &#x27;user&#x27;@&#x27;host&#x27; [, &#x27;user&#x27;@&#x27;host&#x27; ...] 查看权限 1show grants for &#x27;user&#x27;@&#x27;host&#x27;; 🌟性能优化 原则是减少系统的瓶颈，减少资源的占用，增加系统的反应速度。1. 通过优化文件系统，提高磁盘I/O的读写速度。2. 通过优化操作系统调度策略，提高MySQL在高负荷情况下的负载能力。3. 优化表结构、索引、查询语句等查询响应更快。 性能参数 1show status like &#x27;value&#x27;; value是要查询的参数值，一些常用的性能参数如下： connections：连接MySQL服务器的次数。 uptime：MySQL服务器的上线时间。 slow_queries：慢查询的次数。 com_select：查询操作的次数。 com_insert：插入操作的次数。 com_update：更新操作的次数。 com_delete：删除操作的次数。 分析查询语句 1explian select select_options 📌分析结果列解析： id：select查询的系列号，表示查询中执行select子句或者操作表中的顺序。id的数值越大，优先级越高，越先执行 select_type：表示语句的类型。1. SIMPLE：表示简单的查询，其中不包括连接查询和子查询。2. PRIMARY：表示主查询，或者最外层的查询语句。3. UNION：表示连接查询的第2个或者后面的查询语句。4. DEPENDENT UNION，连接查询中的第2个或后面的SELECT语句，取决于外面的查询。5. UNION RESULT，连接查询的结果。6. SUBQUERY，子查询中的第1个SELECT语句。7. DEPENDENT SUBQUERY，子查询中的第1个SELECT，取决于外面的查询。8. DERIVED，导出表的SELECT（FROM子句的子查询）。 table：表示涉及的表 type：表示连接类型。1. ALL：全表查询2. system：该表是仅一行的系统表。这是const连接类型的一个特例3. const：数据表最多只能匹配一行数据。出现在主键或者索引的场合4. eq_ref：当一个索引的所有部分都在查询中使用并且索引是UNIQUE或PRIMARY KEY时，即可使用这种类型5. ref：可以用于使用=或&lt;=&gt;操作符带索引的列6. ref_or_null：该连接类型如同ref，但是添加了MySQL可以专门搜索包含NULL值的行。在解决子查询中经常使用该连接类型的优化。7. index_merge：该连接类型表示使用了索引合并优化方法。在这种情况下，key列包含了使用的索引的清单，key_len包含了使用索引的最长关键元素。8. unique_subquery：该类型替换了下面形式的IN子查询的ref：value IN (SELECT primary_key FROM single_table WHERE some_expr)unique_subquery是一个索引查找函数，可以完全替换子查询，效率更高。9. index_subquery：该连接类型类似于unique_subquery，可以替换IN子查询，但只适合下列形式的子查询中的非唯一索引。value IN (SELECT key_column FROM single_table WHERE some_expr)10.range：只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引。key_len包含所使用索引的最长关键元素。当使用=、&lt;&gt;、&gt;、&gt;=、&lt;、&lt;=、IS NULL、&lt;=&gt;、BETWEEN或者IN操作符用常量比较关键字列时，类型为range。11. index：该连接类型与ALL相同，除了只扫描索引树。这通常比ALL快，因为索引文件通常比数据文件小。 📌优化索引 在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不会起作用。只有“%”不在第一个位置，索引才会起作用。 MySQL可以为多个字段创建索引。一个索引可以包括16个字段。对于多列索引，只有查询条件中使用了这些字段中的第1个字段时索引才会被使用。最左匹配原则 查询语句的查询条件中只有OR关键字，且OR前后的两个条件中的列都是索引时，查询中才使用索引；否则，查询将不使用索引。 避免使用子查询。子查询会导致MySQL多次扫描表 子查询虽然可以使查询语句很灵活，但执行效率不高。执行子查询时，MySQL需要为内层查询语句的查询结果建立一个临时表。然后外层查询语句从临时表中查询记录。查询完毕后，再撤销这些临时表。因此，子查询的速度会受到一定的影响。如果查询的数据量比较大，这种影响就会随之增大。在MySQL中，可以使用连接（JOIN）查询来替代子查询。 📌优化表 优化数据库结构 拆分水平宽表 添加中间表 合理添加冗余字段。虽然可以避开一些连接查询，但是在数据改动后会造成数据不一致问题，同时浪费里一定的磁盘空间。慎用！慎用！慎用！ 优化插入记录的速度 禁用索引 禁用唯一性检查 使用批量插入 使用load data infile批量导入 分析表 1ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name[,tbl_name]… LOCAL关键字是NO_WRITE_TO_BINLOG关键字的别名，二者都是执行过程不写入二进制日志，tbl_name为分析表的表名，可以有一个或多个。 使用ANALYZE TABLE分析表的过程中，数据库系统会自动对表加一个只读锁。在分析期间，只能读取表中的记录，不能更新和插入记录。ANALYZE TABLE语句能够分析InnoDB、BDB和MyISAM类型的表。 Table：表示分析的表名称 Op：表示执行的操作。analyze表示进行分析操作。 Msg_type：表示信息类型，其值通常是状态(status)，信息(info)，注意(note)，警告(warning)和错误(error)之一。 Msg_text：显示信息。 检查表 12 CHECK TABLE tbl_name [, tbl_name] ... [option] ... option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125; QUICK：不扫描行，不检查错误的连接 FAST：只检查没有被正确关闭的表 MEDIUM：：扫描行，以验证被删除的连接是有效的。也可以计算各行的关键字校验和，并使用计算出的校验和验证这一点 EXTENDED：对每行的所有关键字进行一个全面的关键字查找。这可以确保表是100%一致的，但是化的时间比较长。 CHANGED：只检查上次检查后被更改的表和没有被正确关闭的表 option只对MyISAM类型的表有效，对InnoDB类型的表无效。check table语句在执行过程中会给表上只读锁。 优化表 MySQL中使用OPTIMIZE TABLE语句来优化表。该语句对InnoDB和MyISAM类型的表都有效。但是，OPTILMIZE TABLE语句只能优化表中VARCHAR、BLOB或TEXT类型的字段，会给表上只读锁 1OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 📌优化MySQL服务器 优化服务器配件 配置较大的内存。通过增加系统的缓冲区容量使数据在内存停留的时间更长，以减少磁盘I/O。 配置高速磁盘，减少读盘的等待时间，提高响应速度。 合理分布磁盘I/O，把磁盘I/O分散在多个设备上，以减少资源竞争，提高并行操作能力。 配置多处理器。MySQL是多线程的数据库，多处理器可同时执行多线程。 优化MySQL参数 12# 查看MySQL服务器参数show variables; 内存缓存 MySQL的性能受内存的影响很大，因此需要合理地分配内存。 innodb_buffer_pool_size：InnoDB存储引擎的缓存池大小，建议设置为物理内存的70%~80%。 key_buffer_size：MyISAM存储引擎的缓存池大小，建议设置为物理内存的25%。 query_cache_size：查询缓存的大小，建议设置为物理内存的5%~10%。 线程缓存 MySQL使用线程处理客户端的请求，因此需要设置线程缓存的大小。 thread_cache_size：线程缓存的大小，建议设置为100~200。 max_connections：最大连接数，建议设置为500~1000。 日志 MySQL的性能受内存的影响很大，因此需要合理地分配内存。 log_show_queries：慢查询日志，可以记录查询时间超过设定阈值的SQL语句。建议设置为1，即开启慢查询日志。 binlog_cache_size：二进制日志缓存大小，建议设置为32M。 innodb_log_file_size：InnoDB日志文件的大小，建议设置为物理内存的10%。 可以通过MySQL配置文件my.cnf来调整MySQL参数。在修改之前，建议备份。 1234# 查找my.cnf 文件的位置mysql --help | grep cnf# 修改后重启生效service mysql resatrt 语句超时处理 123# 设置服务器语句超时的限制，可以通过设置系统变量max_execution_time来实现。set global max_execution_time=2000;set session max_execution_time=2000; 全局通用表空间 MySQL 8.0支持创建全局通用表空间，全局表空间可以被所有数据库的表共享，而且相比于独享表空间，手动创建共享表空间可以节约元数据方面的内存。可以在创建表的时候指定属于哪个表空间，也可以对已有表进行表空间修改，具体的信息可以查看官方文档。 12alter table t1 tablespace dxy;drop tablespace dxy;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/tags/MySQL/"}]},{"title":"MySQL|基础","slug":"data/mysql","date":"2024-03-25T08:20:00.000Z","updated":"2024-04-22T01:13:06.299Z","comments":true,"path":"2024/03/25/data/mysql/","permalink":"https://solitaire-12.github.io/2024/03/25/data/mysql/","excerpt":"","text":"安装 Linux环境请点博客跳转 Windows安装mysql详细步骤（通俗易懂，简单上手）-CSDN博客用下别人的轮子 数据库的基础操作 123456# 查看所有数据库show databases;# 创建数据库create database database_name;# 删除数据库drop database database_name; ⚠️使用drop database命令时，MySQL并不会给出任何提醒、确认信息。该命令执行后，所有数据表和数据也将删除，且操作不可逆。请慎重使用。 数据表的基本操作 🌟想要操作数据表，需要使用use database_name指定操作在哪个数据库中执行。如果没有选择数据库，就会抛出No database selected的错误。 创建表 1234567# 创建数据表create table &lt;table_name&gt;( column_1 data_type, ...);# 查看数据表show tables; 🌟约束 ⭐主键约束 主键，又称主码，是表中一列或多列的组合。主键约束（Primary Key Constraint）要求主键列的数据唯一，并且不允许为空。 12345# 单字段主键字段名 数据类 primary key [默认值]# 多字段主键[constraint &lt;约束名&gt;] primary key [字段名1,字段名2...] 外键约束 外键，用来在两个表的数据之间的建立连接，可以是一列或者是多列，以确保引用数据的完整性。定义外键之后，不允许删除在另一张表中具有关联关系的行。 主表（父表）：对于两个具有关联关系的表而言，相关联字段中主键所在的那个表即是主表。 从表（子表）：对于两个具有关联关系的表而言，相关联字段中外键所在的那个表即是从表。 优点：保证数据的完整性和一致性，级联操作方便，数据的一致性交给数据库，代码量小。 1constraint &lt;外键约束名&gt; foreign key(外键名) references &lt;主表&gt; 主键列1 [,主键列2...] 提示 子表的外键必须关联父表的主键，且关联字段的数据类型必须匹配，如果类型不一样，则创建子表时，就会出现错误“ERROR 1005(HY000): Can't create table 'database.tablename'(errno: 150)”。 ⚠️ 【阿里规范-强制】不得使用外键与级联，一切外键概念在应用层解决。 原因：外键约束每次做delete或者update都必须考虑外键约束，在开发测试时极为不便。 非空约束 非空约束，指字段的值不能为空。用户添加数据未指定值，数据库系统报错。 1字段名 数据类型 not null 唯一性约束 唯一性约束，要求该列唯一，允许为空，但只能出现一个空值。 1234# 方式一字段名 数据类型 unique# 方式二[constraint &lt;约束名&gt;] unique(&lt;字段名&gt;) 默认值约束 默认值约束，指给某列设定默认值。当插入一条记录没有这个字段值时，系统自动赋值。 1字段名 数据类型 default 默认值 自增 在数据库应用中，经常希望在每次插入新记录时，系统自动生成字段的主键值。可以通过为表主键添加auto_increment关键字来实现。默认的，在MySQL中auto_increment的初始值是1，每新增一条记录，字段值自动加1。一个表只能有一个字段使用auto_increment约束，且该字段必须为主键的一部分。auto_increment约束的字段可以是任何整数类型（tinyint、smallint、int、bigint等）。 1字段名 数据类型 auto_increment ⚠️自增持久化： 在MySQL 8.0之前，自增主键auto_increment的值自增主键auto_increment的值如果大于max(primary key)+1，在MySQL重启后，会重置auto_increment=max(primary key)+1。 原理是，在MySQL 5.7系统中，对于自增主键的分配规则，是由InnoDB数据字典内部一个计数器来决定的，而该计数器只在内存中维护，并不会持久化到磁盘中。当数据库重启时，该计数器会通过下面这种方式初始化。 到了MySQL 8.0之后，将自增主键的计数器持久化到重做日志中。每次计数器发生改变，都会将其写入重做日志中。如果数据库重启，InnoDB会根据重做日志中的信息来初始化计数器的内存值。为了尽量减小对系统性能的影响，计数器写入到重做日志时并不会马上刷新数据库系统。 查看表结构 desc / describe语句可以查看表的字段信息，其中包括字段名、字段数据类型、是否为主键、是否有默认值等 123desc (describe) 表名;# 查看表详细结构；\\G 参数会使结果更加直观。show create table &lt;表名\\G&gt; 其中，各个字段的含义分别解释如下： NULL：表示该列是否可以存储NULL值 Key：表示该列是否已编制索引。PRI表示该列是表主键的一部分；UNI表示该列是UNIQUE索引的一部分；MUL表示在列中某个给定值允许出现多次。 Default：表示该列是否有默认值，有的话指定值是多少。 Extra：表示可以获取的与给定列有关的附加信息，例如AUTO_INCREMENT等。 🌟修改表 12345678910111213141516# 修改表名alter table &lt;旧表名&gt; rename [to] &lt;新表名&gt;;# 修改字段的数据类型alter table &lt;表名&gt; modify &lt;字段名&gt; &lt;数据类型&gt;;# 修改字段排列 first添加在最前面，after表示某字段之后，没有配置位置，默认添加在最后。alter table &lt;表名&gt; modify &lt;字段名&gt; &lt;数据类型&gt; &lt;first | after 已有字段&gt;;# 修改字段名alter table &lt;表名&gt; change &lt;旧字段名&gt; &lt;新字段名&gt; &lt;新数据类型&gt;;# 添加字段alter table &lt;表名&gt; add &lt;新字段名&gt; &lt;新数据类型&gt; [约束条件] [first | after &lt;已存在字段名&gt;];# 删除字段alter table &lt;表名&gt; drop &lt;字段名&gt;;# 删除表的外键约束alter table &lt;表名&gt; drop foreign key &lt;外键约束名&gt;;# 更换存储引擎alter table &lt;表名&gt; engine=&lt;更换后的存储引擎名称&gt;; 删除表 12drop table [if exists]表1[,表2,...];# 存在外键约束要先解除外键，才能删除表。 数据类型 整数类型 类型名称 存储空间(/字节) 范围（有符号） 范围（无符号） tinyint 1 $-27$~$27-1$ 0~$2^8-1$ smallint 2 $-2{15}$~$2{15}-1$ 0~$2^{16}-1$ mediumint 3 $-2{23}$~$2{23}-1$ 0~$2^{24}-1$ int 4 $-2{31}$~$2{31}-1$ 0~$2^{32}-1$ bigint 8 $-2{63}$~$2{63}-1$ 40~$2^{64}-1$ ⚠️显示宽度只用于显示，并不能限制取值范围和占用空间。例如：int(3)会占用4字节的存储空间，并且允许的最大值不会是999，而是INT整型所允许的最大值。 浮点数和定点数类型 类型名称 说明 存储空间(/字节) float 单精度 4 double 双精度 8 decimal(M,D),dec 定点 M+2 ⚠️用户指定的精度超出精度范围，则会四舍五入。 💬在MySQL中，定点数以字符串形式存储，在对精度要求比较高的时候（如货币、科学数据等）使用decimal的类型比较好，另外两个浮点数进行减法和比较运算时容易出问题，所以在使用浮点数时需要注意，并尽量避免做浮点数比较。 日期和时间类型 类型名称 日期格式 日期范围 存储空间(/字节) year yyy 1901~2155 1 time hh:mm:ss -839:59:59 ~ 838:59:59 3 date yyy-MM-dd 1001-01-01 ~ 9999-12-31 3 datetime yyy-MM-dd hh:mm:ss 1001-01-01 00:00:00 ~ 9999-12-3 23:59:59 8 timestamp yyy-MM-dd hh:mm:ss 1970-01-01 00:00:01 utc ~ 2038-01-19 03:17:07 utc 4 ⚠️ time类型的输入格式为d hh:mm:ss，最终可以转换到hh:mm:ss格式。注意，使用d hh格式输入时，小时一定要是双位，小于10的在前面加0 date允许不严格语法：任何标点符号都可以用做日期部分之间的间隔符。 datetime在存储日期数据时，按实际输入的格式存储，即输入什么就存储什么，与时区无关； timestamp值的存储是以UTC（世界标准时间）格式保存的，存储时对当前时区进行转换，检索时再转换回当前时区。查询时，不同时区显示的时间值是不同的。 文本字符串类型 类型名称 说明 存储空间(/字节) char(M) 固定长度 $1\\leq{M}\\leq{255}$ varchar(M) 可变长度 L+1，$L\\leq{M}$ 和 $1\\leq{M}\\leq{255}$ tinytext L+1，$L\\leq{2^8}$ text L+2，$L\\leq{2^{16}}$ mediumtext L+3，$L\\leq{2^{24}}$ longtext L+4，$L\\leq{2^{32}}$ enum 枚举类型，只能有一个字符串值 1或者2，取决于枚举值的数目（最大值为65535） set 设置，字符串对象可以有零个或者多个set成员 1，2，3，4或者8，取决于集合成员的数量（最多64个成员） 二进制字符串类型 类型名称 说明 存储空间(/字节) bti(M) 位字段 $\\frac{M+7}8$ binary(M) 固定长度 M varbinary(M) 可变长度 M+1 tinyblob(M) 最大长度：$(28-1)2$B L+1，$L\\leq{2^8}$ blob(M) 最大长度：$(2{16}-1)2$B L+2，$L\\leq{2^{16}}$ mediumblob(M) 最大长度：$(2{24}-1)2$B L+3，$L\\leq{2^{24}}$ longblob(M) 最大长度：$(2{32}-1)2$B 或 4GB L+4，$L\\leq{2^{32}}$ 运算符 运算符是告诉MySQL执行特定算术或逻辑操作的符号。MySQL中的运算符逻辑与C、Java等语言的运算符逻辑相通的内容，就不过多赘述了。以下比较运算符是MySQL特有的。 比较运算符 作用 &lt;=&gt; 安全等于。这个操作符和=操作符执行相同的比较操作，不过&lt;=&gt;可以用来判断NULL值 is null 判断是否为null is not null 判断是否不为null least 在有两个或多个参数时，返回最小值 greatest 当有两个或多个参数时，返回最大值 between and 判断一个值是否落在两个值之间 isnull 与is null 作用一致 in 判断一个值是否存在与给定的列表中 not in 判断一个值是否不存在与给定的列表中 like 通配符匹配 【%匹配任意数目(包括0个)的字符；_只能匹配一个字符】 regexp 正则匹配 regexp: ^ 开头 $结尾 .任意一个 *匹配任意个 函数 数学函数 表达式 作用 abs(x) 绝对值 pi() $\\pi$，结果保留7位小数 sqrt(x) $\\sqrt{x}$ mod(x,y) x被y除后的余数 ceil(x) / ceiling(x) 返回不小于x的最小整数值，返回值转化为一个BIGINT floor(x) 返回不大于x的最大整数值，返回值转化为一个BIGINT rand() rand()返回一个随机浮点值v，范围在0到1之间（0 ≤ v ≤1.0）若已指定一个整数参数xrand(x)，则它被用作种子值，用来产生重复序列。 round(x) round(x,y) round(x) 返回最接近于参数x的整数，对x值进行四舍五入使用round(x,y)函数对操作数进行四舍五入操作，结果保留小数点后面指定y位。y为负数时，保留的小数点左边的相应位数直接保存为0，不进行四舍五入。 truncate(x,y) 返回被舍去至小数点后y位的数字x。若y的值为0，则结果不带有小数点或不带有小数部分。若y设为负数，则截去（归零）x小数点左起第y位开始后面所有低位的值 sign(x) 返回参数的符号，x的值为负、零或正时返回结果依次为-1、0或1 pow(x,y)power(x,y) 函数返回x的y次乘方的结果值 log(x)log10(x) log(x)返回x的自然对数，x相对于基数e的对数log10(x)返回x的基数为10的对数 radians(x)degrees(x) radians(x)将参数x由角度转化为弧度degrees(x)将参数x由弧度转化为角度 sin \\ cos \\ asin \\ acos \\ tan \\ atan \\ cot 正弦 \\ 余弦 \\ 反正弦 \\ 反余弦 \\ 正切 \\ 反正切 \\ 余切 字符串函数 函数 作用 char_length(str)length(str) 返回值为字符串str所包含的字符个数返回值为字符串的字节长度[中文2 / 英文1] concat(s1,s2,…) 返回结果为连接参数产生的字符串，或许有一个或多个参数。有null返回null，有二进制返回二进制 concat_ws(x,s1,s2,…) 参数x是其他参数的分隔符，分隔符的位置放在要连接的两个字符串之间。分隔符可以是一个字符串，也可以是其他参数。如果分隔符为NULL，则结果为NULL。函数会忽略任何分隔符参数后的NULL值。 insert(s1,x,len,s2) 返回字符串s1，其子字符串起始于x位置和被字符串s2取代的len字符。如果x超过字符串长度，则返回值为原始字符串。假如len的长度大于其他字符串的长度，则从位置x开始替换。若任何一个参数为NULL，则返回值为NULL lower (str)lcase (str) 将字符串str中的字母字符全部转换成小写字母。 upper(str)ucase(str) 将字符串str中的字母字符全部转换成大写字母。 left(s,n)right(s,n) 返回字符串s开始的最【左 | 右】边n个字符 lpad(s1,len,s2)rpad(s1,len,s2) 字符串s1，其【左 | 右】边由字符串s2填补到len字符长度 ltrim(s)rtrim(s)trim(s) 返回字符串s，字符串【左 | 右 | 两】侧空格字符被删除 trim(s1 from s) 删除字符串s中两端所有的子字符串s1 repeat(s,n) 重复生成字符串 space(n) 返回一个由n个空格组成的字符串 replace(s,s1,s2) 使用字符串s2替代字符串s中所有的字符串s1。 strcmp(s1,s2) 若所有的字符串均相同，则返回0；若根据当前分类次序，第一个参数小于第二个，则返回-1；其他情况返回1 substring(s,n,len) 带有len参数的格式，从字符串s返回一个长度与len字符相同的子字符串，起始于位置n。也可能对n使用一个负值。假若这样，则子字符串的位置起始于字符串结尾的n字符，即倒数第n个字符，而不是字符串的开头位置 mid(s,n,len) 与上一致。len小于1，结果时空字符串。 locate(str1,str)position(str1 in str)instr(str, str1) 返回子字符串str1在字符串str中的开始位置。 reverse(s) 字符串反转 elt(n，字符串1，字符串2，…，字符串n) 若n =1，则返回值为字符串1；若n=2，则返回值为字符串2；以此类推；若n小于1或大于参数的数目，则返回值为null field(s,s1,s2,…,sn) 返回字符串s在列表s1,s2,…,sn中第一次出现的位置，在找不到s的情况下，返回值为0。如果s为null，则返回值为0，原因是null不能同任何值进行同等比较。 make_set(x,s1,s2,…,sn) 函数按x的二进制数从s1，s2,…,sn中选取字符串。例如5的二进制是0101，这个二进制从右往左的第1位和第3位是1，所以选取s1和s3。s1,s2,…,sn中的null值不会被添加到结果中。 日期时间函数 函数 作用 curdate()current_date() 将当前日期按照 YYYY-MM-DD current_timestamp()localtime()now()sysdate() 返回当前日期和时间值，格式为YYYY-MM-DD HH:MM:SS unix_timestamp(date)from_unixtime(date) 时间转时间戳时间戳转时间 utc_date() 返回当前UTC（世界标准时间）日期值，其格式为YYYY-MM-DD month(date)monthname(date) 返回date对应的月份，范围值为1~12返回日期date对应月份的英文全名。 dayname(d)dayofweek(d)weekday(d) dayname(d)：返回d对应的工作日的英文名称dayofweek(d)：返回d对应的一周中的索引（位置，1表示周日，2表示周一，…，7表示周六）weekday(d)：返回d对应的工作日索引：0表示周一，1表示周二，…，6表示周日。 week(d)weekofyear(d) week(d)计算日期d是一年中的第几周weekofyear(d)计算某天位于一年中的第几周，范围是1~53 dayofyear(d)dayofmonth(d) dayofyear(d)函数返回d是一年中的第几天，范围是1~366 dayofmonth(d)函数返回d是一个月中的第几天，范围是1~31。 year(date)quarter(date)month(date)day(date)hour(time)minute(time)second(time) 年份、季度、月份、日期、小时、分钟和秒钟 extract(type from date) 例如extract(year_month from date)其中的type是上栏中的类型，年月和day time_to_sec(time)sec_to_time(seconds) 时间转成秒数秒数转成时间 date_add()adddate()date_sub()subdate()addtime()subtime()date_diff() 计算日期和时间的函数 date_format(date,format) 格式化日期 time_format(time,format) 格式化时间 条件判断函数 if(expr,v1,v2) 如果表达式expr是true(expr &lt;&gt; 0 and expr &lt;&gt; null)，则返回值为v1；否则返回值为v2。if()的返回值为数字值或字符串值，具体情况视其所在语境而定。 如果v1或v2中只有一个明确是NULL，则if()函数的结果类型为非null表达式的结果类型。 ifnull(v1,v2) 假如v1不为null，则ifnull()的返回值为v1；否则其返回值为v2。ifnull()的返回值是数字或者字符串，具体情况取决于其所在 case case expr when v1 then r1 [when v2then r2]…[else rn+1] end：如果expr值等于某个vn，则返回对应位置then后面的结果；如果与所有值都不相等，则返回else后面的rn+1 系统信息函数 作用 函数 获取版本号 version() 获取MySQL服务器当前连接的次数 cibbectuib_id() 显示有哪些线程在运行（包括当前连接数，连接状态、帮助识别） processlist 获取用户名 user()、current_user、current_user()、system_user()和session_user() 获取字符串的字符集 charset(str) 返回字符串str的字符排列方式 collation(str) 返回最后生成的AUTO_INCREMENT值 last_insert_id() 加密函数 函数 描述 md5(str) 字符串算出一个MD5 128比特校验和。该值以32位十六进制数字的二进制字符串形式返回，若参数为NULL，则会返回NULL sha(str) 从原明文密码str计算并返回加密后的密码字符串，当参数为NULL时，返回NULL。SHA加密算法比MD5更加安全。 sha2(str, hash_length) 使用hash_length作为长度，加密str。hash_length支持的值为224、256、384、512和0。其中，0等同于256。 其他 函数 描述 format(x,n) 将数字x格式化，并以四舍五入的方式保留小数点后n位，结果以字符串的形式返回。若n为0，则返回结果函数不含小数部分。 conv(n, from_base, to_base) 函数进行不同进制数间的转换。返回值为数值n的字符串表示，由from_base进制转化为to_base进制。如有任意一个参数为null，则返回值为null。自变量n被理解为一个整数，但是可以被指定为一个整数或字符串。最小基数为2，最大基数为36。 inet_aton(expr) 给出一个作为字符串的网络地址的点地址表示，返回一个代表该地址数值的整数。地址可以是4或8bit地址。 get_lock(str,timeout)release_lock(str) 加锁解锁 benchmark(count,expr) 函数重复count次执行表达式expr convert(… using …) 带有using的convert()函数被用来在不同的字符集之间转化数据。 cast(x, as type)convert(x, type) 函数将一个类型的值转换为另一个类型的值，可转换的type有binary、char(n)、date、time、datetime、decimal、signed、unsigned。 🌟数据的操作 查询语句 &#123;* | &lt;字段列表&gt;&#125;包含星号通配符和字段列表，表示查询的字段。其中，字段列表至少包含一个字段名称，如果要查询多个字段，多个字段之间用逗号隔开，最后一个字段后不加逗号。 在字段列前添加distinct去重 FROM &lt;表1&gt;,&lt;表2&gt;...，表1和表2表示查询数据的来源，可以是单个或者多个。 WHERE子句是可选项，如果选择该项，将限定查询行必须满足的查询条件。 GROUP BY &lt;字段&gt;，该子句告诉MySQL如何显示查询出来的数据，并按照指定的字段分组。 [HAVING &lt;条件表达式&gt;]用来筛选分组之后的数据展示 [ORDER BY &lt;字段&gt;]，该子句告诉MySQL按什么样的顺序显示查询出来的数据，可以进行的排序有升序（ASC）、降序（DESC）。 [LIMIT [&lt;offset&gt;,] &lt;row count&gt;]，该子句告诉MySQL每次显示查询出来的数据条数。 AS 用于表、列的重命名 聚合函数 函数 描述 avg() 平均值 count() 计数；count(*)，会统计空值行；count(字段)会忽略空值行 max() 最大值 min() 最小值 sum() 求和 连接查询 123456789101112CREATE TABLE `a` ( `a_id` varchar(100) DEFAULT NULL, `b_id` varchar(100) DEFAULT NULL, `name` varchar(100) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ciINSERT INTO `a` VALUES (&#x27;1&#x27;,&#x27;1&#x27;,&#x27;Duo&#x27;),(&#x27;2&#x27;,&#x27;1&#x27;,&#x27;Tony&#x27;),(&#x27;3&#x27;,&#x27;2&#x27;,&#x27;Lili&#x27;),(&#x27;4&#x27;,&#x27;4&#x27;,&#x27;Seri&#x27;);CREATE TABLE `b` ( `b_id` varchar(100) DEFAULT NULL, `class_name` varchar(100) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ciINSERT INTO `b` VALUES (&#x27;1&#x27;,&#x27;体育&#x27;),(&#x27;2&#x27;,&#x27;美术&#x27;),(&#x27;3&#x27;,&#x27;舞蹈&#x27;); 表a 表b a_id b_id b_id class_name name 12345678# 1. 使用where进行连接select a.name,b.class_name from a,b where a.b_id = b.b_id;# 2. 使用[inner] join进行连接，等价于使用where连接select a.name,b.class_name from a [inner] join b on a.b_id = b.b_id;# 3. left joinselect a.name,b.class_name from a left join b on a.b_id = b.b_id;# 4. right joinselect a.name,b.class_name from a right join b on a.b_id = b.b_id; 连接方式 描述 where 内连接，$a\\cap{b}$ [inner] join 内连接，$a\\cap{b}$ left join 外连接，数据展示以a为主 right join 外连接，数据展示以b为主 子查询 1234567891011# 1. any(任意一个) / some(多个，包括一个) / all 所有select a_id from a where a_id &gt; any (select b_id from b);select a_id from a where a_id &gt; some (select b_id from b);select a_id from a where a_id &gt; all (select b_id from b);# 2. exists / not exists [系统对子查询进行运算以判断它是否返回行，如果至少返回一行，那么exists的结果为true，此时外层查询语句将进行查询；如果子查询没有返回任何行，那么exists返回的结果是false，此时外层语句将不进行查询。not exists 与之相反]select * from a where [not] exists (select b_id from b where b_id &gt; 12);# 3. in 限制某字段范围在子查询查询结果中；in 也可以替换成比较运算符select * from a where b_id in (select b_id from b where class_name != &#x27;体育&#x27;);# 4. union / union all 合并查询 [两个查询结果列数不等不能合并]### union all 保留重复值select * from a union [all] select * from b 插入数据 12insert into table_name [(column_list)] values (value_list);insert into table_name values (value_list1),(value_list2),...; 更新数据 1update table_name set column_name1 = value1,... [where 条件]; 删除数据 1delete from table_name [where 条件] 添加计算列 12345create table tb1( a int(9), b int(9), c int(9) generated always as ((a+b)) virtual);","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/tags/MySQL/"}]},{"title":"Spring|基础|AOP","slug":"spring/基础AOP","date":"2024-03-25T08:20:00.000Z","updated":"2024-03-25T08:48:41.136Z","comments":true,"path":"2024/03/25/spring/基础AOP/","permalink":"https://solitaire-12.github.io/2024/03/25/spring/%E5%9F%BA%E7%A1%80AOP/","excerpt":"","text":"面向切面(AOP) AOP的本质也是为了解耦，它是一种设计思想 AOP(Aspect Oriented Programming)，意为面向切面编程。最早由AOP联盟组织提出的，指定的一套规范，Spring将AOP的思想引入到预编译方式和运行期间东岱代理实现程序的统一维护的一种技术。 从要点实例分析，类似于添加日志这种解耦类的问题，如果没有Spring框架， 我们需要在每个service的方法中添加记录日志的方法；由Spring之后，通过@Aspect注解定义切面，这个切面中定义了拦截所有service中的方法，并记录日志，从代码角度而言，记录日志与业务代码之间已经解耦。 123456789101112@Aspectpublic class LogAspect &#123; /** * set aspect to get the name of the service method being done */ @Around(&quot;execution(* x.service.*.*(..))&quot;) public Object businessService(ProceedingJoinPoint pjp) throws Throwable&#123; Method method = ((MethodSignature) pjp.getSignature()).getMethod(); System.out.println(&quot;execute method:&quot;+method.getName()); return pjp.proceed(); &#125;&#125; 将记录日志功能解耦为日志切面，它的目标是解耦。进而引出AOP的理念：将分散在业务逻辑代码中相同的代码通过横向切割的方式抽取到一个独立的模块中。 OOP面向对象编程，针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。而AOP则是针对业务处理过程中的切面进行提取，它所面对的是处理过程的某个步骤或阶段，以获得逻辑过程中的各种部分之间低耦合的隔离效果。两种设计思想在目标上有本质的区别。 术语 首先明确，AOP的概念和术语并非Spring特有的。 连接点(Jointpoint)：表示需要在程序中插入横切关注点的拓展点，连接点可能是类初始化、方法执行、方法调用、字段调用和处理异常等等，Spring只支持方法执行连接点，在AOP中表示在哪里干。 切入点(Pointcut)：选择一组相关连接点的模式，即可以任务连接点的集合，Spring支持perl5正则表达式和AspectJ切入点模式，Spring默认使用AspectJ语法，在AOP中表示在哪里干的集合 通知(Advice)：在连接点上执行的代码，通知提供了在AOP中需要在切入点所选择的连接点出进行拓展现有行为的手段；包括前置通知、后置通知、环绕通知，在Spring中通过代理模式实现AOP，并通过连接器模式以环绕连接点的拦截器链织入通知；在AOP中表示干什么。 💡 通知的类型： 前置通知(before advice)：在某个连接点之前执行的通知，但这个通知不能阻止连接点之前执行流程(除非它抛出一个异常)。 后置通知(after returning advice)：在某连接点正常执行完成后执行的通知。 异常通知(after throwing advice)： 在方法抛出异常退出时执行的通知。 最终通知(after [finally] advice)： 当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 环绕通知(around advice)： 包围一个连接点的通知，如方法调用。这是最强大的一种通知类型。环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它自己的返回值或抛出异常来结束执行。 环绕通知是最常用的通知类型。和AspectJ一样，Spring提供所有类型的通知，我们推荐你使用尽可能简单的通知类型来实现需要的功能。例如，如果你只是需要一个方法的返回值来更新缓存，最好使用后置通知而不是环绕通知，尽管环绕通知也能完成同样的事情。用最合适的通知类型可以使得编程模型变得简单，并且能够避免很多潜在的错误。比如，你不需要在JoinPoint上调用用于环绕通知的proceed()方法，就不会有调用的问题。 切面(Aspect)：横切关注点的模块化，比如上述代码中的日志组件。可以认为i是通知、引入和切入点的组合；在Spring中可以使用Schema和@AspectJ方式进行组织实现；在AOP中表示在哪干了什么； 引入(inter-type declaration)：也成为内部类声明，为已有的类添加额外新的字段或者方法，Spring允许引入新的接口(必须对应一个实现)到所有被代理对象(目标对象)，在AOP中表示为引入什么。 目标对象(Target Object)：需要被织入横切关注点的对象，即该对象是切入点选择的对象，需要被通知的对象，从而也可以称为被通知对象。由于Spring AOP通过代理实现，从而这个对象永远是被代理对象，在AOP中表示对谁干。 织入(Weaving)：把切面连接到其他的应用程序类型或者对象上，并创建一个被通知的对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。在AOP中表示如何实现。 AOP代理(AOP Proxy)：AOP框架使用代理模式创建的兑现给，从而实现在连接带你处插入通知（即应用切面），就是通过代理来对目标对象应用切面。在Spring中，AOP代理可以用JDK动态代理或者CGLIB代理实现，而通过连接器模型应用切面。在AOP中表示为怎么实现的一种典型方式。 Spring AOP和AspectJ是什么关系 首先AspectJ是什么？ AspectJ是一个java实现的AOP框架，它能够对java代码进行AOP编译（一般在编译期进行），让java代码具有AspectJ的AOP功能（当然需要特殊的编译器）。可以这样说AspectJ是目前实现AOP框架中最成熟，功能最丰富的语言，而且AspectJ与java程序完全兼容，几乎是无缝关联。 其次，为什么需要理清楚Spring AOP和AspectJ的关系？ Spring AOP和AspectJ是什么关系？ AspectJ是更强的AOP框架，是实际意义的AOP标准； Spring为何不写类似AspectJ的框架？ Spring AOP使用纯Java实现, 它不需要专门的编译过程, 它一个重要的原则就是无侵入性（non-invasiveness）; Spring 小组完全有能力写类似的框架，只是Spring AOP从来没有打算通过提供一种全面的AOP解决方案来与AspectJ竞争。Spring的开发小组相信无论是基于代理（proxy-based）的框架如Spring AOP或者是成熟的框架如AspectJ都是很有价值的，他们之间应该是互补而不是竞争的关系。 Spring小组喜欢@AspectJ注解风格更胜于Spring XML配置； 所以在Spring 2.0使用了和AspectJ 5一样的注解，并使用AspectJ来做切入点解析和匹配。但是，AOP在运行时仍旧是纯的Spring AOP，并不依赖于AspectJ的编译器或者织入器（weaver）。 Spring 2.5对AspectJ的支持：在一些环境下，增加了对AspectJ的装载时编织支持，同时提供了一个新的bean切入点。 更多关于AspectJ？ 了解AspectJ应用到java代码的过程（这个过程称为织入），对于织入这个概念，可以简单理解为aspect(切面)应用到目标函数(类)的过程。 对于这个过程，一般分为动态织入和静态织入： 动态织入的方式是在运行时动态将要增强的代码织入到目标类中，这样往往是通过动态代理技术完成的，如Java JDK的动态代理(Proxy，底层通过反射实现)或者CGLIB的动态代理(底层通过继承实现)，Spring AOP采用的就是基于运行时增强的代理技术 ApectJ采用的就是静态织入的方式。ApectJ主要采用的是编译期织入，在这个期间使用AspectJ的acj编译器(类似javac)把aspect类编译成class字节码后，在java目标类编译时织入，即先编译aspect类再编译目标类。 AOP的配置方式 Spring AOP支持对XML模式和基于@AspectJ注解的两种配置方式。 XML Schema配置方式 目标类 12345678910111213141516public class AopDemoServiceImpl &#123; public void doMethod1() &#123; System.out.println(&quot;AopDemoServiceImpl.doMethod1()&quot;); &#125; public String doMethod2() &#123; System.out.println(&quot;AopDemoServiceImpl.doMethod2()&quot;); return &quot;hello world&quot;; &#125; public String doMethod3() throws Exception &#123; System.out.println(&quot;AopDemoServiceImpl.doMethod3()&quot;); throw new Exception(&quot;some exception&quot;); &#125;&#125; 切面类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class LogAspect &#123; /** * 环绕通知. * * @param pjp pjp * @return obj * @throws Throwable exception */ public Object doAround(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;-----------------------&quot;); System.out.println(&quot;环绕通知: 进入方法&quot;); Object o = pjp.proceed(); System.out.println(&quot;环绕通知: 退出方法&quot;); return o; &#125; /** * 前置通知. */ public void doBefore() &#123; System.out.println(&quot;前置通知&quot;); &#125; /** * 后置通知. * * @param result return val */ public void doAfterReturning(String result) &#123; System.out.println(&quot;后置通知, 返回值: &quot; + result); &#125; /** * 异常通知. * * @param e exception */ public void doAfterThrowing(Exception e) &#123; System.out.println(&quot;异常通知, 异常: &quot; + e.getMessage()); &#125; /** * 最终通知. */ public void doAfter() &#123; System.out.println(&quot;最终通知&quot;); &#125;&#125; XML配置AOP 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;x&quot; /&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;!-- 目标类 --&gt; &lt;bean id=&quot;demoService&quot; class=&quot;x.service.AopDemoServiceImpl&quot;&gt; &lt;!-- configure prop&lt;/bean&gt; &lt;!-- 切面 --&gt; &lt;bean id=&quot;logAspect&quot; class=&quot;x.aspect.LogAspect&quot;&gt; xfig&gt; &lt;!-- 配置切面 --&gt; &lt;aop:aspect ref=&quot;logAspect&quot;&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut id=&quot;pointCutMethod&quot; expression=&quot;execution(* x.service.*.*(..))&quot;/&gt; &lt;!-- 环绕通知 --&gt;x method=&quot;doAround&quot; pointcut-ref=&quot;pointCutMethod&quot;/&gt; &lt;!-- 前置通知 --&gt; &lt;aop:before method=&quot;doBefore&quot; pointcut-ref=&quot;pointCutMethod&quot;/&gt; &lt;!-- 后置通知；returning属性：用于设置后置通知的第二个参数的名称，类型是Object --&gt; &lt;aop:after-returning method=&quot;doAfterReturning&quot; pointcut-ref=&quot;pointCutMethod&quot; returning=&quot;result&quot;/&gt; &lt;!-- 异常通知：如果没有异常，将不会执行增强；throwing属性：用于设置通知第二个参数的的名称、类型--&gt; &lt;aop:after-throwing method=&quot;doAfterThrowing&quot; pointcut-ref=&quot;pointCutMethod&quot; throwing=&quot;e&quot;/&gt; &lt;aop:after method=&quot;doAfter&quot; pointcut-ref=&quot;pointCutMethod&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 测试 1234567891011public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;aspects.xml&quot;); AopDemoServiceImpl service = context.getBean(&quot;demoService&quot;, AopDemoServiceImpl.class); service.doMethod1(); service.doMethod2(); try &#123; service.doMethod3(); &#125; catch (Exception e) &#123; // e.printStackTrace(); &#125;&#125; AspectJ注解方式 注解方式配置aop 切面 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Aspect@Componentpublic class LogAspect &#123; /** * 环绕通知. */ @Around(&quot;execution(* x.service.*.*(..))&quot;) public Object doAround(ProceedingJoinPoint pjp) throws Throwable &#123; System.out.println(&quot;-----------------------&quot;); System.out.println(&quot;环绕通知: 进入方法&quot;); Object o = pjp.proceed(); System.out.println(&quot;环绕通知: 退出方法&quot;); return o; &#125; /** * 前置通知. */ @Before(&quot;execution(* x.service.*.*(..))&quot;) public void doBefore() &#123; System.out.println(&quot;前置通知&quot;); &#125; /** * 后置通知. */ @AfterReturning(value = &quot;execution(* x.service.*.*(..))&quot;,returning = &quot;result&quot;) public void doAfterReturning(String result) &#123; System.out.println(&quot;后置通知, 返回值: &quot; + result); &#125; /** * 异常通知. */ @AfterThrowing(value = &quot;execution(* x.service.*.*(..))&quot;,throwing = &quot;e&quot;) public void doAfterThrowing(Exception e) &#123; System.out.println(&quot;异常通知, 异常: &quot; + e.getMessage()); &#125; /** * 最终通知. */ @After(&quot;execution(* x.service.*.*(..))&quot;) public void doAfter() &#123; System.out.println(&quot;最终通知&quot;); &#125;&#125; 12345678910111213141516@EnableAspectJAutoProxy@Configurationpublic class App &#123; public static void main(String[] args) &#123; AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(&quot;x&quot;); AopServiceImpl service = context.getBean(AopServiceImpl.class); service.doMethod1(); service.doMethod2(); try &#123; service.doMethod3(); &#125; catch (Exception e) &#123; // e.printStackTrace(); &#125; &#125;&#125; 🌟execution语法： 关于 @After(&quot;execution()&quot;)中execution()的作用是去找到与之匹配的切点。 语法： execution(&lt;修饰符模式&gt;?&lt;返回类型模式&gt;&lt;方法名模式&gt;(&lt;参数模式&gt;)&lt;异常模式&gt;?) ,例如下↓： 1// execution(* com.service.impl..*.*(..)) 含义： *,代表任意 com.service.impl，代表AOP的切入点包位置 com.service.impl..，表示当前包以及子包 com.service.impl..*，表示当前包以及子包下的所有的类 com.service.impl..*(..)，表示表示当前包以及子包下的所有的类的任何方法，(..)中的点点表示任意参数类型 ❗AOP切不到static方法 🔗：获取到方法的参数，返回值","categories":[{"name":"Spring","slug":"Spring","permalink":"https://solitaire-12.github.io/categories/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://solitaire-12.github.io/tags/spring/"},{"name":"AOP","slug":"AOP","permalink":"https://solitaire-12.github.io/tags/AOP/"}]},{"title":"Spring|基础|IOC/DI","slug":"spring/基础IOC","date":"2024-03-25T08:15:00.000Z","updated":"2024-03-25T08:45:44.941Z","comments":true,"path":"2024/03/25/spring/基础IOC/","permalink":"https://solitaire-12.github.io/2024/03/25/spring/%E5%9F%BA%E7%A1%80IOC/","excerpt":"","text":"控制反转(IOC) 从【基础|第一个spring项目】中的实例分析，如果没有Spring框架，就需要手动去创建User/Dao/Service等；有了Spring框架后，可以将原有的Bean的创建工作转给框架，需要用的时候从Bean的容器中获取即可。 1234567// create and configure beansApplicationContext context = new ClassPathXmlApplicationContext(&quot;aspects.xml&quot;, &quot;daos.xml&quot;, &quot;services.xml&quot;);// retrieve configured instanceUserServiceImpl service = context.getBean(&quot;userService&quot;, UserServiceImpl.class);// use configured instanceList&lt;User&gt; userList = service.findUserList(); Spring Bean Spring里边的bean就类似是定义的一个组件，而这个组件的作用就是是心啊某个功能。这里所定义的bean就相当于给了你一个更为简便的方法去调用组件来实现你的业务。 IOC Inversion of Control，即控制反转，是一种设计思想。 谁控制谁，控制什么？ 传统的Java SE程序设计，直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IOC是由专门的一个容器来创建这些对象，即由IOC容器来控制对象的创建； 所以，IOC容器控制了对象。主要控制了外部资源获取。 为什么是反转？ 传统的由自己在对象中主动控制去获取依赖对象的就是正转；通过容器来帮忙创建以及注入依赖对象的，叫反转。因为由容器将依赖对象注入，对象是被动的接收依赖对象的，所以称为反转。 传统方式： IOC： IOC和DI 控制反转是通过依赖注入实现的，其实它们是同一个概念的不同角度的描述。简单来说IOC是设计思想，DI是实现方式。 DI(Dependency Injection)，依赖注入。组件之间依赖关系由容器在运行期间决定。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。 我们来深入分析一下： 谁依赖于谁？ 当然是应用程序依赖于IoC容器； 为什么需要依赖？ 应用程序需要IoC容器来提供对象需要的外部资源； 谁注入谁？ 很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象； 注入了什么？ 就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。 IoC和DI有什么关系呢？ 其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。通俗来说就是IoC是设计思想，DI是实现方式。 IOC三种配置方式 xml配置 顾名思义，就是将bean的信息配置.xml文件里，通过Spring加载文件为我们创建bean。这种方式出现很多早前的SSM项目中，将第三方类库或者一些配置工具类都以这种方式进行配置，主要原因是由于第三方类不支持Spring注解。 优点： 可以使用于任何场景，结构清晰，通俗易懂 缺点： 配置繁琐，不易维护，枯燥无味，扩展性差 举例： 配置xx.xml文件 声明命名空间和配置bean 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- services --&gt; &lt;bean id=&quot;userService&quot; class=&quot;x.service.UserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; Java配置 将类的创建交给我们配置的JavcConfig类来完成，Spring只负责维护和管理，采用纯Java创建方式。其本质上就是把在XML上的配置声明转移到Java配置类中 优点：适用于任何场景，配置方便，因为是纯Java代码，扩展性高，十分灵活 缺点：由于是采用Java类的方式，声明不明显，如果大量配置，可读性比较差 举例： 创建一个配置类， 添加@Configuration注解声明为配置类 创建方法，方法上加上@bean，该方法用于创建实例并返回，该实例创建后会交给spring管理，方法名建议与实例名相同（首字母小写）。注：实例类不需要加任何注解 123456789@EnableAspectJAutoProxy@Configurationpublic class BeanConfig &#123; @Bean(&quot;userDao&quot;) public UserDaoImpl userDao()&#123; return new UserDaoImpl(); &#125; //...&#125; 注解配置 通过在类上加注解的方式，来声明一个类交给Spring管理，Spring会自动扫描带有@Component，@Controller，@Service，@Repository这四个注解的类，然后帮我们创建并管理，前提是需要先配置Spring的注解扫描器。 优点：开发便捷，通俗易懂，方便维护。 缺点：具有局限性，对于一些第三方资源，无法添加注解。只能采用XML或JavaConfig的方式配置 举例： 对类添加@Component相关的注解，比如@Controller，@Service，@Repository 设置ComponentScan的basePackage，例如&lt;context:component-scan base-package='x'&gt;,或者@ComponentScan(&quot;x&quot;)注解，或者new AnnotationConfigApplicationContext(&quot;x&quot;);都是在指定扫描的basePackage。 DI的三种方式 setter 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- services --&gt; &lt;bean id=&quot;userService&quot; class=&quot;x.service.UserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 本质上包含两步： 需要new UserServiceImpl()创建对象，所以需要默认构造函数 调用setUserDao()函数注入userDao的值，所以需要setUserDao()函数 对应的service类： 1234567891011public class UserServiceImpl &#123; private UserDaoImpl userDao; public UserServiceImpl() &#123; &#125; public List&lt;User&gt; findUserList()&#123; return this.userDao.findUserList(); &#125; public void setUserDao(UserDaoImpl userDao)&#123; this.userDao = userDao; &#125;&#125; 注解Java配置： 12345678910public class UserServiceImpl &#123; private UserDaoImpl userDao; public List&lt;User&gt; findUserList() &#123; return this.userDao.findUserList(); &#125; @Autowired public void setUserDao(UserDaoImpl userDao) &#123; this.userDao = userDao; &#125;&#125; 构造函数 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- services --&gt; &lt;bean id=&quot;userService&quot; class=&quot;x.service.UserServiceImpl&quot;&gt; &lt;constructor-arg name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;!-- additional collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions for services go here --&gt;&lt;/beans&gt; 注解输入 以@Autowired（自动注入）注解注入为例，修饰符有三个属性：Constructor，byType，byName。默认按照byType注入。 constructor：通过构造方法进行自动注入，spring会匹配与构造方法参数类型一致的bean进行注入，如果有一个多参数的构造方法，一个只有一个参数的构造方法，在容器中查找到多个匹配多参数构造方法的bean，那么spring会优先将bean注入到多参数的构造方法中。 byName：被注入bean的id名必须与set方法后半截匹配，并且id名称的第一个单词首字母必须小写，这一点与手动set注入有点不同。 byType：查找所有的set方法，将符合符合参数类型的bean注入。 12345678@Servicepublic class UserServiceImpl &#123; @Autowired private UserDaoImpl userDao; public List&lt;User&gt; findUserList() &#123; return userDao.findUserList(); &#125;&#125; IOC和DI使用问题小结 为什么推荐构造器注入方式？ Spring文档中，有这么一段描述： The Spring team generally advocates constructor injection as it enables one to implement application components as immutable objects and to ensure that required dependencies are not null. Furthermore constructor-injected components are always returned to client (calling) code in a fully initialized state. Spring团队通常提倡构造函数注入，因为它允许将应用程序组件实现为不可变对象，并确保所需的依赖项不为空。此外，构造函数注入的组件总是以完全初始化的状态返回给客户端(调用)代码。 不可变对象：其实说的就是final关键字 依赖项不为空：当实例化UserServiceImpl的时候，由于自己实现了有参数的构造函数，所以不会调用默认构造参数，那么就需要Spring容器传入所需要的此参数，这时就会出现两种情况：有该类型的参数，传入；没有，报错。 完全初始化的状态：这个可以跟上面的依赖项不为空组合起来看，向构造器传参之前，要确保注入的内容不为空，那么肯定要调用依赖组件的构造方法完成实例化。而在Java类加载实例化的过程中，构造方法是最后一步（之前那如果有父类先初始化父类，然后自己的成员变量，最后才是构造方法），所以返回来的都是初始化之后的状态。 1234567@Servicepublic class UserServiceImpl&#123; private final UserDaoImpl userDao; public UserServiceImpl(final UserDaoImpl userDaoImpl)&#123; this.userDao = userDaoImpl; &#125;&#125; 如果使用setter注入，缺点显而易见，对于IOC容器以外的环境，除了使用反射来提供它需要的以来之外，无法无用该类的实现类。而且将一直是个潜在隐患，因为你不调用将一直无法确定NullPointerException异常是否存在。 循环依赖问题 使用field植入可能导致循环依赖，即A里面注入B，B里面注入A： 12345678public class A&#123; @Autowired private B b;&#125;public class B&#123; @Autowired private A a;&#125; 如果使用构造器注入，在项目启动的时候，就会抛出BeanCurrentlyCreationException:Requested bean is currently in creation:Is there an unresolvable circular reference?从而在项目启动之前处理掉循环依赖的问题。 DI的三种注解 @Autowired Spring2.5引入了@Autowired注解： 123456@Target(&#123;ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Autowired &#123; boolean required() default true;&#125; 从元注解@Target注解上看，可以使用在： ElementType.CONSTRUCTOR 构造函数 ElementType.METHOD 方法 ElementType.PARAMETER 方法参数 ElementType.FIELD 字段、枚举的常量 ElementType.ANNOTATION_TYPE 注解 还有一个required，默认是true，代表注入bean的时候该bean必须存在，不然就会注入失败；设置成false，代表注入bean时，bean存在，则注入成功；不存在就忽略跳过，启动时不报错，会将NullPointerException隐藏。 @Autowired总结： 是Spring自带的注解，通过AutowiredAnnotationBeanPostProcessor类实现的依赖注入。 可以作用在构造函数，方法，方法参数，字段、枚举的常量，注解 默认根据类型(byType)进行自动装配 🔶 ​如果有多个类型一样的Bean候选者，需要指定按照名称(byName)进行装配，则需要配合@Qualifier 123@Qualifier(&quot;xxxxyyyy&quot;)@Autowired(required = false)private HelloDao helloDao; @Resoure 123456@Target(&#123;TYPE, FIELD, METHOD&#125;)@Retention(RUNTIME)public @interface Resource &#123; String name() default &quot;&quot;; // ...&#125; 从元注解@Target注解上看，可以使用在： TYPE 接口、类、枚举、注解 FIELD 字段、枚举的常量 METHOD 方法 name指定注入指定名称的组件。 @Resoure总结： 是JSR250规范的是实现，在javax.annotation包下 可以作用在TYPE, FIELD, METHOD 默认根据属性名称进行自动装配的，如果有多个类型一样的Bean候选者，则可以通过name进行指定进行注入 123456@Componentpublic class SuperMan &#123; @Resource(name = &quot;BMW&quot;) private Car car;&#125;// name的作用类似于@Qualifier @Inject 1234@Target(&#123; METHOD, CONSTRUCTOR, FIELD &#125;)@Retention(RUNTIME)@Documentedpublic @interface Inject &#123;&#125; 从元注解@Target注解上看，可以使用在： CONSTRUCTOR 构造函数 FIELD 字段、枚举的常量 METHOD 方法 @Inject总结： 是JSR330 (Dependency Injection for Java)中的规范，需要导入javax.inject.Inject jar包 ，才能实现注入 可以作用CONSTRUCTOR、METHOD、FIELD上 根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named； 123@Inject@Named(&quot;BMW&quot;) // 作用类似于@Qualifierprivate Car car; 总结 @Autowired是Spring自带的，@Resource是JSR250规范实现的，@Inject是JSR330规范实现的 @Autowired、@Inject用法基本一样，不同的是@Inject没有required属性 @Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的 @Autowired如果需要按照名称匹配需要和@Qualifier一起使用，@Inject和@Named一起使用，@Resource则通过name进行指定 🔗 Spring源码分析@Autowired、@Resource注解的区别","categories":[{"name":"Spring","slug":"Spring","permalink":"https://solitaire-12.github.io/categories/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://solitaire-12.github.io/tags/spring/"},{"name":"IOC","slug":"IOC","permalink":"https://solitaire-12.github.io/tags/IOC/"}]},{"title":"Spring|基础|第一个spring项目","slug":"spring/spring项目","date":"2024-03-25T08:05:00.000Z","updated":"2024-03-25T08:44:59.827Z","comments":true,"path":"2024/03/25/spring/spring项目/","permalink":"https://solitaire-12.github.io/2024/03/25/spring/spring%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"hello Spring 结合上面的使用场景，设计一个查询用户的案例的两个需求，来看Spring框架帮我们简化了什么开发工作: 查询用户数据 - 来看DAO+POJO-&gt; Service 的初始化和装载。 给所有Service的查询方法记录日志 创建maven项目 添加pom依赖 1234567891011121314151617181920212223242526272829&lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.3.9&lt;/spring.version&gt; &lt;aspectjweaver.version&gt;1.9.6&lt;/aspectjweaver.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;$&#123;aspectjweaver.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 创建用户实例 - User 12345678910111213141516171819202122232425262728public class User &#123; private String userName; private Integer age; public User() &#123; &#125; public User(String userName, Integer age) &#123; this.userName = userName; this.age = age; &#125; public String getUserName() &#123; return userName; &#125; public void setUserName(String userName) &#123; this.userName = userName; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125;&#125; 配置dao层 1234567891011public class UserDaoImpl &#123; public UserDaoImpl() &#123; &#125; /** * forged a query instance of the findUserList method */ public List&lt;User&gt; findUserList()&#123; return Collections.singletonList(new User(&quot;wey&quot;,18)); &#125;&#125; 配置daos.xml 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;userDao&quot; class=&quot;x.dao.UserDaoImpl&quot;&gt; &lt;!-- additional collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions for data access objects go here --&gt;&lt;/beans&gt; 配置服务层 123456789101112131415public class UserServiceImpl &#123; private UserDaoImpl userDao; public UserServiceImpl() &#123; &#125; // get the result of the forged a query instance of the findUserList method public List&lt;User&gt; findUserList()&#123; return this.userDao.findUserList(); &#125; public void setUserDao(UserDaoImpl userDao)&#123; this.userDao = userDao; &#125;&#125; 配置services.xml 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- services --&gt; &lt;bean id=&quot;userService&quot; class=&quot;x.service.UserServiceImpl&quot;&gt; &lt;!-- ps:The ref attribute of the property may be marked red in IDEA, but this does not affect the execution result --&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;!-- additional collaborators and configuration for this bean go here --&gt; &lt;/bean&gt;&lt;!-- ps:If you don&#x27;t want to see this red mark, add the following line of code --&gt;&lt;!-- &lt;bean id=&quot;userDao&quot; class=&quot;x.dao.UserDaoImpl&quot;/&gt;--&gt; &lt;!-- more bean definitions for services go here --&gt;&lt;/beans&gt; 配置aspect 123456789101112131415@Aspectpublic class LogAspect &#123; /** * set aspect to get the name of the service method being done * @param pjp * @return * @throws Throwable */ @Around(&quot;execution(* x.service.*.*(..))&quot;) public Object businessService(ProceedingJoinPoint pjp) throws Throwable&#123; Method method = ((MethodSignature) pjp.getSignature()).getMethod(); System.out.println(&quot;execute method:&quot;+method.getName()); return pjp.proceed(); &#125;&#125; 配置axpects.xml 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;x&quot; /&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;bean id=&quot;logAspect&quot; class=&quot;x.aspect.LogAspect&quot;&gt; &lt;!-- configure properties of x.aspect here as normal --&gt; &lt;/bean&gt; &lt;!-- more bean definitions for data access objects go here --&gt;&lt;/beans&gt; 实现App 12345678910111213141516public class App &#123; public static void main(String[] args) &#123; // create and configure beans ApplicationContext context = new ClassPathXmlApplicationContext(&quot;aspects.xml&quot;, &quot;daos.xml&quot;, &quot;services.xml&quot;); // retrieve configured instance UserServiceImpl service = context.getBean(&quot;userService&quot;, UserServiceImpl.class); // use configured instance List&lt;User&gt; userList = service.findUserList(); // print info from beans userList.forEach(a -&gt; System.out.println(a.getUserName() + &quot;,&quot; + a.getAge())); &#125;&#125; 项目结构 简化上述项目的配置 通过config简化配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@EnableAspectJAutoProxy@Configurationpublic class BeanConfig &#123; /** * @return userDao */ @Bean(&quot;userDao&quot;) public UserDaoImpl userDao()&#123; return new UserDaoImpl(); &#125; /** * @return user service */ @Bean(&quot;userService&quot;) public UserServiceImpl userService() &#123; UserServiceImpl userService = new UserServiceImpl(); userService.setUserDao(userDao()); return userService; &#125; /** * @return log aspect */ @Bean(&quot;logAspect&quot;) public LogAspect logAspect() &#123; return new LogAspect(); &#125;&#125;public class App &#123; public static void main(String[] args) &#123; // create and configure beans AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(BeanConfig.class); // retrieve configured instance UserServiceImpl service = context.getBean(&quot;userService&quot;, UserServiceImpl.class); // use configured instance List&lt;User&gt; userList = service.findUserList(); // print info from beans userList.forEach(a -&gt; System.out.println(a.getUserName() + &quot;,&quot; + a.getAge())); &#125;&#125; 这种简化方式，没创建一个Bean都需要在config文件中去添加相应的映射。 ⭐注解简化配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Configuration@EnableAspectJAutoProxypublic class BeanConfig &#123;&#125;// UserDaoImpl 添加@Repository注解@Repositorypublic class UserDaoImpl &#123; /** * forged a query instance of the findUserList method */ public List&lt;User&gt; findUserList()&#123; return Collections.singletonList(new User(&quot;wey&quot;,18)); &#125;&#125;// UserServiceImpl添加了@Service注解，并通过@Autowired注解注入userDao@Servicepublic class UserServiceImpl &#123; /** * user dao impl. */ @Autowired private UserDaoImpl userDao; /** * find user list. * @return user list */ public List&lt;User&gt; findUserList() &#123; return userDao.findUserList(); &#125;&#125;// apppublic class App &#123; public static void main(String[] args) &#123; // create and configure beans AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext( &quot;x&quot;); // retrieve configured instance UserServiceImpl service = context.getBean(UserServiceImpl.class); // use configured instance List&lt;User&gt; userList = service.findUserList(); // print info from beans userList.forEach(a -&gt; System.out.println(a.getUserName() + &quot;,&quot; + a.getAge())); &#125;&#125; 🌟SpringBoot托管 SpringBoot实际上通过约定大于配置的方式，使用xx-starter统一的对Bean进行默认初始化，用户只需要很少的配置就可以进行开发了。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://solitaire-12.github.io/categories/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://solitaire-12.github.io/tags/spring/"}]},{"title":"Spring|基础|框架的组成","slug":"spring/spring框架解析","date":"2024-03-25T07:31:00.000Z","updated":"2024-03-25T08:35:08.642Z","comments":true,"path":"2024/03/25/spring/spring框架解析/","permalink":"https://solitaire-12.github.io/2024/03/25/spring/spring%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90/","excerpt":"","text":"Spring框架的特性与优势 特性 非侵入式：基于Spring开发的应用中的对象可以不依赖于Spring的API 控制反转：IOC（Inversion of control），指的是将对象的创建权交给Spring去创建。使用Spring之前，对象的创建都是由我们在代码new出来的，使用Spring之后，对象的创建就可以交给Spring框架。 依赖注入：DI（Dependency Injection），是指依赖的对象不需要手动的调用set方法去设置，而是通过配置赋值。 面向切面：AOP（Aspect Oriented Programming） 容器：Spring是一个容器，以为它包含并且管理引用对象的生命周期 组件化：Spring实现了使用简单的组件配置组合成一个复杂的应用。在Spring中可以使用XML和Java注解组合这些对象。 一站式： 在 IOC 和 AOP 的基础上可以整合各种企业应用的开源框架和优秀的第三方类库（实际上 Spring 自身也提供了表现层的 Spring MVC 和持久层的 Spring JDBC） 好处 Spring 可以使开发人员使用 POJOs 开发企业级的应用程序。只使用 POJOs 的好处是你不需要一个 EJB 容器产品，比如一个应用程序服务器，但是你可以选择使用一个健壮的 servlet 容器，比如 Tomcat 或者一些商业产品。 Spring 在一个单元模式中是有组织的。即使包和类的数量非常大，你只要担心你需要的，而其它的就可以忽略了。 Spring 不会让你白费力气做重复工作，它真正的利用了一些现有的技术，像 ORM 框架、日志框架、JEE、Quartz 和 JDK 计时器，其他视图技术。 测试一个用 Spring 编写的应用程序很容易，因为环境相关的代码被移动到这个框架中。此外，通过使用 JavaBean-style POJOs，它在使用依赖注入注入测试数据时变得更容易。 Spring 的 web 框架是一个设计良好的 web MVC 框架，它为比如 Structs 或者其他工程上的或者不怎么受欢迎的 web 框架提供了一个很好的供替代的选择。MVC 模式导致应用程序的不同方面(输入逻辑，业务逻辑和UI逻辑)分离，同时提供这些元素之间的松散耦合。模型(Model)封装了应用程序数据，通常它们将由 POJO 类组成。视图(View)负责渲染模型数据，一般来说它生成客户端浏览器可以解释 HTML 输出。控制器(Controller)负责处理用户请求并构建适当的模型，并将其传递给视图进行渲染。 Spring 对 JavaEE 开发中非常难用的一些 API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。 轻量级的 IOC 容器往往是轻量级的，例如，特别是当与 EJB 容器相比的时候。这有利于在内存和 CPU 资源有限的计算机上开发和部署应用程序。 Spring 提供了一致的事务管理接口，可向下扩展到（使用一个单一的数据库，例如）本地事务并扩展到全局事务（例如，使用 JTA） 组件 1️⃣Core Container(Spring核心容器) Beans模块：提供了框架的基础部分，包括控制反转和依赖注入。 Core模块：封装了Spring框架的底层部分，包括资源访问、类型转换以及一些常用的工具类。 Context上下文模块：建立在Core和Beans两个模块的基础之上，集成Beans模块的功能并添加了资源绑定、数据验证、国际化、Java EE支持、容器生命周期、事件传播等。ApplicationContext接口是上下文模块的焦点。 SpEl：提供了强大的表达式语言支持，支持访问和修改属性值，方法调用，支持访问及修改数组、容器和索引器，命名变量，支持算数和逻辑运算，支持从Spring容器获取Bean，也支持列表投影、悬着和一般的列表聚合。 2️⃣Data Access/Integration（数据访问/集成） JDBC模块：提供了一个JBDC的样例模块，使用这些模块能消除传统冗长的JDBC编码还有必须的事务控制，而且能享受到Spring管理事务的好处。 ORM模块：提供与流行的&quot;对象-关系&quot;映射框架无缝集成的API，包括JPA、JDO、Hibernate和Mybatis等。而且还可以使用Spring事务管理。 OXM模块：提供了一个支持Object/XML映射的抽象层实现。如JAXB、Castor、XMLBeans、JiBX和XStream。将Java对象映射成XML数据，或者将XML数据映射成Java对象。 JMS模块：指Java消息服务，提供一套&quot;消息生产者/消费者&quot;模块用于更简单的使用JMS，JMS用于两个应用程序之间，或分布式系统中发送消息，进行异步通信。 Transaction事务模块：支持编程和声明式事务管理。 3️⃣Web模块 Web模块：提供了基本的Web开发集成特性，例如多文件上传功能，使用的Servlet监听器和IOC容器初始化以及Web应用上下文。 Servlet模块：提供了一个Spring MVC Web框架的是心啊。Spring MVC框架提供了基于注解的请求资源注入，更简单的数据绑定、数据验证等及一套非常易用的JSP标签，完全无缝与Spring其他技术协作。 Web Socket模块：提供了简单的接口，用户只要实现响应的接口就可以快速搭建Web Socket Server，从而实现双向通讯。 Webfulx模块：Spring WebFlux是Spring Framework5.x中引入的新的响应式web框架。和Spring MVC不同，它不需要Servlet API，是完全异步非阻塞的，并且通过Reactor实现了Reactive Streams规范。Spring WebFlux用于创建基于事件循环执行模型的完全异步且非阻塞的应用程序。 Portlet模块：提供了在Portlet环境中使用MVC实现，类似Web-Servlet模块的功能。 （此外Spring4.x中还有Portlet 模块，在Spring 5.x中已经移除 。） 4️⃣AOP、Aspect、Instrumentation和Messaging AOP模块：提供了面向切面编程实现，提供比如日志记录、权限控制、线程统计等通用功能和业务逻辑分离的技术，并且能动态的把这些功能添加到需要的代码中，这样各司其职，降低业务逻辑和通用功能的解耦。 Aspects模块：提供与AspectJ的集成，是一个功能强大且成熟的面向切面编程的（AOP）框架。 Instrumentation模块：提供了类工具的支持和类加载器的实现，可以在特定的应用服务器中使用。 messaging模块：Spring 4.0以后新增了雄安锡（Spring-messaging）模块，该模块提供了对消息传递体系结构的协议的支持。 jcl模块：Spring 5.x中新增了日志框架集成的模块。 5️⃣Test模块 Test 模块：Spring 支持 Junit 和 TestNG 测试框架，而且还额外提供了一些基于 Spring 的测试功能，比如在测试 Web 框架时，模拟 Http 请求的功能。 包含Mock Objects, TestContext Framework, Spring MVC Test, WebTestClient。","categories":[{"name":"Spring","slug":"Spring","permalink":"https://solitaire-12.github.io/categories/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://solitaire-12.github.io/tags/spring/"}]},{"title":"Spring|汇总","slug":"spring/spring","date":"2024-03-25T07:30:00.000Z","updated":"2024-03-25T08:52:02.660Z","comments":true,"path":"2024/03/25/spring/spring/","permalink":"https://solitaire-12.github.io/2024/03/25/spring/spring/","excerpt":"","text":"spring框架解析 spring项目 基础IOC 基础AOP 基础 - MVC请求流程 // todo 进阶 - IOC实现原理 体系架构 整体功能 结构设计 BeanFactory和BeanRegistry IOC容器功能规范和Bean的注册 Spring Bean的创建时典型的工厂模式，这一系列的Bean工厂，也就是IOC容器。为对象间的依赖关系提供了很多便利和基础服务，在Spring中有许多的IOC容器的实现提供用户选择和使用，这是IOC容器的基础；在顶层的结构设计主要围绕着BeanFactory和xxRegistry进行： BeanFactory：工厂模式定义了IOC容器的基本功能规范。 BeanRegistory：向IOC容器手工注册BeanDefinition对象的方法。 BeanFactory作为最顶层的一个接口类，它定义了IOC容器的基本功能规范，BeanFactory 有三个子类：ListableBeanFactory、HierarchicalBeanFactory 和AutowireCapableBeanFactory。我们看下BeanFactory接口： 1234567891011121314151617181920212223242526272829303132333435public interface BeanFactory &#123; //用于取消引用实例并将其与FactoryBean创建的bean区分开来。例如，如果命名的bean是FactoryBean，则获取将返回Factory，而不是Factory返回的实例。 String FACTORY_BEAN_PREFIX = &quot;&amp;&quot;; //根据bean的名字和Class类型等来得到bean实例 Object getBean(String name) throws BeansException; Object getBean(String name, Class requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; &lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException; //返回指定bean的Provider &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(Class&lt;T&gt; requiredType); &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(ResolvableType requiredType); //检查工厂中是否包含给定name的bean，或者外部注册的bean boolean containsBean(String name); //检查所给定name的bean是否为单例/原型 boolean isSingleton(String name) throws NoSuchBeanDefinitionException; boolean isPrototype(String name) throws NoSuchBeanDefinitionException; //判断所给name的类型与type是否匹配 boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException; //获取给定name的bean的类型 @Nullable Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; //返回给定name的bean的别名 String[] getAliases(String name); &#125; ❓BeanFactory为何要定义这么多层次的接口？定义了哪些接口？ 主要是为了区分在 Spring 内部在操作过程中对象的传递和转化过程中，对对象的数据访问所做的限制。 ListableBeanFactory：该接口定义了访问容器中 Bean 基本信息的若干方法，如查看Bean 的个数、获取某一类型 Bean 的配置名、查看容器中是否包括某一 Bean 等方法； HierarchicalBeanFactory：父子级联 IoC 容器的接口，子容器可以通过接口方法访问父容器； 通过 HierarchicalBeanFactory 接口， Spring 的 IoC 容器可以建立父子层级关联的容器体系，子容器可以访问父容器中的 Bean，但父容器不能访问子容器的 Bean。Spring 使用父子容器实现了很多功能，比如在 Spring MVC 中，展现层 Bean 位于一个子容器中，而业务层和持久层的 Bean 位于父容器中。这样，展现层 Bean 就可以引用业务层和持久层的 Bean，而业务层和持久层的 Bean 则看不到展现层的 Bean。 ConfigurableBeanFactory：是一个重要的接口，增强了 IoC 容器的可定制性，它定义了设置类装载器、属性编辑器、容器初始化后置处理器等方法； ConfigurableListableBeanFactory: ListableBeanFactory 和 ConfigurableBeanFactory的融合； AutowireCapableBeanFactory：定义了将容器中的 Bean 按某种规则（如按名字匹配、按类型匹配等）进行自动装配的方法； ❓如何将Bean注册到BeanFactory中？ Spring 配置文件中每一个&lt;bean&gt;节点元素在 Spring 容器里都通过一个 BeanDefinition 对象表示，它描述了 Bean 的配置信息。而 BeanDefinitionRegistry 接口提供了向容器手工注册 BeanDefinition 对象的方法。 BeanDefinniton 各种Bean对象及其相互的关系 Bean对象存在依赖嵌套等关系，所以设计者设计了BeanDefinition，它用来对Bean对象及关系定义；我们在理解时只需要抓住如下三个要点： BeanDefinition 定义了各种Bean对象及其相互的关系 BeanDefinitionReader 这是BeanDefinition的解析器 BeanDefinitionHolder 这是BeanDefination的包装类，用来存储BeanDefinition，name以及aliases等。 BeanDefinition SpringIOC容器管理了我们定义的各种Bean对象及其相互的关系，Bean对象在Spring实现中是以BeanDefinition来描述的，其继承体系如下 ： BeanDefinitionReader Bean 的解析过程非常复杂，功能被分的很细，因为这里需要被扩展的地方很多，必须保证有足够的灵活性，以应对可能的变化。Bean 的解析主要就是对 Spring 配置文件的解析。这个解析过程主要通过下图中的类完成： BeanDefinitionHolder BeanDefinitionHolder 这是BeanDefination的包装类，用来存储BeanDefinition，name以及aliases等。 ApplicationContext IOC接口设计和实现 IoC容器的接口类是ApplicationContext，很显然它必然继承BeanFactory对Bean规范（最基本的ioc容器的实现）进行定义。而ApplicationContext表示的是应用的上下文，除了对Bean的管理外，还至少应该包含了 访问资源： 对不同方式的Bean配置（即资源）进行加载。(实现ResourcePatternResolver接口) 国际化: 支持信息源，可以实现国际化。（实现MessageSource接口） 应用事件: 支持应用事件。(实现ApplicationEventPublisher接口 接口设计： HierarchicalBeanFactory 和 ListableBeanFactory： ApplicationContext 继承了 HierarchicalBeanFactory 和 ListableBeanFactory 接口，在此基础上，还通过多个其他的接口扩展了 BeanFactory 的功能： ApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。实现了 ApplicationListener 事件监听接口的 Bean 可以接收到容器事件 ， 并对事件进行响应处理 。 在 ApplicationContext 抽象实现类AbstractApplicationContext 中，我们可以发现存在一个 ApplicationEventMulticaster，它负责保存所有监听器，以便在容器产生上下文事件时通知这些事件监听者。 MessageSource：为应用提供 i18n 国际化消息访问的功能； ResourcePatternResolver ： 所 有 ApplicationContext 实现类都实现了类似于PathMatchingResourcePatternResolver 的功能，可以通过带前缀的 Ant 风格的资源文件路径装载 Spring 的配置文件。 LifeCycle：该接口是 Spring 2.0 加入的，该接口提供了 start()和 stop()两个方法，主要用于控制异步处理过程。在具体使用时，该接口同时被 ApplicationContext 实现及具体 Bean 实现， ApplicationContext 会将 start/stop 的信息传递给容器中所有实现了该接口的 Bean，以达到管理和控制 JMX、任务调度等目的。 接口实现： 第一，从类结构设计上看， 围绕着是否需要Refresh容器衍生出两个抽象类： GenericApplicationContext： 是初始化的时候就创建容器，往后的每次refresh都不会更改 AbstractRefreshableApplicationContext： AbstractRefreshableApplicationContext及子类的每次refresh都是先清除已有(如果不存在就创建)的容器，然后再重新创建；AbstractRefreshableApplicationContext及子类无法做到GenericApplicationContext混合搭配从不同源头获取bean的定义信息 第二， 从加载的源来看（比如xml,groovy,annotation等）， 衍生出众多类型的ApplicationContext, 典型比如: FileSystemXmlApplicationContext： 从文件系统下的一个或多个xml配置文件中加载上下文定义，也就是说系统盘符中加载xml配置文件。 ClassPathXmlApplicationContext： 从类路径下的一个或多个xml配置文件中加载上下文定义，适用于xml配置的方式。 AnnotationConfigApplicationContext： 从一个或多个基于java的配置类中加载上下文定义，适用于java注解的方式。 ConfigurableApplicationContext： 扩展于 ApplicationContext，它新增加了两个主要的方法： refresh()和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力。在应用上下文关闭的情况下调用 refresh()即可启动应用上下文，在已经启动的状态下，调用 refresh()则清除缓存并重新装载配置信息，而调用close()则可关闭应用上下文。这些接口方法为容器的控制管理带来了便利，但作为开发者，我们并不需要过多关心这些方法。 第三， 更进一步理解： 设计者在设计时AnnotationConfigApplicationContext为什么是继承GenericApplicationContext？ 因为基于注解的配置，是不太会被运行时修改的，这意味着不需要进行动态Bean配置和刷新容器，所以只需要GenericApplicationContext。 而基于XML这种配置文件，这种文件是容易修改的，需要动态性刷新Bean的支持，所以XML相关的配置必然继承AbstractRefreshableApplicationContext； 且存在多种xml的加载方式（位置不同的设计），所以必然会设计出AbstractXmlApplicationContext, 其中包含对XML配置解析成BeanDefination的过程。 那么细心的你从上图可以发现AnnotationWebConfigApplicationContext却是继承了AbstractRefreshableApplicationContext而不是GenericApplicationContext， 为什么AnnotationWebConfigApplicationContext继承自AbstractRefreshableApplicationContext呢 ？ 因为用户可以通过ApplicationContextInitializer来设置contextInitializerClasses（context-param / init-param）， 在这种情况下用户倾向于刷新Bean的，所以设计者选择让AnnotationWebConfigApplicationContext继承了AbstractRefreshableApplicationContext。（如下是源码中Spring设计者对它的解释） 1234567* &lt;p&gt;As an alternative to setting the &quot;contextConfigLocation&quot; parameter, users may* implement an &#123;@link org.springframework.context.ApplicationContextInitializer* ApplicationContextInitializer&#125; and set the* &#123;@linkplain ContextLoader#CONTEXT_INITIALIZER_CLASSES_PARAM &quot;contextInitializerClasses&quot;&#125;* context-param / init-param. In such cases, users should favor the &#123;@link #refresh()&#125;* and &#123;@link #scan(String...)&#125; methods over the &#123;@link #setConfigLocation(String)&#125;* method, which is primarily for use by &#123;@code ContextLoader&#125;. 把之前的设计要点和设计结构结合起来看： 🔗 Spring IOC实现原理详解之IOC体系结构设计 初始化流程 //todo 进阶 - AOP实现原理 // todo 文献： 官方使用文档：Spring Framework Spring归档文件：Index of /spring-framework/docs Spring源码地址：GitHub - spring-projects/spring-framework: Spring Framework","categories":[{"name":"Spring","slug":"Spring","permalink":"https://solitaire-12.github.io/categories/Spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://solitaire-12.github.io/tags/spring/"}]},{"title":"Redis|集群","slug":"redis/集群","date":"2024-03-25T05:10:55.000Z","updated":"2024-03-25T05:52:36.611Z","comments":true,"path":"2024/03/25/redis/集群/","permalink":"https://solitaire-12.github.io/2024/03/25/redis/%E9%9B%86%E7%BE%A4/","excerpt":"","text":"未完成 集群 Redis集群是Redis提供的分布式数据库方案，集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。 节点 槽指派 执行命令 重新分片 ASK错误 复制和转移 消息","categories":[{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"},{"name":"高可用","slug":"高可用","permalink":"https://solitaire-12.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"Redis|哨兵","slug":"redis/哨兵","date":"2024-03-25T05:10:00.000Z","updated":"2024-03-25T05:51:19.762Z","comments":true,"path":"2024/03/25/redis/哨兵/","permalink":"https://solitaire-12.github.io/2024/03/25/redis/%E5%93%A8%E5%85%B5/","excerpt":"","text":"Sentinel Sentinel是redis的高可用性(high availability)解决方案：由一个或者多个Sentinel实例(instance)组成的Sentinel系统，可以监视任意多个主服务器，以及这些主服务器属下的所用从服务器，并在被监视的主服务器进入下线状态时，自动将下线的主服务器属下的某个从服务器升级为新的主服务器，然后由新的主机服务器代替已下线的主服务器继续处理命令请求。 启动并初始化 当一个Sentinel启动时，它需要执行以下步骤： 初始化服务器。 将普通Redis服务器使用的代码替换成Sentinel专用代码。 初始化Sentinel状态。 根据给定的配置文件，初始化Sentinel的监视主服务器列表。 创建连向主服务器的网络连接。 初始化服务器 sentinel本质上只是一个运行在特殊模式下的redis服务器，所以启动sentinel的第一步是初始化一个普通的redis服务器。只不过这个服务器不会载入RDB文件或者AOF文件。 使用专用代码 例如sentinel.c文件中定义了默认的sentinel端口#define REDIS_SENTINEL_PORT 26379； 除此之外：普通服务器使用的命令表（service.c文件中【6.0.6源码】）： sentinel中的指令表：sentinel.c文件中 初始化sentinel 在应用了sentinel的专用代码之后，接下来，服务器会初始化一个sentinel.c/sentinelState，简称&quot;sentinel状态&quot;。这个结构保存了服务器中所有和sentinel功能有关的状态。 12345678910111213141516171819202122/* Main state. */struct sentinelState &#123; char myid[CONFIG_RUN_ID_SIZE+1]; /* This sentinel ID. */ // 用于实现故障转移 uint64_t current_epoch; /* Current epoch. */ dict *masters; /* Dictionary of master sentinelRedisInstances. Key is the instance name, value is the sentinelRedisInstance structure pointer. */ int tilt; /* Are we in TILT mode? */ int running_scripts; /* Number of scripts in execution right now. */ mstime_t tilt_start_time; /* When TITL started. */ mstime_t previous_time; /* Last time we ran the time handler. */ // 一个FIFO队列，包含所有需要执行的用户脚本 list *scripts_queue; /* Queue of user scripts to execute. */ char *announce_ip; /* IP addr that is gossiped to other sentinels if not NULL. */ int announce_port; /* Port that is gossiped to other sentinels if non zero. */ unsigned long simfailure_flags; /* Failures simulation. */ int deny_scripts_reconfig; /* Allow SENTINEL SET ... to change script paths at runtime? */&#125; sentinel; 初始化masters属性 sentinel状态中的masters字典记录了所有被sentinel坚实的主服务器的相关信息，其中 字典的键是被监视主服务器的名字 而字典的值则是被监视主服务器对应的sentinel.c/sentinelRedisInstance 每个sentinelRedisInstance结构(实例结构)代表一个被sentinel监视的redis服务器实例(instance)，这个实例可以是主服务器、从服务器或者另外一个sentinel。 实例结构包含的属性非常多，源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586typedef struct sentinelRedisInstance &#123; // 标记，记录了实例的类型以及该实例的当前状态 int flags; /* See SRI_... defines */ /* 实例名称 * 主服务器的名字由用户在配置文件中设置 * 从服务器以及sentinel的名字由sentinel自动设置，一般格式为IP:port,例 * 如&#x27;127.0.0.1:26379&#x27; */ char *name; /* Master name from the point of view of this sentinel. */ char *runid; /* Run ID of this instance, or unique ID if is a Sentinel.*/ uint64_t config_epoch; /* Configuration epoch. */ sentinelAddr *addr; /* Master host. */ instanceLink *link; /* Link to the instance, may be shared for Sentinels. */ mstime_t last_pub_time; /* Last time we sent hello via Pub/Sub. */ mstime_t last_hello_time; /* Only used if SRI_SENTINEL is set. Last time we received a hello from this Sentinel via Pub/Sub. */ mstime_t last_master_down_reply_time; /* Time of last reply to SENTINEL is-master-down command. */ mstime_t s_down_since_time; /* Subjectively down since time. */ mstime_t o_down_since_time; /* Objectively down since time. */ /* sentinel down-after-milliseconds &lt;master-name&gt; &lt;number&gt; 选项设置的值 * 实例无响应多少毫秒之后才会被判断为主观下线(subjectively down) */ mstime_t down_after_period; /* Consider it down after that period. */ mstime_t info_refresh; /* Time at which we received INFO output from it. */ dict *renamed_commands; /* Commands renamed in this instance: Sentinel will use the alternative commands mapped on this table to send things like SLAVEOF, CONFING, INFO, ... */ /* Role and the first time we observed it. * This is useful in order to delay replacing what the instance reports * with our own configuration. We need to always wait some time in order * to give a chance to the leader to report the new configuration before * we do silly things. */ int role_reported; mstime_t role_reported_time; mstime_t slave_conf_change_time; /* Last time slave master addr changed. */ /* Master specific. */ dict *sentinels; /* Other sentinels monitoring the same master. */ dict *slaves; /* Slaves for this master instance. */ /* 判断这个实例为客观下线(objectively down)所需的支持投票数 * 由sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;port&gt; &lt;quorum&gt;设定 */ unsigned int quorum;/* Number of sentinels that need to agree on failure. */ /* 在执行故障转移操作时，可以同时对新的主服务器进行同步的从服务器数量 * 由sentinel parallel_syncs &lt;master-name&gt; &lt;number&gt;设定 */ int parallel_syncs; /* How many slaves to reconfigure at same time. */ char *auth_pass; /* Password to use for AUTH against master &amp; replica. */ char *auth_user; /* Username for ACLs AUTH against master &amp; replica. */ /* Slave specific. */ mstime_t master_link_down_time; /* Slave replication link down time. */ int slave_priority; /* Slave priority according to its INFO output. */ mstime_t slave_reconf_sent_time; /* Time at which we sent SLAVE OF &lt;new&gt; */ struct sentinelRedisInstance *master; /* Master instance if it&#x27;s slave. */ char *slave_master_host; /* Master host as reported by INFO */ int slave_master_port; /* Master port as reported by INFO */ int slave_master_link_status; /* Master link status as reported by INFO */ unsigned long long slave_repl_offset; /* Slave replication offset. */ /* Failover */ char *leader; /* If this is a master instance, this is the runid of the Sentinel that should perform the failover. If this is a Sentinel, this is the runid of the Sentinel that this Sentinel voted as leader. */ uint64_t leader_epoch; /* Epoch of the &#x27;leader&#x27; field. */ uint64_t failover_epoch; /* Epoch of the currently started failover. */ int failover_state; /* See SENTINEL_FAILOVER_STATE_* defines. */ mstime_t failover_state_change_time; mstime_t failover_start_time; /* Last failover attempt start time. */ /* 刷新故障迁移状态的最大时限 * 由sentinel failover_timeout &lt;master-name&gt; &lt;ms&gt;设定 */ mstime_t failover_timeout; /* Max time to refresh failover state. */ mstime_t failover_delay_logged; /* For what failover_start_time value we logged the failover delay. */ struct sentinelRedisInstance *promoted_slave; /* Promoted slave instance. */ /* Scripts executed to notify admin or reconfigure clients: when they * are set to NULL no script is executed. */ char *notification_script; char *client_reconfig_script; sds info; /* cached INFO output */&#125; sentinelRedisInstance; 其中sentinelRedisInstance.addr属性是一个指向sentinel.c/sentinelAddr结构的指针，这个结构保存着实例的IP地址和端口号： 12345/* Address object, used to describe an ip:port pair. */typedef struct sentinelAddr &#123; char *ip; int port;&#125; sentinelAddr; 对Sentinel状态的初始化将引发对masters字典的初始化，而masters字典的初始化是根据被载入的Sentinel配置文件来进行的。 创建连向主服务器的网络连接 初始化Sentinel的最后一步是创建连向被监视主服务器的网络连接，Sentinel将成为主服务器的客户端，它可以向主服务器发送命令，并从命令回复中获取相关的信息。 初始化Sentinel的最后一步是创建连向被监视主服务器的网络连接，Sentinel将成为主服务器的客户端，它可以向主服务器发送命令，并从命令回复中获取相关的信息。 一个是命令连接，这个连接专门用于向主服务器发送命令，并接收命令回复。 另一个是订阅连接，这个连接专门用于订阅主服务器的__sentinel__:hello频道 为什么有两个连接？在Redis目前的发布与订阅功能中，被发送的信息都不会保存在Redis服务器里面，如果在信息发送时，想要接收信息的客户端不在线或者断线，那么这个客户端就会丢失这条信息。因此，为了不丢失__sentinel__:hello频道的任何信息，Sentinel必须专门用一个订阅连接来接收该频道的信息。另一方面，除了订阅频道之外，Sentinel还必须向主服务器发送命令，以此来与主服务器进行通信，所以Sentinel还必须向主服务器创建命令连接。因为Sentinel需要与多个实例创建多个网络连接，所以Sentinel使用的是异步连接。 获取主服务信息 Sentinel默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送INFO命令，并通过分析INFO命令的回复来获取主服务器的当前信息。 🔎例子： 主服务器master有三个从服务器slave0、slave1和slave2，并且一个Sentinel正在连接主服务器，那么Sentinel将持续地向主服务器发送INFO命令，并获得类似于以下内容的回复： 12345678910111213# Server...run_id:7611c59dc3a29aa6fa0609f841bb6a1019008a9c...# Replicationrole:master...slave0:ip=127.0.0.1,port=11111,state=online,offset=43,lag=0slave1:ip=127.0.0.1,port=22222,state=online,offset=43,lag=0slave2:ip=127.0.0.1,port=33333,state=online,offset=43,lag=0...# Other sections... 通过分析主服务器返回的INFO命令回复，Sentinel可以获取以下两方面的信息： 关于主服务器本身的信息，包括run_id域记录的服务器运行ID，以及role域记录的服务器角色； 关于主服务器属下所有从服务器的信息，每个从服务器都由一个&quot;slave&quot;字符串开头的行记录，每行的ip=域记录了从服务器的IP地址，而port=域则记录了从服务器的端口号。根据这些IP地址和端口号，Sentinel无须用户提供从服务器的地址信息，就可以自动发现从服务器。 根据run_id域和role域记录的信息，Sentinel将对主服务器的实例结构进行更新，例如，主服务器重启之后，它的运行ID就会和实例结构之前保存的运行ID不同，Sentinel检测到这一情况之后，就会对实例结构的运行ID进行更新。 至于主服务器返回的从服务器信息，则会被用于更新主服务器实例结构的slaves字典，这个字典记录了主服务器属下从服务器的名单： 字典的键是由Sentinel自动设置的从服务器名字，格式为ip:port：如对于IP地址为127.0.0.1，端口号为11111的从服务器来说，Sentinel为它设置的名字就是127.0.0.1:11111。 至于字典的值则是从服务器对应的实例结构：比如说，如果键是127.0.0.1:11111，那么这个键的值就是IP地址为127.0.0.1，端口号为11111的从服务器的实例结构。 Sentinel在分析INFO命令中包含的从服务器信息时，会检查从服务器对应的实例结构是否已经存在于slaves字典： 如果从服务器对应的实例结构已经存在，那么Sentinel对从服务器的实例结构进行更新。 如果从服务器对应的实例结构不存在，那么说明这个从服务器是新发现的从服务器，Sentinel会在slaves字典中为这个从服务器新创建一个实例结构。 对于之前列举的主服务器master和三个从服务器slave0、slave1和slave2的例子来说，Sentinel将分别为三个从服务器创建它们各自的实例结构，并将这些结构保存到主服务器实例结构的slaves字典里面： 注意对比图中主服务器实例结构和从服务器实例结构之间的区别： 主服务器实例结构的flags属性的值为SRI_MASTER，而从服务器实例结构的flags属性的值为SRI_SLAVE。 主服务器实例结构的name属性的值是用户使用Sentinel配置文件设置的，而从服务器实例结构的name属性的值则是Sentinel根据从服务器的IP地址和端口号自动设置的。 获取从服务器信息 当Sentinel发现主服务器有新的从服务器出现时，Sentinel除了会为这个新的从服务器创建相应的实例结构之外，Sentinel还会创建连接到从服务器的命令连接和订阅连接。 🔎例子： 在创建命令连接之后，Sentinel在默认情况下，会以每十秒一次的频率通过命令连接向从服务器发送INFO命令，并获得类似于以下内容的回复： 12345678910111213# Server...run_id:32be0699dd27b410f7c90dada3a6fab17f97899f...# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upslave_repl_offset:11887slave_priority:100# Other sections... 根据INFO命令的回复，Sentinel会提取出以下信息： 从服务器的运行ID run_id。 从服务器的角色role。 主服务器的IP地址master_host，以及主服务器的端口号master_port。 主从服务器的连接状态master_link_status。 从服务器的优先级slave_priority。 从服务器的复制偏移量slave_repl_offset。 根据这些信息，Sentinel会对从服务器的实例结构进行更新，更新之后的结构样子： 向主服务器和从服务器发送信息 在默认情况下，Sentinel会以每两秒一次的频率，通过命令连接向所有被监视的主服务器和从服务器发送以下格式的命令： 1PUBLISH __sentinel__:hello &quot;＜s_ip＞,＜s_port＞,＜s_runid＞,＜s_epoch＞,＜m_name＞,＜m_ip＞,＜m_port＞,＜m_epoch＞&quot; 这条命令向服务器的__sentinel__:hello频道发送了一条信息，信息的内容由多个参数组成： 以s_开头的参数记录的是Sentinel本身的信息 m_开头的参数记录的则是主服务器的信息。如果Sentinel正在监视的是主服务器，那么这些参数记录的就是主服务器的信息；如果Sentinel正在监视的是从服务器，那么这些参数记录的就是从服务器正在复制的主服务器的信息 参数表： 参数 意义 a_ip Sentinel的IP地址 a_port Sentinel的端口号 s_runid Sentinel的运行ID s_epoch Sentinel当前的Configuration epoch【配置纪元】 m_name 主服务器的名字 m_ip 主服务器的IP地址 m_port 主服务器的端口号 m_epoch 主服务器当前的配置纪元 检查主观下线状态 在默认情况下，Sentinel会以每秒一次的频率向所有与它创建了命令连接的实例（包括主服务器、从服务器、其他Sentinel在内）发送PING命令，并通过实例返回的PING命令回复来判断实例是否在线。 如上图中，Sentinel1将向Sentinel2、主服务器master、从服务器slave1和slave2发送PING命令。Sentinel2将向Sentinel1、主服务器master、从服务器slave1和slave2发送PING命令。实例对ping命令的回复可以分成以下两种情况： 有效回复：实例返回+PONG、-LOADING、-MASTERDOWN三种回复其中一种。 无效回复：实例返回+PONG、-LOADING、-MASTERDOWN三种回复之外的其他回复，或者在指定时限内没有返回任何回复。 Sentinel配置文件中的down-after-milliseconds选项指定了Sentinel判断实例进入主观下线所需的时间长度：如果一个实例在down-after-milliseconds毫秒内，连续向Sentinel返回无效回复，那么Sentinel会修改这个实例所对应的实例结构，在结构的flags属性中打开SRI_S_DOWN标识，以此来表示这个实例已经进入主观下线状态。 用户设置的down-after-milliseconds选项的值，不仅会被Sentinel用来判断主服务器的主观下线状态，还会被用于判断主服务器属下的所有从服务器，以及所有同样监视这个主服务器的其他Sentinel的主观下线状态。举个例子，如果用户向Sentinel设置了以下配置： 12sentinel monitor master 127.0.0.1 6379 2sentinel down-after-milliseconds master 50000 那么50000毫秒不仅会成为Sentinel判断master进入主观下线的标准，还会成为Sentinel判断master属下所有从服务器，以及所有同样监视master的其他Sentinel进入主观下线的标准。 多个Sentinel设置的主观下线时长可能不同： down-after-milliseconds选项另一个需要注意的地方是，对于监视同一个主服务器的多个Sentinel来说，这些Sentinel所设置的down-after-milliseconds选项的值也可能不同，因此，当一个Sentinel将主服务器判断为主观下线时，其他Sentinel可能仍然会认为主服务器处于在线状态。举个例子，如果Sentinel1载入了以下配置： 12sentinel monitor master 127.0.0.1 6379 2sentinel down-after-milliseconds master 50000 而Sentinel2则载入了以下配置： 12sentinel monitor master 127.0.0.1 6379 2sentinel down-after-milliseconds master 10000 那么当master的断线时长超过10000毫秒之后，Sentinel2会将master判断为主观下线，而Sentinel1却认为master仍然在线。只有当master的断线时长超过50000毫秒之后，Sentinel1和Sentinel2才会都认为master进入了主观下线状态。 检查客观下线状态 当Sentinel将一个主服务器判断为主观下线之后，为了确认这个主服务器是否真的下线了，它会向同样监视这一主服务器的其他Sentinel进行询问，看它们是否也认为主服务器已经进入了下线状态（可以是主观下线或者客观下线）。当Sentinel从其他Sentinel那里接收到足够数量的已下线判断之后，Sentinel就会将从服务器判定为客观下线，并对主服务器执行故障转移操作。 Sentinel使用 SENTINEL is-master-down-by-addr ＜ip＞ ＜port＞ ＜current_epoch＞ ＜runid＞命令询问其他Sentinel是否同意主服务器已下线。 当目标Sentinel接收到源Sentinel发来的 SENTINEL is-master-down-by命令包时，目标Sentinel会分析并取出命令请求中包含的各个参数，并根据其中的主服务器IP和端口号，检查主服务器是否已下线，然后向源Sentinel返回一条包含三个三叔的MultiBulk回复。 根据其他Sentinel发回的SENTINEL is-master-down-by命令回复，Sentinel将统计其他Sentinel同意主服务器已下线的数量，当这一数量达到配置指定的判断客观下线所需要的数量时，Sentinel鬼将主服务器实例结构flags属性的SRI_O_DOWN标识打开，标识主服务器已经进入客观下线状态。 客观下线状态的判断条件： 当认为主服务器已经进入下线状态的Sentinel的数量，超过Sentinel配置中设置的quorum参数的值，那么该Sentinel就会认为主服务器已经进入客观下线状态。 比如，sentinel启动时加载下述配置： 1sentinel monitor master 127.0.0.1 6379 2 意味着 包括当前sentinel在内，只要总共有两个sentinel认为主服务器已经进入下线状态，那么当前sentinel就将主服务器判断为客观下线。 在同一个sentinel服务系统中，不同的sentinel实例可以有不同的判断客观下线条件。 选举领头Sentinel 当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个Sentinel会进行协商，选举出一个领头Sentinel，并由领头Sentinel对下线主服务器执行故障转移操作。 // 待续 故障转移 在选举产生出领头Sentinel之后，领头Sentinel将对已下线的主服务器执行故障转移操作，该操作包含以下三个步骤： 在已下线主服务器属下的所有从服务器里面，挑选出一个从服务器，并将其转换为主服务器。 让已下线主服务器属下的所有从服务器改为复制新的主服务器。 将已下线主服务器设置为新的主服务器的从服务器，当这个旧的主服务器重新上线时，它就会成为新的主服务器的从服务器。 // 待续 案例 🔍环境：redis 3.2.1/windows；目标：构建一主二从三哨兵的哨兵系统。 准备redis windows版本 redis windows社区 🔗笔者提供 提取码：emj2 redis官方 为啥redis官方没有提供windows版本呢 复制三份代码，其中一份做master，另外两份做slave master配置 redis.windows.conf 1port 6380 sentinel.conf 12345678910111213#当前Sentinel服务运行的端口port 26380#master#Sentinel去监视一个名为mymaster的主redis实例，这个主实例的IP地址为本机地址127.0.0.1，端口号为6379，#而将这个主实例判断为失效至少需要2个 Sentinel进程的同意，只要同意Sentinel的数量不达标，自动failover就不会执行sentinel monitor mymaster 127.0.0.1 6380 1#指定了Sentinel认为Redis实例已经失效所需的毫秒数。当 实例超过该时间没有返回PING，或者直接返回错误，那么Sentinel将这个实例标记为主观下线。#只有一个 Sentinel进程将实例标记为主观下线并不一定会引起实例的自动故障迁移：只有在足够数量的Sentinel都将一个实例标记为主观下线之后，实例才会被标记为客观下线，这时自动故障迁移才会执行sentinel down-after-milliseconds mymaster 5000#指定了在执行故障转移时，最多可以有多少个从Redis实例在同步新的主实例，在从Redis实例较多的情况下这个数字越小，同步的时间越长，完成故障转移所需的时间就越长sentinel config-epoch mymaster 12#如果在该时间（ms）内未能完成failover操作，则认为该failover失败sentinel leader-epoch mymaster 13 slave配置 redis.windows.conf 123# 另一台是6382port 6381slaveof 127.0.0.1 6380 sentinel.conf 12345678910#slave1 哨兵运行的端口port 26381#slave2 哨兵运行的端口#port 26382#slavesentinel monitor mymaster 127.0.0.1 6380 1sentinel down-after-milliseconds mymaster 5000sentinel config-epoch mymaster 12sentinel leader-epoch mymaster 13 运行 启动服务bat 123# 假如你使用记事本来编写这个bat，中文会乱码；推荐使用notepad+，将编码格式改为ANSItitle &quot;6380服务端&quot;redis-server.exe redis.windows.conf 哨兵启动bat 12title sentinel-6380redis-server.exe sentinel.conf --sentinel 客户端启动bat 12title &quot;6380客户端&quot;redis-cli.exe -h 127.0.0.1 -p 6380 连接到哨兵 12redis-cli.exe -h 127.0.0.1 -p 26381info replication 断开6380，可以观察到，某一台slave成为了主服务器；重新连接6380，变成从服务器 头脑风暴 ❓假设现在有一套redis sentinel服务系统，假设客户端向服务器发送命令，这时主服务器宕机了，这条命令会怎么样？ 在主服务器宕机期间，客户端可能会收到一些错误信息或超时提示，这是因为主服务器已经宕机无法处理请求。客户端可以选择等待一段时间后重新发送命令，或者连接到新的主服务器来继续执行操作： 服务器宕机前完全接收到该命令 在主服务器宕机后，哨兵模式会自动将新的主服务器与旧的从服务器进行同步，确保数据的一致性 服务器已经处理完毕 命令已经处理完，新的服务器就忽略这条命令 服务器未处理完毕 新选举出来的主服务器通过复制机制来获取到未完成的命令，然后执行。 服务器宕机前未能介绍到该命令 这种情况，sentinel 是没办法处理的，这属于数据持久化的范畴。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"},{"name":"高可用","slug":"高可用","permalink":"https://solitaire-12.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"Redis|主从","slug":"redis/主从","date":"2024-03-25T02:16:00.000Z","updated":"2024-03-25T04:06:09.979Z","comments":true,"path":"2024/03/25/redis/主从/","permalink":"https://solitaire-12.github.io/2024/03/25/redis/%E4%B8%BB%E4%BB%8E/","excerpt":"","text":"主从复制 在redis中，用户可以通过 slaveof命令或者设置slaveof选项，让一个服务器去复制(replicate)另一个服务器，我们将被复制的服务器称为主服务器(master)，而对主服务器进行复制的服务器称为从服务器(slave)。关系图如下： 🔎基础案例： 现在有两台redis服务器，分别是 127.0.0.1:6379和127.0.0.1:6380 在6380服务器指定为6379的从客户机 1slaveof 127.0.0.1 6379 进行复制中的主从服务器双方的数据库将保存相同的数据，概念上将这种现象称作“数据库状态一致”，或者简称“一致”。 现象： 在主服务器执行 set test 'hello' 在从服务器执行 get test 这时，从服务器是可以获取到这个test的数据的。 SYNC redis主从复制功能分为同步(sync)和命令传报(command propagate)两个操作： 同步操作用户将从服务器的数据库状态更新至主服务器当前所在的数据库状态。 命令传播操作则是用于保证主服务器的数据状态被修改时，使主从服务器的数据库重新回到一致状态。 同步 当客户端向从服务器发送slaveof命令的时候，要求从服务器复制主服务器时，从服务器首先需要执行同步操作。也就是：将从服务器的数据库状态更新至主服务器当前所在的数据库状态。 从服务器对主服务器的同步操作时，会向主服务器发送 SYNC命令，该命令的执行步骤如下： 从服务器向主服务器发送SYNC命令 收到SYNC命令的主服务器执行bgsave命令，在后台生成一个RDB文件，并使用一个缓冲区记录从现在开始执行的所有命令。 当主服务器的bgsave命令执行完毕时，主服务器会将RDB文件发送给从服务器，从服务器接收并载入这个RDB文件，将自己的数据库状态更新至主服务器执行bgsave命令时的数据库状态。 主服务器将记录在缓冲区里面的所有写命令发送给从服务器，从服务器执行这些写命令，将自己的数据库状态更新至主服务器数据库当前所处的状态。 命令传播 在同步操作执行完毕之后，主从数据库是一致的，当主服务器执行了其他写命令之后，会打破这种状态。故而，在主服务器执行写命令时，将这一命令发送给从服务器执行，从而使数据库状态达到一致。 缺陷 对处理主从服务断开后重复制的处理不是很理想，举个例子 从上述例子中，可以看出，主从服务器断开重连之后，SYNC会重新生成一个完整的RDB，对于从服务器来将，它只缺失了主从服务断开之后的数据而已，但是SYNC却是重新生成了一整个完整的RDB。 SYNC命令的缺陷 每次执行SYNC命令，主从服务器需要执行以下动作： 主服务器需要执行BGSAVE命令来生成RDB文件，这个生成操作会耗费主服务器大量的CPU、内存和磁盘I/O资源。 主服务器需要将自己生成的RDB文件发送给从服务器，这个发送操作会耗费主从服务器大量的网络资源（带宽和流量），并对主服务器响应命令请求的时间产生影响。 接收到RDB文件的从服务器需要载入主服务器发来的RDB文件，并且在载入期间，从服务器会因为阻塞而没办法处理命令请求。 ⭐PSYNC redis从2.8之后，使用PSYNC命令代替SYNC命令执行复制时的同步操作。 PSYNC命令具有完整重同步(full resynchronization)和部分重同步(partial resynchronization)两种模式： 完整重同步用于处理初次复制情况：完整重同步的执行步骤和SYNC命令的执行步骤基本一样，它们都是通过让主服务器创建并发送RDB文件，以及向从服务器发送保存在缓冲区里面的写命令来进行同步。 部分重同步则用于处理断线后重复制情况：当从服务器在断线后重新连接主服务器时，如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只要接收并执行这些写命令，就可以将数据库更新至主服务器当前所处的状态。 部分重同步 由一下三个部分组成： 主服务器的复制偏移量(replication offset)和从服务器的复制偏移量 主服务器的复制积压缓冲区(replication backlog) 服务器的运行ID(run ID) 复制偏移量 执行复制的双方-主服务器和从服务器会分别维护一个复制偏移量： 主服务器，每次向从服务器传播N个字节的数据时，就将自己的复制偏移量的值加上N 从服务器，每次接收到主服务器传播来的N个字节的数据时，就将自己的复制偏移量的值加上N 通过对比主从服务器的复制偏移量，程序就可以直到从服务器是否处于一致状态：如果主从服务器处于一致状态，那么主从服务器两者的偏移量总是相同的；如果偏移量并不相同，那么主从服务器不一致。 ❓从上图中，服务器A断线之后，立即重连，向主服务器发送了自己的偏移量。那么主服务器应该对从服务器执行完整重同步还是部分重同步？如果是部分重同步的话，主服务器是如何补偿从服务器A在断线期间丢失的数据？ 复制积压缓冲区 是由主服务器维护的一个固定长度(fixed-size)先进先出的(FIFO)队列，默认大小为1MB。 当主服务器进行命令传播时，他不仅会将命令发送个所有服务器，还会将写命令入队到复制积压缓冲区里面， 因此，主服务器的复制积压缓冲区里面会保存着一部分最近传播的写命令，并且复制积压缓冲区会为队列的每个字节记录相应的复制偏移量，例如： 当服务器重新连上主服务器时，从服务器会通过PSYNC命令将自己的复制偏移量offset发送给主服务器，主服务器会根据这个偏移量来决定对从服务器执行何种同步操作： 如果offset偏移量之后的数据（也即是偏移量offset+1开始的数据）仍然存在于复制积压缓冲区里面，那么主服务器将对从服务器执行部分重同步操作。 如果offset偏移量之后的数据（也即是偏移量offset+1开始的数据）仍然存在于复制积压缓冲区里面，那么主服务器将对从服务器执行部分重同步操作。 拿上节中的示例图为例： 当服务器A断线之后，立即重新来凝结主服务器，并发送PSYNC命令，报告自己的复制偏移量时10086； 主服务器介绍到从服务器法洛斯的PSYNC命令以及偏移量10086之后，检查偏移量之后的数据是否存在于复制积压缓冲区里边，发现存在，于是向从服务器发送+CONTINUE回复，表示数据同步将以部分冲同步模式来进行。 接着主服务器将复制积压缓冲区10086偏移量之后的所有数据都发送给从服务器 Redis为复制积压缓冲区设置的默认大小为1MB，如果主服务器需要执行大量写命令，或者主从服务器短线之后重连所需要的事件比较长，那么这个大小也许并不合适。如果复制积压缓冲区的大小设置得不恰当，那么PSYNC命令的复制重同步模式就不能正常发挥作用，因此，正确估算和设置复制积压缓冲区的大小非常重要。复制积压缓冲区的最小大小可以根据公式second*write_size_per_second来估算：【其中，second为从服务器断线后重新连接上主服务器所需要的时间(以秒衡量)；write_size_per_second则是主服务器平均每秒产生的写命令数据量(协议格式的写命令的长度总和)】 例如，如果主服务器平均每秒产生1 MB的写数据，而从服务器断线之后平均要5秒才能重新连接上主服务器，那么复制积压缓冲区的大小就不能低于5MB。 为了安全起见，可以将复制积压缓冲区的大小设为2*second*write_size_per_second，这样可以保证绝大部分断线情况都能用部分重同步来处理。 至于复制积压缓冲区大小的修改方法，可以参考配置文件中关于repl-backlog-size选项的说明。 服务器运行ID ​ 每个Redis服务器，不论主服务器还是从服务，都会有自己的运行ID。运行ID在服务器启动时自动生成，由40个随机的十六进制字符组成。 ​ 当从服务器对主服务器进行初次复制时，主服务器会将自己的运行ID传送给从服务器，而从服务器则会将这个运行ID保存起来。当从服务器断线并重新连上一个主服务器时，从服务器将向当前连接的主服务器发送之前保存的运行ID： 如果从服务器保存的运行ID和当前连接的主服务器的运行ID相同，那么说明从服务器断线之前复制的就是当前连接的这个主服务器，主服务器可以继续尝试执行部分重同步操作。 相反地，如果从服务器保存的运行ID和当前连接的主服务器的运行ID并不相同，那么说明从服务器断线之前复制的主服务器并不是当前连接的这个主服务器，主服务器将对从服务器执行完整重同步操作。 PSYNC的实现 PSYNC命令的调用方法有两种： 如果从服务器以前没有复制过任何主服务器，或者之前执行过SLAVEOF no one命令，那么从服务器在开始一次新的复制时将向主服务器发送PSYNC ? -1命令，主动请求主服务器进行完整重同步（因为这时不可能执行部分重同步）。 相反地，如果从服务器已经复制过某个主服务器，那么从服务器在开始一次新的复制时将向主服务器发送PSYNC ＜runid＞ ＜offset＞命令：其中runid是上一次复制的主服务器的运行ID，而offset则是从服务器当前的复制偏移量，接收到这个命令的主服务器会通过这两个参数来判断应该对从服务器执行哪种同步操作。 根据情况，接收到PSYNC命令的主服务器会向从服务器返回以下三种回复的其中一种： 如果主服务器返回+FULLRESYNC ＜runid＞ ＜offset＞回复，那么表示主服务器将与从服务器执行完整重同步操作：其中runid是这个主服务器的运行ID，从服务器会将这个ID保存起来，在下一次发送PSYNC命令时使用；而offset则是主服务器当前的复制偏移量，从服务器会将这个值作为自己的初始化偏移量。 如果主服务器返回+CONTINUE回复，那么表示主服务器将与从服务器执行部分重同步操作，从服务器只要等着主服务器将自己缺少的那部分数据发送过来就可以了。 如果主服务器返回-ERR回复，那么表示主服务器的版本低于Redis 2.8，它识别不了PSYNC命令，从服务器将向主服务器发送SYNC命令，并与主服务器执行完整同步操作。 复制的实现 设置主服务器的地址和端口 1slaveof 127.0.0.1 6379 ​ 从服务器首先要做的就是将客户端给定的主服务IP地址127.0.0.1以及端口6379保存到服务器状态的masterhost属性和masterport属性中 123456struct redisServer&#123; //... char *masterhost; // 主服务器地址 int masterport; // 主服务端口 //...&#125; ​ slaveof命令是一个异步命令，在完成masterhost属性和masterport属性的设置工作之后，从服务器将向发送slaveof命令的客户端返回OK，表示复制指令已经被接收，而实际的复制工作将在OK返回之后才真正开始执行。 建立套接字连接 ​ 在slaveof命令执行之后，从服务器将根据命令所设置的IP地址和端口，创建连向主服务器的套接字连接 ​ 如果从服务器创建的套接字能成功连接（connect）到主服务器，那么从服务器将为这个套接字关联一个专门用于处理复制工作的文件事件处理器，这个处理器将负责执行后续的复制工作，比如接收RDB文件，以及接收主服务器传播来的写命令，诸如此类。 ​ 而主服务器在接受（accept）从服务器的套接字连接之后，将为该套接字创建相应的客户端状态，并将从服务器看作是一个连接到主服务器的客户端来对待，这时从服务器将同时具有服务器（server）和客户端（client）两个身份：从服务器可以向主服务器发送命令请求，而主服务器则会向从服务器返回命令回复。 ​ 因为复制工作接下来的几个步骤都会以从服务器向主服务器发送命令请求的形式来进行，所以理解“从服务器是主服务器的客户端”这一点非常重要。 发送PING命令 ​ 从服务器成为主服务器的客户端之后，做的第一件事就是向主服务器发送一个PING命令。 PING命令起到两个作用： 主从服务器成功建立起了套接字连接，但双方并未使用该套接字进行任何通信，通过发送PING命令可以检查套接字的读写状态是否正常 检查主服务器能否正常处理命令请求。 ​ 从服务器在发送PING命令之后将遇到以下三种情况的其中一种： 如果主服务器向从服务器返回了一个命令回复，但从服务器却不能在规定的时限（timeout）内读取出命令回复的内容，那么表示主从服务器之间的网络连接状态不佳，不能继续执行复制工作的后续步骤。当出现这种情况时，从服务器断开并重新创建连向主服务器的套接字。 如果主服务器向从服务器返回一个错误，那么表示主服务器暂时没办法处理从服务器的命令请求，不能继续执行复制工作的后续步骤。当出现这种情况时，从服务器断开并重新创建连向主服务器的套接字。比如说，如果主服务器正在处理一个超时运行的脚本，那么当从服务器向主服务器发送PING命令时，从服务器将收到主服务器返回的BUSY Redisis busy running a script.You can only call SCRIPT KILL or SHUTDOWN NOSAVE.错误。 如果从服务器读取到&quot;PONG&quot;回复，那么表示主从服务器之间的网络连接状态正常，并且主服务器可以正常处理从服务器（客户端）发送的命令请求，在这种情况下，从服务器可以继续执行复制工作的下个步骤。 身份验证 ​ 从服务器在收到主服务器返回的&quot;PONG&quot;回复之后，下一步要做的就是决定是否进行身份验证： 如果从服务器设置了masterauth选项，那么进行身份验证。 如果从服务器没有设置masterauth选项，那么不进行身份验证。 ​ 在需要进行身份验证的情况下，从服务器将向主服务器发送一条AUTH命令，命令的参数为从服务器masterauth选项的值。举个例子，如果从服务器masterauth选项的值为10086，那么从服务器将向主服务器发送命令AUTH 10086。 ​ 从服务器在身份验证阶段可能遇到的情况有以下几种： 如果主服务器没有设置requirepass选项，并且从服务器也没有设置masterauth选项，那么主服务器将继续执行从服务器发送的命令，复制工作可以继续进行。 如果从服务器通过AUTH命令发送的密码和主服务器requirepass选项所设置的密码相同，那么主服务器将继续执行从服务器发送的命令，复制工作可以继续进行。与此相反，如果主从服务器设置的密码不相同，那么主服务器将返回一个invalid password错误。 如果主服务器设置了requirepass选项，但从服务器却没有设置masterauth选项，那么主服务器将返回一个NOAUTH错误。另一方面，如果主服务器没有设置requirepass选项，但从服务器却设置了masterauth选项，那么主服务器将返回一个no password is set错误。 所有错误情况都会令从服务器中止目前的复制工作，并从创建套接字开始重新执行复制，直到身份验证通过，或者从服务器放弃执行复制为止。 发送端口信息 ​ 在身份验证步骤之后，从服务器将执行命令REPLCONF listening-port ＜port-number＞，向主服务器发送从服务器的监听端口号。例如在我们的例子中，从服务器的监听端口为6380，那么从服务器将向主服务器发送命令REPLCONF listening-port 6380。 ​ 主服务器在接收到这个命令之后，会将端口号记录在从服务器所对应的客户端状态的slave_listening_port属性中： 123456typedef struct redisClient &#123; // ... //从服务器的监听端口号 int slave_listening_port; // ...&#125; redisClient; ​ slave_listening_port属性目前唯一的作用就是在主服务器执行INFO replication命令时打印出从服务器的端口号。主服务器客户端通过 info relication命令查看。 同步 在这一步，从服务器将向主服务器发送PSYNC命令，执行同步操作，并将自己的数据库更新至主服务器数据库当前所处的状态。 值得一提的是，在同步操作执行之前，只有从服务器是主服务器的客户端，但是在执行同步操作之后，主服务器也会成为从服务器的客户端： 如果PSYNC命令执行的是完整重同步操作，那么主服务器需要成为从服务器的客户端，才能将保存在缓冲区里面的写命令发送给从服务器执行。 如果PSYNC命令执行的是部分重同步操作，那么主服务器需要成为从服务器的客户端，才能向从服务器发送保存在复制积压缓冲区里面的写命令。 正因为主服务器成为了从服务器的客户端，所以主服务器才可以通过发送写命令来改变从服务器的数据库状态，不仅同步操作需要用到这一点，这也是主服务器对从服务器执行命令传播操作的基础。 命令传播 当完成了同步之后，主从服务器就会进入命令传播阶段，这时主服务器只要一直将自己执行的写命令发送给从服务器，而从服务器只要一直接收并执行主服务器发来的写命令，就可以保证主从服务器一直保持一致了。 心跳检查 在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令： replconf ack ＜replication_offset＞ 其中replication_offset是从服务器当前的复制偏移量。发送replconf ack命令对于主从服务器有三个作用： 检测主从服务器的网络连接状态。 通过在主服务器的客户端 info replication命令查看其中的lag一栏，这表示从服务器最后一次向主服务器发送 replconf ack命令距离现在过去了多少秒。一般在0或者1之间跳动，如果超过1秒，说明主从服务器之间的连接出现了问题。 辅助实现min-slaves选项。 redis的min-slaves-to-write和min-slaves-max-lag两个选项可以防止主服务器在不安全的情况下执行写命令。 12min-slaves-to-write 3min-slaves-max-lag 10 例如上述代码就意味着：在从服务器的数量少于3或者3个从服务器的延迟(lag)值都大于等于10秒时，主服务器将拒绝执行写命令。 检测命令丢失。 如果因为网络故障，主服务器传播给从服务器的写命令在半路丢失，那么当从服务器向主服务器发送replconf ack命令时，主服务器将发觉从服务器当前的复制偏移量少于自己的复制偏移量，然后主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区里面找到从服务器缺少的数据，并将这些数据重新发送给从服务器。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"},{"name":"高可用","slug":"高可用","permalink":"https://solitaire-12.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"Redis|RDB持久化","slug":"redis/RDB持久化","date":"2024-03-25T02:15:00.000Z","updated":"2024-03-25T06:00:17.913Z","comments":true,"path":"2024/03/25/redis/RDB持久化/","permalink":"https://solitaire-12.github.io/2024/03/25/redis/RDB%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"RDB持久化 RDB持久化既可以手动执行，也可以根据服务器配置选项定期执行，该功能可以将某一时刻上的数据保存到一个RDB文件中，RDB文件保存在硬盘里，所以即使redis服务器进程退出，甚至运行redis服务器的计算机宕机，只要RDB文件存在，redis服务器就能通过RDB文件还原。 RDB文件的创建和载入 创建 save命令 会阻塞redis服务器进程，直到RDB文件创建完毕为止。在服务器进程阻塞期间，服务器不能处理任何命令请求。 bgsave命令 或派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程(父进程)继续处理命令请求。 创建RDB文件的实际工作是由rdb.c/rdbSave函数完成的。save和bgsave命令会以不同的方式调用这个函数。 123456789101112131415// 伪代码理解这两个命令void save()&#123; rdbSave(); //创建RDB文件 &#125;void bgsave()&#123; pid = fork();//创建子进程 if(pid == 0) rdbSave();//子进程负责创建RDB文件 signal_parent(); //完成之后向父进程发送信号 else if (pid &gt; 0) //父进程继续处理命令请求，并通过轮询等待子进程的信号 handle_request_and_wait_signal(); else handle_fork_error();//处理出错情况&#125; 命令执行时的服务状态： save命令：当save命令执行时，redis服务器会阻塞。save命令处理期间，客户端发送的所有命令都会被拒绝；只有在save命令执行结束、重新开始接受命令请求之后，客户端发送的命令才会被处理。 bgsave命令：由于bgsave命令的保存工作是由子进程执行的，所以子进程在执行创建RDB文件的过程中，redis服务器仍然可以继续处理客户端的命令请求。但是，在bgsave命令执行期间，服务器处理save、bgsave、bgrewriteaof三个命令的方式会有所不同。 bgsave执行期间，客户端发送的sava命令会被服务器拒绝，服务器禁止save和bgsave命令同时执行（为了避免父进程和子进程同时执行rdbSave函数，防止产生竞争条件）。 bgsave执行期间，客户端发送的bgsava命令会被服务器拒绝，因为同时执行两个bgsave会产生竞争条件。 bgsave和bgrewriteaof两个命令不能同时执行 如果basave命令正在执行，客户端发送的bgrewriteaof或被延迟到bgsava执行结束后执行。 如果bgrewriteaof正在执行，客户端发送的bgsave会被服务器拒绝。 因为bgrewriteaof和bgsave命令的实际工作都是由子线程执行的，所以两个命令在操作方面并没有冲突，不能同时执行是从性能上考虑的(并发出两个子线程，且这两个子线程同时执行大量的磁盘写入操作)。 载入 与创建方式不同，RDB文件的载入工作是在服务器启动时自动执行的，没有设置专门用于载入RDB文件的命令，只要redis服务器在启动时，检测到RDB文件存在，就会自动载入RDB文件。 在启动时打印的日志中：红色框选出来的内容，就是redis服务器成功加载RDB文件之后打印的。 由于AOF文件的更新频率通常比RDB文件的更新频率高，所以： 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据 如果服务器AOF持久化功能处于关闭状态，那么服务器才会使用RDB文件来还原数据 载入RDB文件的实际工作由rdb.c/rdbLoad函数完成，这个函数和rdbSave函数之间的关系可以用下图表示： 服务器在载入时的状态：服务器载入RDB文件期间，会一直处于阻塞状态，直到载入工作执行完成。 自动间隔性保存 由于bgsave命令可以在不阻塞服务器进程的情况下执行，所以redis允许用户通过设置服务器配置中的save选项，让服务器每隔一段时间自动执行一次bgsave命令。 用户可以通过save选项设置多个保存条件，但是其中任意一个条件被满足，服务器就会执行bgsave命令。如下redis服务配置文件中默认的save配置： 123save 900 1 #服务器在900秒之内，对数据库进行了至少1次修改。save 300 10 #服务器在300秒之内，对数据库进行了至少10次修改。save 60 10000 #服务器在60秒之内，对数据库进行了至少10000次修改。 保存条件 当redis服务器启动时，用户可以通过指定配置文件或者传入启动参数的方式设置save选项，如果用户没有主动设置save选项，那么服务器会为save设置默认条件。 123save 900 1save 300 10save 60 10000 接着，服务器程序会根据save选项设置的条件，设置服务器状态redisServer结构的saveparams属性。 123456struct redisServer &#123; // ... //记录了保存条件的数组 struct saveparam *saveparams; // ...&#125;; saveparams属性时一个数组，数组中的每个元素都是一个saveparam结构，每个saveparam结构都保存了一个save选项设置的保存条件： 123456struct saveparam &#123; //秒数 time_t seconds; //修改数 int changes;&#125;; 例如，上述中默认的配置，就会如下图所示。 dirty计数器和lastsave属性 除了saveparams数组之外，服务器状态还维持着一个dirty计数器，以及一个lastsave属性。 dirty计数器记录距离上一次成功执行save命令或者bgsave命令之后，服务器对数据进行了多少次写操作(包含写入、删除、更新等) lastsave属性是一个unix时间戳，记录了服务器上一次成功执行save命令或bgsave命令的时间。 12345678struct redisServer &#123; // ... //修改计数器 long long dirty; //上一次执行保存的时间 time_t lastsave; // ...&#125;; 检查保存条件是否满足 redis的服务器周期性操作函数serverCron默认每个100毫秒就会执行一次，该函数用于对正在执行的服务器进行维护，它的其中一项工作就是检查save选项设置的保存条件是否已经满足，如果满足的话，就执行bgsave命令。 123456789101112131415void saverCron()&#123; // ... // 遍历所有保存条件 server.saveparams.forEach(saveparam-&gt;&#123; // 计算距离上次执行保存操作有多少秒 save_interval = unixtime_now()-server.lastsave // 如果数据库状态的修改次数超过条件所设置的次数 ///并且距离上次保存的时间超过条件所设置的时间 ///那么执行保存操作 if (server.dirty &gt;= saveparam.changes &amp;&amp; save_interval &gt; saveparam.seconds)&#123; bgsave() &#125; &#125;)&#125; 程序或遍历并检查saveparams数组中的所有保存条件，只要任意一个条件满足，那么服务器就会执行bgsave。 示例： 一台redis服务器的当前状态： 那么当时间来到1378271101，也即是1378270800的301秒之后，且数据有10改动，那么服务器将自动执行一次bgsave。假设，bgsave执行了5秒，那么其中的dirty及数据会被重重为0，而lastsave属性或被更新成1378271106。此时服务器的当前状态为： RDB文件结构 todo","categories":[{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"},{"name":"持久化","slug":"持久化","permalink":"https://solitaire-12.github.io/tags/%E6%8C%81%E4%B9%85%E5%8C%96/"}]},{"title":"Redis|AOF持久化","slug":"redis/AOF持久化","date":"2024-03-25T02:15:00.000Z","updated":"2024-03-25T06:00:02.834Z","comments":true,"path":"2024/03/25/redis/AOF持久化/","permalink":"https://solitaire-12.github.io/2024/03/25/redis/AOF%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"AOF持久化 AOF持久化是通过保存redis服务器所执行的写命令来记录数据库状态的。 写入到AOF文件的命令都是以redis的命令请求格式保存的，且redis的命令请求是纯文本格式，四亿可以直接打开AOF文件查看内容。 具体实现 AOF持久化功能的实现可以分为命令追加(append)、文件写入、文件同步(sync)三个步骤。 命令追加 当AOF持久化功能处于开启状态时，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾： 12345struct redisServer &#123; // ... sds aof_buf; // AOF缓冲区 // ...&#125;; 写入和同步 redis的服务器进程就是一个事件循环(loop)，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件则负责执行像serverCron函数这样需要定时运行的函数。 在处理文件事件的可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以在服务器每次结束一个事件循环之前，它都会调用flushAppendOnlyFile函数，判断是否需要将aof_buf缓冲区的内容写入和保存到AOF文件中，用一段伪代码表示这一过程。 1234567891011void eventLoop()&#123; while True&#123; //处理文件事件，接收命令请求以及发送命令回复 //处理命令请求时可能会有新内容被追加到 aof_buf缓冲区中 processFileEvents() //处理时间事件 processTimeEvents() //考虑是否要将 aof_buf中的内容写入和保存到 AOF文件里面 flushAppendOnlyFile() &#125;&#125; flushAppendOnlyFile函数的行为由服务器配置的appendfsync选项的值来决定，各个不同值产生的行为如下 appendfsync选项的值 flushAppendOnlyFile函数的行为 always 将aof_buf缓冲区中的所有内容写入并同步到AOF文件中 everysec(默认值) 将aof_buf缓冲区中的所有内容写入到AOF文件中，如果上一次同步AOF文件的时间距离现在超过一秒钟，那么再次对AOF文件进行同步，并且这个同步操作事由一个线程专门负责执行的。 no 将aof_buf缓冲区中的所有内容写入到AOF文件中，但是并不对AOF文件进行同步，何时同步由操作系统来决定。 如果用户没有主动为appendfsync选项设置值，那么appendfsync选项的默认值为everysec，关于appendfsync选项的更多信息，请参考Redis项目附带的示例配置文件redis.conf。 如何选择： 当appendfsync的值为always时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且同步AOF文件，所以always的效率是appendfsync选项三个值当中最慢的一个，但从安全性来说，always也是最安全的，因为即使出现故障停机，AOF持久化也只会丢失一个事件循环中所产生的命令数据。 当appendfsync的值为everysec时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，并且每隔一秒就要在子线程中对AOF文件进行一次同步。从效率上来讲，everysec模式足够快，并且就算出现故障停机，数据库也只丢失一秒钟的命令数据。 当appendfsync的值为no时，服务器在每个事件循环都要将aof_buf缓冲区中的所有内容写入到AOF文件，至于何时对AOF文件进行同步，则由操作系统控制。因为处于no模式下的flushAppendOnlyFile调用无须执行同步操作，所以该模式下的AOF文件写入速度总是最快的，不过因为这种模式会在系统缓存中积累一段时间的写入数据，所以该模式的单次同步时长通常是三种模式中时间最长的。从平摊操作的角度来看，no模式和everysec模式的效率类似，当出现故障停机时，使用no模式的服务器将丢失上次同步AOF文件之后的所有写命令数据。 文件载入与还原 因为AOF文件里面包含了重建数据库状态的所有命令，所以服务器只要读入并重新执行一边AOF文件里面保存的写命令，就可以还原服务器关闭之前的数据库。还原步骤大致如下： 创建一个不带网络连接的伪客户端(fake client)：因为redis的命令只能在客户端上下文中执行，而载入AOF文件时使用的命令直接来源于AOF文件而不是网络连接，所以服务器使用了一个没有网络连接的客户端来执行AOF文件所保存的写命令，伪客户端执行命令的效果和带网络连接的客户端执行命令效果是一样的。 从AOF文件中分析并读取一条写命令 使用伪客户端执行被读出的写命令 一直执行步骤2、3，直到AOF文件中的命令都被处理完毕。 AOF重写 从上述表达的内容来看，会暴露出一个问题：随着服务器的运行，AOF文件记录的内容也会越来越多，文件体积也会越来越大，如果不加以控制，并且AOF文件的体积越大，使用AOF文件来进行数据还原所需的时间就越多。 举个例子： 1234567rpush list &#x27;a&#x27; &#x27;b&#x27;rpush list &#x27;c&#x27;rpush list &#x27;d&#x27; &#x27;e&#x27;lpop listlpop listrpush list &#x27;f&#x27; &#x27;g&#x27;# 此时的数据库状态：[&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;,&#x27;f&#x27;,&#x27;g&#x27;] 当客户端执行了上述6条命令，AOF文件就会依次记录这6条命令。在业务中，这类写操作会很频繁，也就导致了AOF的体积不断膨胀。 为了解决AOF文件体积不断膨胀的问题，redis提供了AOF文件重写(rewrite)功能。通过该功能，redis服务器可以创建一个新的AOF文件来代替现在的AOF文件，新旧两个AOF文件所保存的数据库状态相同，但是新的AOF文件不会包含任何冗余的命令，所以新的AOF文件的体积会比原先小得多。 实现： 🚨注意：redis重写后的生成的新AOF文件替换掉旧的AOF文件。新的AOF并非通过对现有AOF文件进行读取分析或者其他操作产生的，而是通过读取服务器当前的数据库状态来实现的。 好比上述例子中，旧的AOF文件记录了6条数据，重写之后的AOF文件只保留了rpush list 'c' 'd' 'e' 'f' 'g'这么一条命令。 书中对整个重写过程的伪代码解析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def aof_rewrite(new_aof_file_name): #创建新 AOF文件 f = create_file(new_aof_file_name) #遍历数据库 for db in redisServer.db: #忽略空数据库 if db.is_empty(): continue #写入SELECT命令，指定数据库号码 f.write_command(&quot;SELECT&quot; + db.id) #遍历数据库中的所有键 for key in db: #忽略已过期的键 if key.is_expired(): continue #根据键的类型对键进行重写 if key.type == String: rewrite_string(key) elif key.type == List: rewrite_list(key) elif key.type == Hash: rewrite_hash(key) elif key.type == Set: rewrite_set(key) elif key.type == SortedSet: rewrite_sorted_set(key) #如果键带有过期时间，那么过期时间也要被重写 if key.have_expire_time(): rewrite_expire_time(key) #写入完毕，关闭文件 f.close() def rewrite_string(key): #使用GET命令获取字符串键的值 value = GET(key) #使用SET命令重写字符串键 f.write_command(SET, key, value) def rewrite_list(key): #使用LRANGE命令获取列表键包含的所有元素 item1, item2, ..., itemN = LRANGE(key, 0, -1) #使用RPUSH命令重写列表键 f.write_command(RPUSH, key, item1, item2, ..., itemN) def rewrite_hash(key): #使用HGETALL命令获取哈希键包含的所有键值对 field1, value1, field2, value2, ..., fieldN, valueN = HGETALL(key) #使用HMSET命令重写哈希键 f.write_command(HMSET, key, field1, value1, field2, value2, ..., fieldN, valueN) def rewrite_set(key); #使用SMEMBERS命令获取集合键包含的所有元素 elem1, elem2, ..., elemN = SMEMBERS(key) #使用SADD命令重写集合键 f.write_command(SADD, key, elem1, elem2, ..., elemN) def rewrite_sorted_set(key): #使用ZRANGE命令获取有序集合键包含的所有元素 member1, score1, member2, score2, ..., memberN, scoreN = ZRANGE(key, 0, -1, &quot;WITHSCORES&quot;) #使用ZADD命令重写有序集合键 f.write_command(ZADD, key, score1, member1, score2, member2, ..., scoreN, memberN) def rewrite_expire_time(key): #获取毫秒精度的键过期时间戳 timestamp = get_expire_time_in_unixstamp(key) #使用PEXPIREAT命令重写键的过期时间 f.write_command(PEXPIREAT, key, timestamp) 因为aof_rewirte函数生成的新AOF文件只包含还远当前数据库状态所必须的命令，所以新AOF文件不会浪费任何磁盘空间。 ⚠️注意：在实际中，为了避免在执行命令时造成客户端输入缓冲区溢出，重写程序在处理列表、哈希表、集合、有序集合这四种可能会带有多个元素的键时，会先检查键所包含的元素数量，如果元素数量超过了**AOF_REWRITE_ITMES_PER_CMD**常量的值，那么重写程序将使用多条命令来记录键的值，而不单单是用一条命令。 简单说名义下，【 关于AOF的功能都写在了aof.c文件中，其中**AOF_REWRITE_ITMES_PER_CMD常量定义在server.h文件中，值为64**。这里与《Redis设计与实现》这本书中的描述有所差别，原文是redis.h/REDIS_AOF_REWRITE_ITEMS_PER_CMD，该书作者在书中提到，是基于Redis 2.9来编写。而笔者所查看的redis源码是6.0.6版本，故而有所差异。 后台重写 从上述中可以了解到，AOF重写程序可以很好的创建一个新AOF文件的任务，但是因为这个函数会进行大量的写操作，所有调用这个函数的线程将被长时间阻塞，而且redis服务器是使用单线程来处理命令请求的。所以为了在重写AOF文件期间，不影响redis服务器处理命令，redis将AOF重写放在子进程里执行，如此达到两个目的： 子进程进行AOF重写期间，服务器进程可以继续处理命令 子进程带有服务器进程的数据副本，使用子进程而不是线程，可以在避免使用锁的情况下，保证数据的安全性。 这样的话，会产生另一个问题：子进程在进行AOF期间，服务器进程还在处理新的命令请求，这回导致重写后的AOF文件保存的数据库状态与当前数据库状态不一致。 为解决数据不一致问题：redis服务器设置了一个AOF重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用，当redis服务器执行完一个命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区。 如此一来，子进程执行AOF重写期间，服务器进行需要执行一下三个工作： 执行客户端发送的命令 将执行后的写命令追加到AOF缓冲区 将执行后的写命令追加到AOF重写缓冲区 这样即可保证： AOF缓冲区的内容会定期被写入和同步到AOF文件，对现有的AOF文件的处理工作正常进行。 从创建子进程开始，服务器执行的所有写命令都会被记录到AOF重写缓冲区。 当子进程完成了AOF重写工作之后，它会向父进程发送一个信号，父进程接收到信号之后，会调用处理信号的函数，并执行以下工作： 将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保留的数据库状态将和服务器当前的数据库状态一致。 对新的AOF文件进行改名，原子的覆盖现有的AOF文件，完成新旧两个AOF文件的替换。 等这个信号处理函数执行完毕之后，父进程就可以继续执行命令请求。 在整个AOF后台重写过程中，只有信号处理函数执行时会对服务器进程（父进程）造成阻塞，在其他时候，AOF后台重写都不会阻塞父进程，这将AOF重写对服务器性能造成的影响降到了最低。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"},{"name":"持久化","slug":"持久化","permalink":"https://solitaire-12.github.io/tags/%E6%8C%81%E4%B9%85%E5%8C%96/"}]},{"title":"Redis|导航","slug":"redis/导航","date":"2024-03-25T02:14:55.000Z","updated":"2024-03-25T06:12:39.853Z","comments":true,"path":"2024/03/25/redis/导航/","permalink":"https://solitaire-12.github.io/2024/03/25/redis/%E5%AF%BC%E8%88%AA/","excerpt":"","text":"持久化 持久化导读：redis是内存数据库，它将自己的数据库状态储存在内存中。所以当redis服务进程退出时，服务器中的数据也将消失，故而需要将其数据保存到磁盘中。 RDB持久化 博客跳转↗️ AOF持久化 博客跳转↗️ RDB RDB（Redis DataBase）是redis默认的持久化方式。通过快照（snapshotting）完成。从上述的实现方式看，它所保存的数据是某一时刻的数据，即RDB保存redis某一时刻的数据快照。 触发方式 符合自定义配置的快照规则 执行了save或者bgsave命令 执行了flushall（清空）命令 执行主从复制操作（第一次） 特点 优点： RDB是二进制压缩文件，占用空间小，便于传输 主进程fork子进程，可以最大化redis性能 使用RDB文件来恢复数据较快 缺点： 不能保证数据的完整性，会丢失最后一次快照以后更改的数据 父进程在fork子进程的时候，如果主进程较大，容易阻塞 实现方式 windows环境： 手动保存 pushd [your redis path] redis-cli.exe save 保存之后，会在redis目录下，默认生成dump.rdb文件。 redis.conf 配置 1234567891011# 900s后有1次更新，进行持久化save 900 1# 300s后有10次更新，进行持久化save 300 10# 60秒后有10_000次更新，进行持久化save 60 10000## 配置默认文件名dbfilename dump.rdb## 文件存储位置dir ./ #根目录下 Linux环境： 待续 AOF AOF（append only file），redis默认情况下不开启。开启AOF持久化后，redis会将所有的写操作以及参数(RESP)记录到AOF文件中。 实现方式 1234567# redis.conf配置文件中## 开启AOF的持久化appendonly yes## AOF文件的保存位置(RDB AOF共用)dir ./ ## 配置默认文件名appendfilename appendonly.aof 高可用 高可用导读：高可用，也称HA（High Availability），是分布式系统架构设计中必须考虑的因素之一，通常指通过设计减少系统不提供服务的时间。 在Redis中表现为，单一的Redis服务节点出现故障时，整个服务都不能提供服务。以下是Redis三种高可用模式： 主从 博客跳转↗️ 哨兵 博客跳转↗️ 集群 博客跳转↗️ Lua脚本 Redis从2.6版本开始引入对Lua脚本的支持，通过在服务器中嵌入Lua环境，Redis客户端可以使用Lua脚本，直接在服务器端原子地执行多个Redis命令 123eval &quot;return &#x27;hello lua&#x27;&quot; 0script load &quot;return 1+1&quot; -- 返回一串SHA1校验码evalsha &quot;sha1校验码&quot; 创建Lua环境 redis服务器内嵌了一个Lua环境(environ-ment)，并对这个Lua环境进行了一系列的修改，从而确保Lua环境可以满足Redis服务器的使用。 创建一个基本的Lua环境。 服务器调用Lua的C语言的API函数lua_open 载入多个函数库到Lua环境。 基本库 包含Lua的核心（core）函数，比如assert、error、pairs、tostring、pcall等。另外，为了防止用户从外部文件中引入不安全的代码，库中的loadfile函数会被删除。 表格库 包含用于处理表格的通用函数，比如table.concat、table.insert、table.remove、table.sort等。 字符串库 包含用于处理字符串的通用函数，比如用于对字符串进行查找的string.find函数，对字符串进行格式化的string.format函数，查看字符串长度的string.len函数，对字符串进行翻转的string.reverse函数等。 数学库 是标准C语言数学库的接口，它包括计算绝对值的math.abs函数，返回多个数中的最大值和最小值的math.max函数和math.min函数，计算二次方根的math.sqrt函数，计算对数的math.log函数等。 调试库 提供了对程序进行调试所需的函数，比如对程序设置钩子和取得钩子的debug.sethook函数和debug.gethook函数，返回给定函数相关信息的debug.getinfo函数，为对象设置元数据的debug.setmetatable函数，获取对象元数据的debug.getmetatable函数等。 Lua CJSON库 🔗用于处理UTF-8编码的JSON格式，其中cjson.decode函数将一个JSON格式的字符串转换为一个Lua值，而cjson.encode函数将一个Lua值序列化为JSON格式的字符串。 Struct库 🔗用于在Lua值和C结构（struct）之间进行转换，函数struct.pack将多个Lua值打包成一个类结构（struct-like）字符串，而函数struct.unpack则从一个类结构字符串中解包出多个Lua值。 Lua cmsgpack库 🔗用于处理MessagePack格式的数据，其中cmsgpack.pack函数将Lua值转换为MessagePack数据，而cmsgpack.unpack函数则将MessagePack数据转换为Lua值。 创建全局表格redis，包含了对redis进行操作的函数。 服务器将在Lua环境中创建一个redis表格（table），并将它设为全局变量。这个redis表格包含以下函数： 用于执行Redis命令的redis.call和redis.pcall函数。 用于记录Redis日志（log）的redis.log函数，以及相应的日志级别（level）常量：redis.LOG_DEBUG，redis.LOG_VERBOSE，redis.LOG_NOTICE，redis.LOG_WARNING。 用于计算SHA1校验和的redis.sha1hex函数。❑用于返回错误信息的redis.error_reply函数和redis.status_reply函数。在这些函数里面，最常用也最重要的要数redis.call函数和redis.pcall函数，通过这两个函数，用户可以直接在Lua脚本中执行Redis命令： 12EVAL &quot;return redis.call(&#x27;PING&#x27;)&quot; 0# PONG 使用redis自制的随机函数替换掉Lua自带的带有副作用的随机函数。 为了保证相同的脚本可以在不同的机器上产生相同的结果，Redis要求所有传入服务器的Lua脚本，以及Lua环境中的所有函数，都必须是无副作用（side effect）的纯函数（pure function）。 但是，在之前载入Lua环境的math函数库中，用于生成随机数的math.random函数和math.randomseed函数都是带有副作用的，它们不符合Redis对Lua环境的无副作用要求。 因为这个原因，Redis使用自制的函数替换了math库中原有的math.random函数和math.randomseed函数，替换之后的两个函数有以下特征： 对于相同的seed来说，math.random总产生相同的随机数序列，这个函数是一个纯函数。 除非在脚本中使用math.randomseed显式地修改seed，否则每次运行脚本时，Lua环境都使用固定的math.randomseed（0）语句来初始化seed。 创建排序辅助函数。 为了防止带有副作用的函数令脚本产生不一致的数据，Redis对math库的math.random函数和math.randomseed函数进行了替换。对于Lua脚本来说，另一个可能产生不一致数据的地方是那些带有不确定性质的命令。比如对于一个集合键来说，因为集合元素的排列是无序的，所以即使两个集合的元素完全相同，它们的输出结果也可能并不相同。 Redis将SMEMBERS这种在相同数据集上可能会产生不同输出的命令称为“带有不确定性的命令”，这些命令包括：SINTER、SUNION、SDIFF、SMEMBERS、HKEYS、HVALS、KEYS为了消除这些命令带来的不确定性，服务器会为Lua环境创建一个排序辅助函数__redis__compare_helper，当Lua脚本执行完一个带有不确定性的命令之后，程序会使用__redis__compare_helper作为对比函数，自动调用table.sort函数对命令的返回值做一次排序，以此来保证相同的数据集总是产生相同的输出。 举个例子，如果我们在Lua脚本中对fruit集合和another-fruit集合执行SMEMBERS命令，那么两个脚本将得出相同的结果，因为脚本已经对SMEMBERS命令的输出进行过排序了： 12345678redis＞ EVAL &quot;return redis.call(&#x27;SMEMBERS&#x27;, KEYS[1])&quot; 1 fruit1) &quot;apple&quot;2) &quot;banana&quot;3) &quot;cherry&quot;redis＞ EVAL &quot;return redis.call(&#x27;SMEMBERS&#x27;, KEYS[1])&quot; 1 another-fruit1) &quot;apple&quot;2) &quot;banana&quot;3) &quot;cherry&quot; 创建 redis.pcall函数的册数报告辅助函数。 服务器将为Lua环境创建一个名为__redis__err__handler的错误处理函数，当脚本调用redis.pcall函数执行Redis命令，并且被执行的命令出现错误时，__redis__err__handler就会打印出错代码的来源和发生错误的行数，为程序的调试提供方便。 对Lua环境中的全局环境进行保护，防止用户在执行Lua脚本的时候，将额外的全局变量添加到Lua环境中。 服务器将对Lua环境中的全局环境进行保护，确保传入服务器的脚本不会因为忘记使用local关键字而将额外的全局变量添加到Lua环境里面。 将完成修改的Lua环境保存到服务器状态的Lua属性中，等待执行服务器传来的Lua脚本。 经过以上的一系列修改，Redis服务器对Lua环境的修改工作到此就结束了，在最后的这一步，服务器会将Lua环境和服务器状态的lua属性关联起来。 因为Redis使用串行化的方式来执行Redis命令，所以在任何特定时间里，最多都只会有一个脚本能够被放进Lua环境里面运行，因此，整个Redis服务器只需要创建一个Lua环境即可 本站（Redis|x标题）系列的文章是来源于站长阅读黄健宏《Redis设计与实现》后的总结和摘录。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"}]},{"title":"算法|数据结构(Java)","slug":"algorithm/数据结构","date":"2024-03-19T08:49:57.890Z","updated":"2024-06-14T04:04:27.281Z","comments":true,"path":"2024/03/19/algorithm/数据结构/","permalink":"https://solitaire-12.github.io/2024/03/19/algorithm/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"线性表 数组和矩阵 优点 读取速度快 缺点 事先必须直到数组的长度 插入删除元素很慢，效率很低 空间通常是有限制的 需要大块连续的内存块 ArrayList源码解析 相关题目 链表 优点 空间没有限制 插入删除元素很快 缺点 存取速度很慢 分类 单向链表（每个节点，存在一个指向下一个节点的指针） 双向链表（每个节点，存在一个指针指向下一个节点，一个指针指向上一个节点） 循环链表（上述两种链表的基础上，拥有头尾互指指针） 12345678910111213141516public class Node &#123; //数据域 public int data; //指针域，指向下一个节点 public Node next; public Node() &#123; &#125; public Node(int data) &#123; this.data = data; &#125; public Node(int data, Node next) &#123; this.data = data; this.next = next; &#125;&#125; LinkedList源码解析 相关题目 哈希表 栈和队列 栈 - LIFO 数组实现的叫静态栈 链表实现的叫动态栈 队列 - FIFO 数组实现的叫静态队列 链表实现的叫动态队列 树 知识脉络 结构说明 ​ 树是一种数据结构，它是n(n&gt;=0)个节点的有限集。n=0时称为空树。n&gt;0时，有限集的元素构成一个具有层次感的数据结构。 ​ 区别在于线性表是一对一的关系，树中的节点是一对多的关系。树具有以下特点： n&gt;0时，根节点是唯一的，不可能存在多个根节点。 每个节点有零个或者多个子节点；除了根节点外，每个节点有且仅有一个父节点。根节点没有父节点。 树的相关概念 子树：除了根节点外，每个子节点都可以由多个不相交的子树。 孩子与双亲：若一个节点有子树，那么该节点称为子树根的&quot;双亲&quot;，子树的根是该节点的&quot;孩子&quot;。在图一中，B、H是A的孩子，A是B、H的双亲。 兄弟：具有相同双亲的节点互为兄弟。例如B和H互为兄弟。 节点的度：一个节点拥有子树的数目。例如A的度就是2，B的度就是1，C的度就是3。 叶子：没有子树，也即度为0的节点。 分支节点：除了叶子节点之外的节点，也就是度不为0的节点。 内部节点：除了根节点之外的分支节点。 层次：根节点为第一层，其余节点的层次等于其双亲节点的层次加1。 树的高度：也称为树的深度，树中节点的最大层次。 有序树：树中节点各个子树之间的次序是重要的，不可以随意交换位置。 无序树：树中节点各个子树之间的次序是不重要的，可以随意交换位置。 森林：0或者多颗互不相交的树的集合。 二叉树 二叉树 最多有两颗子树的树 二叉树的五种状态 斜树 所有节点都只有左子树的二叉树叫做左斜树；所有节点都只有右子树的二叉树叫做右斜树。其本质是链表 满二叉树 二叉树中所有非叶子节点的的度都是2，且叶子节点都处于同一个层次上。 完全二叉树 如果一个二叉树与满二叉树前m个节点的结构相同，这样的二叉树被称为完全二叉树。 二叉查找树 - BST 二叉查找树(Binary Search Tree)是指一棵空树或者具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若任意节点的右子树不空，则右子树上所有节点的值均大于于它的根节点的值； 任意节点的左右子树也分别为二叉查找树； 没有键值相等的节点。 二叉查找树相比较其他数据结构的优势在于查找、插入的时间复杂度较低，为$O(log_{}n)$。 二叉查找树是基础性的数据结构，用于构建更为抽象的数据结构，如集合，多重集合，关联数组等。 二叉查找树的实现以及测试 平衡二叉树 - AVL 含有相同节点的二叉树可以有不同的形态，而二叉查找树的平均长度与树的深度有关，所以需要找出一个查到平均长度最小的一棵，那就是平衡二叉树，具有一下性质： 要么是棵空树，要么其根节点左右子树的深度之差绝对值不超过1； 其左右子树也都是平衡二叉树； 二叉树节点的平衡因子定义：（该节点的左子树的深度 - 右子树的深度）。则平衡二叉树的所有节点的平衡因子只可能是-1，0，1。 平衡二叉树的实现以及测试 红黑树 红黑树也是一种自平衡的二叉查找树，它的每个节点增加了一个存储位来记录节点的颜色，可以是RED，也可以是BLACK；通过任意一条从根到叶子检点路径上颜色的约束，红黑树保证最长路径不超过最短路径的两倍，因而近似平衡。 每个节点要么是红的要和是黑的。（红或黑） 根节点是黑的。（根黑） 每个叶节点（指树尾端NIL指针或NULL节点）都是黑的。（叶黑） 如果一个节点是红的，那么他的两个儿子都是黑的。（红子黑） 对于任意节点而言，其到叶节点树尾端NIL指针的每条路径都包含相同数目的黑节点。（路径下黑相同）。 应用： Java ConcurrentHashMap &amp; TreeMap C++ STL: map &amp; set linux进程调度Completely Fair Scheduler,用红黑树管理进程控制块 epoll在内核中的实现，用红黑树管理事件块 nginx中，用红黑树管理timer等 AVL树和红黑树的比较： AVL树的时间复杂度虽然优于红黑树，但是对于现在的计算机，CPU效率很高，所以性能差距也就变小了 红黑树的插入删除比AVL树更便于控制操作 红黑树整体性能略优于AVL树（红黑树旋转的情况少于AVL树） 红黑树的实现以及测试 哈夫曼树 哈夫曼树又称最优二叉树。是一种带权路径长度最短的二叉树，一般可以按下面的步骤构建： 将所有左右子树都为空的节点作为根节点。 在森林中选出两颗根节点的权值最小的树作为一棵新树的左右子树，且新树的附加节点的权值为其左右子树根节点的权值之和。注意，左子树的权值要小于右子树的权值。 从森林中删除这两棵树，同时把新的树加入到森林中。 重复2，3步骤，直到森林中只有一棵树为止，这棵树就是哈夫曼树。 B树 B树（B-Tree）是一种自平衡的树，能够保持数据的有序，这种数据结构能够让查找数据、顺序访问、插入数据以及删除的动作，都在对数时间内完成。B树，概括来说是一种自平衡的m阶树，与自平衡二叉树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。 根节点至少有两个子女； 每个中间节点都包含k-1个元素和k个孩子，其中$\\frac{m}{2}\\leq k \\leq m$； 每个叶子节点都包含k-1个元素，其中$\\frac{m}{2}\\leq k \\leq m$； 所有的叶子节点都位于同一层； 每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。 B-Tree中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个3阶的B-Tree: B+树 通常用于关系型数据库（如MySQL）和操作系统的文件系统中。 特点是能够保持数据的稳定有序，其中插入与修改拥有稳定的对数时间复杂度。 元素自底向上插入，这与二叉树相反。 在B树的基础上，为叶子节点增加链表指针(B树+叶子有序链表)，所有关键字都在叶子节点中出现，非叶子节点作为叶子节点的索引；B+树总是到叶子节点才命中。 B+树的非叶子节点不保存数据，只保存子树的临界值(最大或者最小)，所以同样大小的节点，B+树要比B树有更多的分支，使得这棵树更加矮胖，查询时做的I/O操作次数就会更少。 将上一节中的B-Tree优化，由于B+Tree的非叶子节点只存储键值信息，假设每个磁盘块能存储4个键值及指针信息，则变成B+Tree后其结构如下图所示: B+树的 R树 R树是用来做空间数据存储的树状数据结构。例如给地理位置，矩形和多边形这类多维数据建立索引。 R树的核心思想是聚合距离相近的节点并在树结构的上一层将其表示为这些节点的最小外接矩形(MBR)，这个最小外接矩形就成为上一层的一个节点。因为所有节点都在它们的最小外接矩形中，所以跟某个矩形不相交的查询就一定跟这个矩形中的所有节点都不相交。叶子节点上的每个矩形都代表一个对象，节点都是对象的聚合，并且越往上层聚合的对象就越多。也可以把每一层看做是对数据集的近似，叶子节点层是最细粒度的近似，与数据集相似度100%，越往上层越粗糙。 总结 我们知道，实际应用当中，我们经常使用的是查找和排序操作，这在我们的各种管理系统、数据库系统、操作系统等当中，十分常用。 数组的下标寻址十分迅速，但计算机的内存是有限的，故数组的长度也是有限的，实际应用当中的数据往往十分庞大；而且无序数组的查找最坏情况需要遍历整个数组；后来人们提出了二分查找，二分查找要求数组的构造一定有序，二分法查找解决了普通数组查找复杂度过高的问题。任何一种数组无法解决的问题就是插入、删除操作比较复杂，因此，在一个增删查改比较频繁的数据结构中，数组不会被优先考虑 普通链表由于它的结构特点被证明根本不适合进行查找 哈希表是数组和链表的折中，同时它的设计依赖散列函数的设计，数组不能无限长、链表也不适合查找，所以也不适合大规模的查找 二叉查找树因为可能退化成链表，同样不适合进行查找 AVL树是为了解决可能退化成链表问题，但是AVL树的旋转过程非常麻烦，因此插入和删除很慢，也就是构建AVL树比较麻烦 红黑树是平衡二叉树和AVL树的折中，因此是比较合适的。集合类中的Map、关联数组具有较高的查询效率，它们的底层实现就是红黑树。 多路查找树 是大规模数据存储中，实现索引查询这样一个实际背景下，树节点存储的元素数量是有限的(如果元素数量非常多的话，查找就退化成节点内部的线性查找了)，这样导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下。 B树与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。它的应用是文件系统及部分非关系型数据库索引。 B+树在B树基础上，为叶子结点增加链表指针(B树+叶子有序链表)，所有关键字都在叶子结点 中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中。通常用于关系型数据库(如MySQL)和操作系统的文件系统中。 B*树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针, 在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3。 R树是用来做空间数据存储的树状数据结构。例如给地理位置，矩形和多边形这类多维数据建立索引。 Trie树是自然语言处理中最常用的数据结构，很多字符串处理任务都会用到。Trie树本身是一种有限状态自动机，还有很多变体。什么模式匹配、正则表达式，都与这有关。 针对大量数据，如果在内存中作业优先考虑红黑树(map,set之类多为RB-tree实现)，如果在硬盘中作业优先考虑B系列树(B+, B, B*) 图 基础知识点 图(Graph)是由顶点的有穷非空集合和顶点之间的边的集合组成，通常表示为：G(V,E)，其中G表示图，V表示图中的顶点集合，E表示图中边的集合。 与线性表、树的差异：1. 线性表中的数据叫元素；树的数据叫节点；图中的数据叫顶点(Vertex)2. 线性表可以没有元素，叫空表；树可以没有节点，叫空树；图不能没有顶点 [有穷非空性]3. 线性表中的各元素是线性关系；树中的各元素是层级关系；图中的各顶点的关系是用边来表示的 术语 顶点的度 顶点Vi的度(Degree)是指在图中与Vi相关联的边的条数。对于有向图来说，有入度(In-degree)和出度(Out-degree)之分，有向图顶点的度等于该顶点的入度和出度之和。 邻接 若无向图中的两个顶点V1和V2存在一条边(V1,V2)，则称顶点V1和V2邻接(Adjacent)；若有向图中存在一条表&lt;V3,V2&gt;，则称顶点V3与顶点V2邻接，且是V3邻接到V2或者V2邻接到V3 路径 在无向图中，若从顶点Vi出发有一组边可到达顶点Vj，则称之为顶点Vi到顶点Vj序列为从顶点Vi到顶点Vj的路径(Path) 连通 若顶点Vi到顶点Vj有路径可通，则称顶点Vi到顶点Vj是连通的(Connected) 权 有些图的边或弧具有与他相关的数字，这种与图的边或弧相关的数叫权(Weight) 类型 无向图 如果图中任意两个顶点之间的边都是无向边(简而言之就是没有方向的边)，则称该图为无向图(Undirected graphs)。 无向图中的边使用小括号“()”表示; 比如 (V1,V2); 有向图 如果图中任意两个顶点之间的边都是有向边(简而言之就是有方向的边)，则称该图为有向图(Directed graphs)。 有向图中的边使用尖括号“&lt;&gt;”表示; 比如/&lt;V1,V2&gt; 完全图 无向完全图: 在无向图中，如果任意两个顶点之间都存在边，则称该图为无向完全图。(含有n个顶点的无向完全图有(n×(n-1))/2条边) 有向完全图: 在有向图中，如果任意两个顶点之间都存在方向互为相反的两条弧，则称该图为有向完全图。(含有n个顶点的有向完全图有n×(n-1)条边) 存储结构 邻接矩阵表示法 图的邻接矩阵(Adjacency Matrix)存储方式是用两个数组来表示图。一个一维数组存储图中顶点信息，一个二维数组(称为邻接矩阵)存储图中的边或弧的信息。 无向图 我们可以设置两个数组，顶点数组为vertex[4]=&#123;v0,v1,v2,v3&#125;，边数组arc[4][4]为上图右边这样的一个矩阵。对于矩阵的主对角线的值全为0是因为不存在顶点的边。 有向图 顶点数组为vertex[4]=&#123;v0,v1,v2,v3&#125;，弧数组arc[4][4]为下图右边这样的一个矩阵。主对角线上数值依然为0。但因为是有向图，所以此矩阵并不对称，比如由v1到v0有弧，得到arc[1][0]=1，而v0到v1没有弧，因此arc[0][1]=0。 邻接矩阵表示法的缺陷：由于存在n个顶点的图需要n*n个数组元素进行存储，当图为稀疏图时，使用邻接矩阵存储方法将会出现大量0元素，这会造成极大的空间浪费。这时，可以考虑使用邻接表表示法来存储图中的数据 邻接表表示法 无向图 顶点表的各个结点由data和firstedge两个域表示，data是数据域，存储顶点的信息，firstedge是指针域，指向边表的第一个结点，即此顶点的第一个邻接点。边表结点由adjvex和next两个域组成。adjvex是邻接点域，存储某顶点的邻接点在顶点表中的下标，next则存储指向边表中下一个结点的指针。例如: v1顶点与v0、v2互为邻接点，则在v1的边表中，adjvex分别为v0的0和v2的2。 有向图 若是有向图，邻接表结构是类似的，但要注意的是有向图由于有方向的。因此，有向图的邻接表分为出边表和入边表(又称逆邻接表)，出边表的表节点存放的是从表头节点出发的有向边所指的尾节点；入边表的表节点存放的则是指向表头节点的某个顶点。 带权图 Java集合源码解析 ArrayList 概述 ​ ArrayList实现了List接口，是顺序容器，即元素存放的数据与放进去的顺序是相同的，允许放入null元素，底层通过Object数组实现，未实现同步。 ​ 每个ArrayList都有一个容量(capacity)，表示底层数组的实际大小，容量内存储元素的个数不能多于当前容量。在向容器中添加元素时，如果容量不足，容器会自动增大底层数组的大小（扩容机制）。 ​ size()，isEmpty()，get()，set()方法均能在常数时间内完成，add()方法的时间开销跟插入的位置有关，addAll()方法的时间开销跟添加元素的个数成正比。其余方法大都时线性时间。 ​ 追求效率，ArrayList也没有实现同步(synchronized)，如果需要多个线程访问的，用户需要手动同步（分析查看）。 源码解析 底层数据结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Default initial capacity. */private static final int DEFAULT_CAPACITY = 10;/** * Shared empty array instance used for empty instances. */private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;/** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access/** * The size of the ArrayList (the number of elements it contains). * * @serial */private int size;/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125;&#125;/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125;/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection&#x27;s * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public ArrayList(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); if ((size = a.length) != 0) &#123; if (c.getClass() == ArrayList.class) &#123; elementData = a; &#125; else &#123; elementData = Arrays.copyOf(a, size, Object[].class); &#125; &#125; else &#123; // replace with empty array. elementData = EMPTY_ELEMENTDATA; &#125;&#125; 注意： 在无参构造方法中，可以看到ArrayList初始化了一个DEFAULTCAPACITY_EMPTY_ELEMENTDATA，这是一个容量为0的空Object数组。 在有参构造方法中，当传入的参数为零，或者集合大小为0的时候，初始化一个EMPTY_ELEMENTDATA，这是同样是一个容量为0的空Object数组。 综上两点，假若没有规定初始容量，ArrayList会初始化出一个容量为0的空Object数组。随后在往其中第一次添加元素的时候，进行扩容，默认容量(DEFAULT_CAPACITY)为10。 扩容机制 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * Trims the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance to be the * list&#x27;s current size. An application can use this operation to minimize * the storage of an &lt;tt&gt;ArrayList&lt;/tt&gt; instance. */public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125;&#125;/** * Increases the capacity of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance, if * necessary, to ensure that it can hold at least the number of elements * specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It&#x27;s already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125;&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;/** * The maximum size of array to allocate. * Some VMs reserve some header words in an array. * Attempts to allocate larger arrays may result in * OutOfMemoryError: Requested array size exceeds VM limit */private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 描述： ​ 每当添加元素时，都会去检查添加后元素的个数是否会超出当前数组的长度，如果超出，数组将会进行扩容，以满足添加数据的要求。数组扩容通过一个公开的方法ensureCapacity(int minCapacity)来实现。所以，在添加大量元素前，可以通过这个方法来实现手动扩容，减少在添加元素过程中，自动扩容的次数，因为扩容的过程需要去复制源数组到新数组中，这个过程费时费空间。 ​ 数组进行扩容时，会将老数组中的元素拷贝一份到新的数组中，在grow方法中有这么一条代码：int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);可以看出，数组每次扩容的时候，会扩容到当前容量的1.5倍。要是newCapacity超过了Integer.MAX_VALUE - 8($2^{31}-1-8$)就会把容量扩大到(Integer.MAX_VALUE)，这样就不会再扩容了。 add 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;/** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125;/** * Appends all of the elements in the specified collection to the end of * this list, in the order that they are returned by the * specified collection&#x27;s Iterator. The behavior of this operation is * undefined if the specified collection is modified while the operation * is in progress. (This implies that the behavior of this call is * undefined if the specified collection is this list, and this * list is nonempty.) * * @param c collection containing elements to be added to this list * @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the call * @throws NullPointerException if the specified collection is null */public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0;&#125;/** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in the list in the order that they are returned by the * specified collection&#x27;s iterator. * * @param index index at which to insert the first element from the * specified collection * @param c collection containing elements to be added to this list * @return &lt;tt&gt;true&lt;/tt&gt; if this list changed as a result of the call * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; * @throws NullPointerException if the specified collection is null */public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0;&#125; ​ add(int index,E e)需要对元素进行移动，然后完成插入操作，也就意味着该方法的时间复杂度是线性的。 ​ addAll()方法能够一次添加多个元素，根据位置的不同，也有两个版本。一种是尾插法：addAll(Collection&lt;? extends E&gt; c)，一种是指定位置插入：addAll(int index, Collection&lt;? extends E&gt; c)。跟add()方法类似，在插入之前需要进行空间检查，判断是否需要扩容；如果从指定的位置插入，也会移动元素。综上：addAll()方法的时间复杂度和插入的元素个数以及插入位置有关。 set 1234567891011121314151617181920212223242526272829303132/** * Replaces the element at the specified position in this list with * the specified element. * * @param index index of the element to replace * @param element element to be stored at the specified position * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125;/** * Checks if the given index is in range. If not, throws an appropriate * runtime exception. This method does *not* check if the index is * negative: It is always used immediately prior to an array access, * which throws an ArrayIndexOutOfBoundsException if index is negative. */private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;@SuppressWarnings(&quot;unchecked&quot;)E elementData(int index) &#123; return (E) elementData[index];&#125; get 123456789101112/** * Returns the element at the specified position in this list. * * @param index index of the element to return * @return the element at the specified position in this list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123; rangeCheck(index); return elementData(index);&#125; remove 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Removes the element at the specified position in this list. * Shifts any subsequent elements to the left (subtracts one from their * indices). * * @param index the index of the element to be removed * @return the element that was removed from the list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125;/** * Removes the first occurrence of the specified element from this list, * if it is present. If the list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt; * (if such an element exists). Returns &lt;tt&gt;true&lt;/tt&gt; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &lt;tt&gt;true&lt;/tt&gt; if this list contained the specified element */public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125; 两个版本： 一个是remove(int index)删除指定位置的元素，另一个是remove(Object o)删除第一个满足o.equals(elementData[index])的元素 。 注意： ​ elementData[--size] = null; // clear to let GC do its work，这是为了让GC起作用。那为什么这里需要这种显示赋null值呢？ ​ 这是因为GC在判断对象能否回收是根据它是否在引用链上，而底层的数组维系着这个引用链。当删除一个元素之后，元素数量发生变化，所有后续元素往前移动，这样在引用链上就会多出来一个元素，按理这个元素应该是没有值的，要被GC掉。但是在后续元素往前移动的时候，是将后续元素拷贝一份，从删除位置开始从写原始位置上的值，这样的结果就导致了最后一个位置上是存在值的，GC确定不了这个对象是否该回收，故而在通过显示的赋null值，告诉GC可以回收该对象。 indexOf 123456789101112131415161718192021222324252627282930313233343536373839/** * Returns the index of the first occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the lowest index &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. */public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125;/** * Returns the index of the last occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the highest index &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. */public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; Fail-Fast机制 ArrayList采用了快速失败机制，通过AbstractList抽象类中的modCount参数来实现。源码注解第二段提到：如果该字段的值发生意外变化，迭代器会快速失败，抛出ConcurrentModificationException异常，以响应next，remove，previous，set或add操作，以避免在迭代期间面对并发修改时的不确定性行为。 123456789101112131415161718192021222324252627/** * The number of times this list has been &lt;i&gt;structurally modified&lt;/i&gt;. * Structural modifications are those that change the size of the * list, or otherwise perturb it in such a fashion that iterations in * progress may yield incorrect results. * * &lt;p&gt;This field is used by the iterator and list iterator implementation * returned by the &#123;@code iterator&#125; and &#123;@code listIterator&#125; methods. * If the value of this field changes unexpectedly, the iterator (or list * iterator) will throw a &#123;@code ConcurrentModificationException&#125; in * response to the &#123;@code next&#125;, &#123;@code remove&#125;, &#123;@code previous&#125;, * &#123;@code set&#125; or &#123;@code add&#125; operations. This provides * &lt;i&gt;fail-fast&lt;/i&gt; behavior, rather than non-deterministic behavior in * the face of concurrent modification during iteration. * * &lt;p&gt;&lt;b&gt;Use of this field by subclasses is optional.&lt;/b&gt; If a subclass * wishes to provide fail-fast iterators (and list iterators), then it * merely has to increment this field in its &#123;@code add(int, E)&#125; and * &#123;@code remove(int)&#125; methods (and any other methods that it overrides * that result in structural modifications to the list). A single call to * &#123;@code add(int, E)&#125; or &#123;@code remove(int)&#125; must add no more than * one to this field, or the iterators (and list iterators) will throw * bogus &#123;@code ConcurrentModificationExceptions&#125;. If an implementation * does not wish to provide fail-fast iterators, this field may be * ignored. */protected transient int modCount = 0; LinkedList 概述 LinkedList同时实现了List接口和Deque接口，它可以当作顺序容器，队列，栈。 LinkedList的实现方式决定了所有跟下表相关的操作都是线性时间，而在首段或者末尾删除元素只需要常数时间。为了追求效率，LinkedList没有实现同步，如果需要保证线程安全，可从这里了解。 源码解析 底层数据结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546transient int size = 0;/** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */transient Node&lt;E&gt; first;/** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */transient Node&lt;E&gt; last;private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125;/** * Constructs an empty list. */public LinkedList() &#123;&#125;/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection&#x27;s * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c);&#125; LinkedList底层是双向链表，每个节点用内部类Node表示。通过first和last引用分别指向链表的第一个和最后一个元素。当链表为空时，first和last都指向null。 getFirst，getLast 获取第一个元素和获取最后一个元素： 12345678910111213141516171819202122232425/** * Returns the first element in this list. * * @return the first element in this list * @throws NoSuchElementException if this list is empty */public E getFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return f.item;&#125;/** * Returns the last element in this list. * * @return the last element in this list * @throws NoSuchElementException if this list is empty */public E getLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return l.item;&#125; remove 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138/** * Removes and returns the first element from this list. * * @return the first element from this list * @throws NoSuchElementException if this list is empty */public E removeFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f);&#125;/** * Unlinks non-null first node f. */private E unlinkFirst(Node&lt;E&gt; f) &#123; // assert f == first &amp;&amp; f != null; final E element = f.item; final Node&lt;E&gt; next = f.next; f.item = null; f.next = null; // help GC first = next; if (next == null) last = null; else next.prev = null; size--; modCount++; return element;&#125;/** * Removes and returns the last element from this list. * * @return the last element from this list * @throws NoSuchElementException if this list is empty */public E removeLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l);&#125;/** * Unlinks non-null last node l. */private E unlinkLast(Node&lt;E&gt; l) &#123; // assert l == last &amp;&amp; l != null; final E element = l.item; final Node&lt;E&gt; prev = l.prev; l.item = null; l.prev = null; // help GC last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element;&#125;/** * Removes the first occurrence of the specified element from this list, * if it is present. If this list does not contain the element, it is * unchanged. More formally, removes the element with the lowest index * &#123;@code i&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt; * (if such an element exists). Returns &#123;@code true&#125; if this list * contained the specified element (or equivalently, if this list * changed as a result of the call). * * @param o element to be removed from this list, if present * @return &#123;@code true&#125; if this list contained the specified element */public boolean remove(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false;&#125;/** * Removes the element at the specified position in this list. Shifts any * subsequent elements to the left (subtracts one from their indices). * Returns the element that was removed from the list. * * @param index the index of the element to be removed * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125;/** * Unlinks non-null node x. */E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; remove有两个版本，一个用来删除指定下表的元素remove(int index)，一个用来删除与指定元素相等的第一个元素remove(Object o) 删除元素：指删除第一次出现的元素，如果没有这个元素，则返回false；判断的依据是equals方法，如果equals，则直接unlink这个node；由于LinkedList可以存放null元素，故而也可以删除第一次出现的null元素。 add 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * Appends the specified element to the end of this list. * * &lt;p&gt;This method is equivalent to &#123;@link #addLast&#125;. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; linkLast(e); return true;&#125;/** * Links e as last element. */void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125;/** * Inserts the specified element at the specified position in this list. * Shifts the element currently at that position (if any) and any * subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;/** * Inserts element e before non-null Node succ. */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;&#125;private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; add(E e)尾插法 add(int index,E e)在指定下标位置插入元素。当index == size时，等同于add(E e)；如果不是，则分成两步：1. 根据index找到要插入的位置，即node(index)方法；2. 修改引用，完成插入操作。 123456789101112131415161718/** * Returns the (non-null) Node at the specified element index. */Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; node(int index)方法中，通过index &lt; (size &gt;&gt; 1)来提高节点查询的效率，时间复杂度是O($\\frac{n}{2}$)。 addAll addAll(index, c) 实现方式并不是直接调用add(index,e)来实现，主要是因为效率的问题，另一个是fail-fast中modCount只会增加1次； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * Appends all of the elements in the specified collection to the end of * this list, in the order that they are returned by the specified * collection&#x27;s iterator. The behavior of this operation is undefined if * the specified collection is modified while the operation is in * progress. (Note that this will occur if the specified collection is * this list, and it&#x27;s nonempty.) * * @param c collection containing elements to be added to this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws NullPointerException if the specified collection is null */public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c);&#125;/** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in the list in the order that they are returned by the * specified collection&#x27;s iterator. * * @param index index at which to insert the first element * from the specified collection * @param c collection containing elements to be added to this list * @return &#123;@code true&#125; if this list changed as a result of the call * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; * @throws NullPointerException if the specified collection is null */public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) &#123; succ = null; pred = last; &#125; else &#123; succ = node(index); pred = succ.prev; &#125; for (Object o : a) &#123; @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; &#125; if (succ == null) &#123; last = pred; &#125; else &#123; pred.next = succ; succ.prev = pred; &#125; size += numNew; modCount++; return true;&#125; clear 为了让GC更快可以回收放置的元素，需要将node之间的引用关系赋空。 1234567891011121314151617181920/** * Removes all of the elements from this list. * The list will be empty after this call returns. */public void clear() &#123; // Clearing all of the links between nodes is &quot;unnecessary&quot;, but: // - helps a generational GC if the discarded nodes inhabit // more than one generation // - is sure to free memory even if there is a reachable Iterator for (Node&lt;E&gt; x = first; x != null; ) &#123; Node&lt;E&gt; next = x.next; x.item = null; x.next = null; x.prev = null; x = next; &#125; first = last = null; size = 0; modCount++;&#125; get，set 123456789101112131415161718192021222324252627/** * Returns the element at the specified position in this list. * * @param index index of the element to return * @return the element at the specified position in this list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;/** * Replaces the element at the specified position in this list with the * specified element. * * @param index index of the element to replace * @param element element to be stored at the specified position * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E set(int index, E element) &#123; checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal;&#125; 查找 找到返回下标，在、没找到返回-1。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Returns the index of the first occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the lowest index &#123;@code i&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. * * @param o element to search for * @return the index of the first occurrence of the specified element in * this list, or -1 if this list does not contain the element */public int indexOf(Object o) &#123; int index = 0; if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) return index; index++; &#125; &#125; else &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) return index; index++; &#125; &#125; return -1;&#125;/** * Returns the index of the last occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the highest index &#123;@code i&#125; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. * * @param o element to search for * @return the index of the last occurrence of the specified element in * this list, or -1 if this list does not contain the element */public int lastIndexOf(Object o) &#123; int index = size; if (o == null) &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (x.item == null) return index; &#125; &#125; else &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (o.equals(x.item)) return index; &#125; &#125; return -1;&#125; Coding code：数组和矩阵 283. Move Zeroes (Easy) 目标：给定一个数组，将数组中的0，移动到数组的右边。例如[0,1,2,0,3],改成[1,2,3,0,0] 1234567891011121314public static void moveZeroes(int[] nums) &#123; // 记录当前非零数的位置 int idx = 0; for (int num : nums) &#123; if (num != 0) &#123; // 元素不为0，将该元素放到非零位置上 nums[idx++] = num; &#125; &#125; // 将剩余的元素全部赋值为0 while (idx &lt; nums.length) &#123; nums[idx++] = 0; &#125;&#125; 566. Reshape the Matrix (Easy) 目的：改变矩阵的维度， 123456789101112131415public int[][] matrixReshape(int[][] nums, int r, int c) &#123; int m = nums.length, n = nums[0].length; if (m * n != r * c) &#123; return nums; &#125; int[][] reshapedNums = new int[r][c]; int index = 0; for (int i = 0; i &lt; r; i++) &#123; for (int j = 0; j &lt; c; j++) &#123; reshapedNums[i][j] = nums[index / n][index % n]; index++; &#125; &#125; return reshapedNums;&#125; 485. Max Consecutive Ones (Easy) 目的：找到数组中最长连续的1的长度 12345678public int findMaxConsecutiveOnes(int[] nums) &#123; int max = 0, cur = 0; for (int x : nums) &#123; cur = x == 0 ? 0 : cur + 1; max = Math.max(max, cur); &#125; return max;&#125; code：链表 参考 ♥数据结构基础知识体系详解♥ | Java 全栈知识体系 (pdai.tech) java实现AVL树_java实现2-3树-CSDN博客 红黑树(五)之 Java的实现 - 如果天空不死 - 博客园 (cnblogs.com)","categories":[{"name":"算法","slug":"算法","permalink":"https://solitaire-12.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://solitaire-12.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"author":"织秋"},{"title":"Java|集合特性测试","slug":"Java/集合","date":"2024-03-19T08:40:55.000Z","updated":"2024-03-19T08:47:27.895Z","comments":true,"path":"2024/03/19/Java/集合/","permalink":"https://solitaire-12.github.io/2024/03/19/Java/%E9%9B%86%E5%90%88/","excerpt":"","text":"小结 Collection 接口的接口 对象的集合（单列集合） List 接口：元素按进入先后有序保存，可重复 LinkedList：链表， 插入删除， 没有同步， 线程不安全 ArrayList：数组， 随机访问， 没有同步， 线程不安全 Vector ：数组， 同步， 线程安全 Stack：是Vector类的实现类 Set 接口： 仅接收一次，不可重复，并做内部排序 HashSet：使用hash表（数组）存储元素 LinkedHashSet：链表维护元素的插入次序 TreeSet：底层实现为二叉树，元素排好序 Map 接口：键值对的集合 （双列集合） Hashtable：同步， 线程安全 HashMap：没有同步， 线程不安全 LinkedHashMap：双向链表和哈希表实现 WeakHashMap TreeMap：红黑树对所有的key进行排序 IdentifyHashMap 123456789101112131415161718192021222324252627282930private static &lt;T extends AbstractCollection&gt; void setInfo(T s) &#123;setInfo(s,null);&#125;private static &lt;T extends AbstractCollection&gt; void setInfo(T s, String info) &#123; String typeName = s.getClass().getTypeName().split(&quot;\\\\.&quot;)[2]; System.out.println(&quot;+======&quot; + typeName + &quot;=======================&quot;); System.out.print(&quot;| &quot; + typeName + &quot;源数据：&quot;); for (int i = 0; i &lt; 10; i++) &#123; Random random = new Random(); int i1 = random.nextInt(10); if (info != null &amp;&amp; !info.isEmpty()) &#123; System.out.print(info + i1 + &quot;,&quot;); s.add(info + i1); &#125; else &#123; System.out.print(i1 + &quot;,&quot;); s.add(i1); &#125; try &#123; if (i == 7)&#123; s.add(null); &#125; &#125;catch (Exception e)&#123; System.out.print(&quot;[尝试写入null数据,出现异常：e = &quot; + e.getClass().getTypeName().split(&quot;\\\\.&quot;)[2]+&quot;],&quot;); &#125; &#125; System.out.println(); System.out.print(&quot;| &quot; + typeName + &quot;处理后：&quot;); s.forEach(a -&gt; System.out.print(a + &quot;,&quot;)); System.out.println(); System.out.println(&quot;+========================================&quot;);&#125; List 校验array和linked的区别 123456789101112131415161718192021222324252627282930313233343536373839404142434445private static &lt;T extends AbstractList&gt; T setList(T t, int num) &#123; for (int i = 0; i &lt; num; i++) &#123; Random random = new Random(); int i1 = random.nextInt(10); t.add(i1); &#125; return t;&#125;private static &lt;T extends AbstractList&gt; T addOne(T t, int e) &#123; t.add(50000,e); return t;&#125;private static &lt;T extends AbstractList&gt; T removeOne(T t, int index) &#123; t.remove(index); return t;&#125;private static &lt;T extends AbstractList&gt; void getOne(T t, int index)&#123; t.get(index);&#125;private static &lt;T extends AbstractList&gt; T consume(T t, String a, int num) &#123; String typeName = t.getClass().getTypeName().split(&quot;\\\\.&quot;)[2]; if (a != null &amp;&amp; a.equals(&quot;a&quot;)) &#123; long startTime = System.currentTimeMillis(); addOne(t, num); System.out.println(typeName + &quot;:addOne:总计耗时：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); &#125; if (a != null &amp;&amp; a.equals(&quot;r&quot;)) &#123; long startTime = System.currentTimeMillis(); removeOne(t, num); System.out.println(typeName + &quot;:removeOne:总计耗时：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); &#125; if (a != null &amp;&amp; a.equals(&quot;g&quot;)) &#123; long startTime = System.currentTimeMillis(); getOne(t, num); System.out.println(typeName + &quot;:getOne:总计耗时：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); &#125; if (a == null) &#123; long startTime = System.currentTimeMillis(); setList(t, num); System.out.println(typeName + &quot;:setList:总计耗时：&quot; + (System.currentTimeMillis() - startTime) + &quot;ms&quot;); &#125; return t;&#125; 123456789101112public void testList() &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); ArrayList&lt;Integer&gt; list = consume(arrayList, null, 10_000_000); consume(list, &quot;a&quot;, 5); consume(list, &quot;r&quot;, 5_000); consume(list, &quot;g&quot;, 1000_000); LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;&gt;(); LinkedList&lt;Integer&gt; list1 = consume(linkedList, null, 10_000_000); consume(list1, &quot;a&quot;, 5); consume(list1, &quot;r&quot;, 5_000); consume(list1, &quot;g&quot;, 1000_000);&#125; 12345678ArrayList:setList:总计耗时：629msArrayList:addOne:总计耗时：4msArrayList:removeOne:总计耗时：4msArrayList:getOne:总计耗时：0msLinkedList:setList:总计耗时：1901msLinkedList:addOne:总计耗时：1msLinkedList:removeOne:总计耗时：0msLinkedList:getOne:总计耗时：3ms 校验null值 123456public void testList1() &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); setInfo(arrayList); LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;&gt;(); setInfo(linkedList);&#125; 12345678+======ArrayList=======================| ArrayList源数据：4,5,1,1,8,9,8,2,2,8,| ArrayList处理后：4,5,1,1,8,9,8,2,null,2,8,+========================================+======LinkedList=======================| LinkedList源数据：7,6,4,8,0,3,5,9,4,2,| LinkedList处理后：7,6,4,8,0,3,5,9,null,4,2,+======================================== 校验线程安全问题 123456789101112131415161718192021222324252627282930313233public void testSafe() &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); inThread(arrayList, 3, 1000); LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;&gt;(); inThread(linkedList, 3, 1000);&#125;private static &lt;T extends AbstractList&gt; void inThread(T t, int tNum, int eNum) &#123; Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; tNum; i++) &#123; Thread thread1 = new Thread(() -&gt; setList(t, eNum)); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(eNum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //System.out.println( // MessageFormat.format(&quot;在&#123;0&#125;个线程,每线程添加&#123;1&#125;个元素的情况下,&#123;2&#125;中元素数量是:&#123;3&#125;&quot;, // tNum,eNum,t.getClass().getTypeName().split(&quot;\\\\.&quot;)[2],t.size())); System.out.println( String.format(&quot;在%1$s个线程,每线程添加%2$s个元素的情况下,%3$s中元素数量是:%4$s&quot;, tNum,eNum,t.getClass().getTypeName().split(&quot;\\\\.&quot;)[2],t.size()));&#125;// 在3个线程,每线程添加1000个元素的情况下,ArrayList中元素数量是:1242// 在3个线程,每线程添加1000个元素的情况下,LinkedList中元素数量是:2887 list总结 数据可重复,可为null ArrayList和LinkedList区别 两者都不保证线程安全 ArrayList查询快,增删慢LinkedList查询慢,增删快 上述代码中：为什么两者同样是插入10_000_000条数据,LinkedList要比ArrayList慢这么多？ 这是因为setList方法中用来添加元素的方式是尾插法， 参考可以了解到，在大数据量的情况下，ArrayList的扩容机制同LinkedList的Node机制要更有优势。 为什么线程不安全呢？ ArrayList不安全的原因： 首先明确ArrayList添加元素的操作是在add元素时，通过size下标进行写入集合，这一过程中用到了size++操作，这是无法保证原子性的操作，也就导致ArrayList在多线程环境下，可能会触发数组下标越界的错误ArrayIndexOutOfBoundsException，或者size同时被多个线程读取的问题。 LinkdedList不安全的原因： 首先明确LinkedList API中添加元素方法有多种，这里就用linkLast方法为例。该方法涉及变量尾指针``Node last，下一个指针 Node newNode，大小size,以及修改次数modCount。方法内size++和modCount++这两个操作无法保证原子性，导致在多线程环境下，存在多个同时获取相同的size和modCount`数值的线程，从而导致尾指针出现异常情况。 如何保证线程安全呢？ 12Collection&lt;Integer&gt; synchronizedCollection = Collections.synchronizedCollection(new ArrayList&lt;Integer&gt;()); 1CopyOnWriteArrayList&lt;Integer&gt; copyOnWriteArrayList = new CopyOnWriteArrayList&lt;&gt;(); 方式1：属于java.util.Collections，通过synchronized实现线程安全。直接锁集合对象，性能不太行。 12345SynchronizedCollection(Collection&lt;E&gt; c) &#123; this.c = Objects.requireNonNull(c); mutex = this;&#125;public boolean add(E e) &#123;synchronized (mutex) &#123;return c.add(e);&#125;&#125; 方式2：属于java.util.concurrent.CopyOnWriteArrayList，诸如add，remove，set中存在着copyOf操作，导致该api在处理大数据量的时候，性能表现极差。下面是源码： 1234567891011121314151617181920public boolean add(E e) &#123; final ReentrantLock lock = this.lock; // 加锁 lock.lock(); try &#123; // 获取当前元素集 Object[] elements = getArray(); // 获取长度 int len = elements.length; // 拷贝一份容量比原先大1 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将元素加入到新元素集合 newElements[len] = e; // 将新元素集合写到list中 setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 两者都没有继承AbstractList Set 校验特性 1234567891011121314TreeSet&lt;Integer&gt; treeSet = new TreeSet&lt;&gt;();setInfo(treeSet,null);HashSet&lt;Integer&gt; hashSet = new HashSet&lt;&gt;();setInfo(hashSet, null);LinkedHashSet&lt;Integer&gt; linkedHashSet = new LinkedHashSet&lt;&gt;();setInfo(linkedHashSet,null);System.out.println(&quot;TreeSet和HashSet的区别&quot;);TreeSet&lt;Integer&gt; treeSet2 = new TreeSet&lt;&gt;();setInfo(treeSet2, &quot;号&quot;);HashSet&lt;Integer&gt; hashSet2 = new HashSet&lt;&gt;();setInfo(hashSet2, &quot;号&quot;); 123456789101112131415161718192021+======TreeSet=======================| TreeSet源数据：6,0,8,6,6,9,6,2,[尝试写入null数据,出现异常：e = NullPointerException],1,3,| TreeSet处理后：0,1,2,3,6,8,9,+========================================+======HashSet=======================| HashSet源数据：4,0,2,5,5,7,1,7,5,5,| HashSet处理后：0,null,1,2,4,5,7,+========================================+======LinkedHashSet=======================| LinkedHashSet源数据：4,4,1,7,5,3,1,1,0,3,| LinkedHashSet处理后：4,1,7,5,3,null,0,+========================================TreeSet和HashSet的区别+======TreeSet=======================| TreeSet源数据：号1,号5,号5,号4,号4,号5,号6,号0,[尝试写入null数据,出现异常：e = NullPointerException],号7,号0,| TreeSet处理后：号0,号1,号4,号5,号6,号7,+========================================+======HashSet=======================| HashSet源数据：号8,号6,号9,号2,号1,号5,号0,号6,号8,号8,| HashSet处理后：号1,null,号2,号0,号5,号6,号9,号8,+======================================== 校验线程安全 123456789101112131415161718private static &lt;T extends AbstractSet&gt; void inThread(T t, int tNum, int eNum) &#123; Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; tNum; i++) &#123; Thread thread1 = new Thread(() -&gt; setSet(t, eNum)); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(eNum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println( String.format(&quot;在%1$s个线程,每线程添加%2$s个元素的情况下,%3$s中元素数量是:%4$s&quot;, tNum, eNum, t.getClass().getTypeName().split(&quot;\\\\.&quot;)[2], t.size()));&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public void testSetSafe() &#123; // Testing whether a TreeSet is thread-safe TreeSet&lt;Integer&gt; treeSet = new TreeSet&lt;&gt;(); inThread(treeSet, 3, 1000); // Implementing thread-safe TreeSet Collection&lt;Integer&gt; synchronizedTreeSet = Collections.synchronizedCollection(new TreeSet&lt;Integer&gt;()); Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 3; i++) &#123; Thread thread1 = new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; synchronizedTreeSet.add(j); &#125; &#125;); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;synchronizedTreeSet:&quot; + synchronizedTreeSet.size()); // Testing whether a HashSet is thread-safe HashSet&lt;Integer&gt; hashSet = new HashSet&lt;&gt;(); inThread(hashSet,3,1000); // Implementing thread-safe HashSet Collection&lt;Integer&gt; synchronizedHashSet = Collections.synchronizedCollection(new HashSet&lt;&gt;()); Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 3; i++) &#123; Thread thread1 = new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; synchronizedHashSet.add(j); &#125; &#125;); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;synchronizedHashSet:&quot; + synchronizedHashSet.size()); // Testing whether a LinkedHashSet is thread-safe LinkedHashSet&lt;Integer&gt; linkedHashSet = new LinkedHashSet&lt;&gt;(); inThread(linkedHashSet,3,1000); // Implementing thread-safe LinkedHashSet Collection&lt;Integer&gt; synchronizedLinkedHashSet = Collections.synchronizedCollection(new LinkedHashSet&lt;&gt;()); Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 3; i++) &#123; Thread thread1 = new Thread(() -&gt; &#123; for (int j = 0; j &lt; 1000; j++) &#123; synchronizedLinkedHashSet.add(j); &#125; &#125;); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;synchronizedLinkedHashSet:&quot; + synchronizedLinkedHashSet.size());&#125; 结果 123456789java.lang.NullPointerException在3个线程,每线程添加1000个元素的情况下,TreeSet中元素数量是:1005synchronizedTreeSet::1000在3个线程,每线程添加1000个元素的情况下,HashSet中元素数量是:1110synchronizedHashSet:1000在3个线程,每线程添加1000个元素的情况下,LinkedHashSet中元素数量是:1055synchronizedLinkedHashSet:1000 set总结 保证数据是唯一的 TreeSet，HashSet，LinkedHashSet三者的区别 TreeSet不能存储null数据HashSet和LinkedHashSet可以存储null数据 TreeSet可以保证数据有序HashSet可以保证重写hashcode的数据类型有序LinkedHashSet可以保留插入顺序 TreeSet底层是二叉树HashSet底层是hashLinkedHashSet底层是链表hash TreeSet调了一个NavigableMap，其插入操作会去调用TreeMap中的put方法。多线程中，在调用put时，可能遇到多个线程在添加同一个元素的情况出现，这使compare的结果一致，从而触发插入多个同一元素的情况。 1234567891011private transient NavigableMap&lt;E,Object&gt; m; private static final Object PRESENT = new Object(); TreeSet(NavigableMap&lt;E,Object&gt; m) &#123; this.m = m;&#125; public TreeSet() &#123; this(new TreeMap&lt;E,Object&gt;());&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public V put(K key, V value) &#123; Entry&lt;K,V&gt; t = root; if (t == null) &#123; compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) &#123; do &#123; parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; else &#123; if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do &#123; parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; fixAfterInsertion(e); size++; modCount++; return null;&#125; HashSet内部调了个HashMap，其插入操作会去调用HashMap中的put方法。多线程中，在调用put时，可能遇到多个线程在添加同一个元素的情况出现，这使对比hash的结果一致，从而触发插入多个同一元素的情况。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; LinkedHashSet是链表结构的HashSet，调的都是HashMap。 Map 校验特性 Hashtable 123456789101112131415161718Hashtable&lt;Integer, Integer&gt; hashtable = new Hashtable&lt;&gt;();try &#123; hashtable.put(1,1); hashtable.put(1,2); hashtable.put(3,3); hashtable.put(4,null); hashtable.put(null,2);&#125;catch (Exception e)&#123; System.out.println(&quot;e = &quot; + e);&#125;hashtable.keySet().forEach((key)-&gt;&#123; System.out.print(&quot;key = &quot; + key); System.out.println(&quot;,value = &quot;+ hashtable.get(key));&#125;);// e = java.lang.NullPointerException// key = 3,value = 3// key = 1,value = 2 结果： key和value都不能为空 同key覆盖 不建议使用HashTable，Oracle官方也将其废弃，建议在多线程环境下使用ConcurrentHashMap类。 线程安全 1234567891011121314151617181920212223242526272829Hashtable&lt;Integer, Integer&gt; hashtable = new Hashtable&lt;&gt;();inThread(hashtable,3,1000);private static &lt;T extends Dictionary&gt; void inThread(T t, int tNum, int eNum) &#123; Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; tNum; i++) &#123; Thread thread1 = new Thread(() -&gt; setMap(t, eNum)); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(eNum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println( String.format(&quot;在%1$s个线程,每线程添加%2$s个元素的情况下,%3$s中元素数量是:%4$s&quot;, tNum, eNum, t.getClass().getTypeName().split(&quot;\\\\.&quot;)[2], t.size()));&#125;private static &lt;T extends Dictionary&gt; T setMap(T t, int num) &#123; for (int i = 0; i &lt; num; i++) &#123; t.put(i, i); &#125; return t;&#125;// 在3个线程,每线程添加1000个元素的情况下,Hashtable中元素数量是:1000 HashMap 12345678910111213141516171819202122HashMap&lt;Integer, Integer&gt; hashMap = new HashMap&lt;&gt;();try&#123; hashMap.put(1,1); hashMap.put(1,2); hashMap.put(3,3); hashMap.put(4,null); hashMap.put(null,2);&#125;catch (Exception e)&#123; System.out.println(&quot;e = &quot; + e);&#125;hashMap.keySet().forEach((key)-&gt;&#123; System.out.print(&quot;key = &quot; + key); System.out.println(&quot;,value = &quot;+ hashMap.get(key));&#125;);//key = null,value = 2//key = 1,value = 2//key = 3,value = 3//key = 4,value = nullHashMap&lt;Integer, Integer&gt; hashMap = new HashMap&lt;&gt;();inThread(hashMap,3,1000);//在3个线程,每线程添加1000个元素的情况下,HashMap中元素数量是:1020 结果： 允许key和value为null 线程不安全 LinkedMap 1234567891011121314151617181920212223LinkedHashMap&lt;Integer, Integer&gt; linkedHashMap = new LinkedHashMap&lt;&gt;();try&#123; linkedHashMap.put(1,1); linkedHashMap.put(1,2); linkedHashMap.put(3,3); linkedHashMap.put(4,null); linkedHashMap.put(null,2);&#125;catch (Exception e)&#123; System.out.println(&quot;e = &quot; + e);&#125;linkedHashMap.keySet().forEach(key-&gt;&#123; System.out.print(&quot;key = &quot; + key); System.out.println(&quot;,value = &quot;+ linkedHashMap.get(key));&#125;);//key = 1,value = 2//key = 3,value = 3//key = 4,value = null//key = null,value = 2LinkedHashMap&lt;Integer, Integer&gt; linkedHashMap = new LinkedHashMap&lt;&gt;();inThread(linkedHashMap,3,1000);// 在3个线程,每线程添加1000个元素的情况下,LinkedHashMap中元素数量是:1157 结果： 源码，其继承自HashMap 1public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; 线程不安全 TreeMap 1234567891011121314151617181920212223TreeMap&lt;Integer, Integer&gt; treeMap = new TreeMap&lt;&gt;();try &#123; treeMap.put(1, 1); treeMap.put(1, 2); treeMap.put(3, 3); treeMap.put(4, null); treeMap.put(null, 2);&#125; catch (Exception e) &#123; System.out.println(&quot;e = &quot; + e);&#125;treeMap.keySet().forEach((key) -&gt; &#123; System.out.print(&quot;key = &quot; + key); System.out.println(&quot;,value = &quot; + treeMap.get(key));&#125;);//e = java.lang.NullPointerException//key = 1,value = 2//key = 3,value = 3//key = 4,value = nullTreeMap&lt;Integer, Integer&gt; treeMap = new TreeMap&lt;&gt;();inThread(treeMap,3,1000);// NullPointerException// 在3个线程,每线程添加1000个元素的情况下,TreeMap中元素数量是:1016 结果： 不允许key为null，value可以为null 同key覆盖 线程不安全 ConcurrentHashMap 12345678910111213141516171819202122ConcurrentHashMap&lt;Integer, Integer&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;();try &#123; concurrentHashMap.put(1, 1); concurrentHashMap.put(1, 2); concurrentHashMap.put(3, 3); concurrentHashMap.put(4, null); concurrentHashMap.put(null, 2);&#125; catch (Exception e) &#123; System.out.println(&quot;e = &quot; + e);&#125;concurrentHashMap.keySet().forEach((key) -&gt; &#123; System.out.print(&quot;key = &quot; + key); System.out.println(&quot;,value = &quot; + concurrentHashMap.get(key));&#125;);// e = java.lang.NullPointerException// key = 1,value = 2// key = 3,value = 3ConcurrentHashMap&lt;Integer, Integer&gt; concurrentHashMap = new ConcurrentHashMap&lt;&gt;();inThread(concurrentHashMap,3,1000);//在3个线程,每线程添加1000个元素的情况下,concurrent中元素数量是:1000 结果： 属于java.util.concurrent，但继承了AbstractMap 不允许key，value为null 线程安全 检验线程安全代码 123456789101112131415161718192021222324private static &lt;T extends AbstractMap&gt; void inThread(T t, int tNum, int eNum) &#123; Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; tNum; i++) &#123; Thread thread1 = new Thread(() -&gt; setMap(t, eNum)); thread1.start(); &#125; &#125;); thread.start(); try &#123; thread.join(); Thread.sleep(eNum); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println( String.format(&quot;在%1$s个线程,每线程添加%2$s个元素的情况下,%3$s中元素数量是:%4$s&quot;, tNum, eNum, t.getClass().getTypeName().split(&quot;\\\\.&quot;)[2], t.size()));&#125;private static &lt;T extends AbstractMap&gt; T setMap(T t, int num) &#123; for (int i = 0; i &lt; num; i++) &#123; t.put(i, i); &#125; return t;&#125; java.util.concurrent 集合类在java.util，为解决并发问题，提供了java.util.concurrent解决方案。比如常用的HashMap并发解决方案的ConcurrentHashMap就在这个包下。更多内容多线程-concurrent中介绍。 Collections 对collection操作的工具类。 摘要几个常用的API 方法 描述 sort(List&lt;T&gt; list [, Comparator&lt;? super T&gt; c]) 对list进行排序；重载一个比较器版本 synchronizedCollection(Collection&lt;T&gt; c) synchronizedList(List&lt;T&gt; list)synchronizedMap(Map&lt;K,V&gt; map)synchronizedSet(Set&lt;T&gt; set) 提供并发解决方案 min/max (Collection&lt;T&gt; c [, Comparator&lt;? super T&gt; c]) 获取最小/最大值 reverse(List&lt;T&gt; list) 反转列表 replaceAll(List&lt;T&gt; list,T oldVal,T newVal) 替换列表中元素 swap(List&lt;T&gt; list,int i,int j) 列表中元素交换位置 copy(List&lt;? super T&gt; dest, List&lt;? extends T&gt; src) src往dest拷贝，确保src.size比dest.size小","categories":[{"name":"Java","slug":"Java","permalink":"https://solitaire-12.github.io/categories/Java/"}],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://solitaire-12.github.io/tags/Java%E5%9F%BA%E7%A1%80/"}]},{"title":"Java|泛型","slug":"Java/泛型","date":"2024-03-19T08:10:55.000Z","updated":"2024-03-27T01:40:14.201Z","comments":true,"path":"2024/03/19/Java/泛型/","permalink":"https://solitaire-12.github.io/2024/03/19/Java/%E6%B3%9B%E5%9E%8B/","excerpt":"","text":"为啥引入泛型 进入话题前，先看一段代码： 123456private static Integer add(Integer a,Integer b)&#123; return a+b;&#125;private static Double add(Double a,Double b)&#123; return a+b;&#125; 通常情况下，我们给一个方法设定形参时，会直接写上形参的类型。但是，当我们的方法传参不确定类型时，例如方法add的形参是Integer,Integer或者是Double,Double,这时要是传来Integer,Double，你可能会去写重载该方法，使其达到一个方法实现接受几种参数类型这就很折磨人了。为了解决这种麻烦，引入泛型。 引用泛型 123private static &lt;T extends Number&gt; double addt(T a,T b)&#123; return a.doubleValue()+b.doubleValue();&#125; 这样，无论调用这传递什么样的数字类型，都可以调用该方法，不用再去重载方法了。 在集合中，List&lt;String&gt;代表着List只能存放String类型，用来表示约束。 泛型的使用 泛型类 单一泛型类： 123456789101112131415161718192021222324class Hello&lt;T&gt;&#123; private T word; public T getWord() &#123; return word; &#125; public void setWord(T word) &#123; this.word = word; &#125;&#125;public class test &#123; public static void main(String[] args) &#123; Hello&lt;String&gt; hello = new Hello&lt;String&gt;(); hello.setWord(&quot;你好，泛型&quot;); // 你好，泛型 System.out.println(hello.getWord()); Hello&lt;Integer&gt; hello1 = new Hello&lt;Integer&gt;(); hello1.setWord(12323); // 12323 System.out.println(hello1.getWord()); &#125;&#125; 多元泛型类： 123456789101112131415161718192021222324252627282930class Notepad&lt;K,V&gt;&#123; private K key; private V value; public K getKey() &#123; return key; &#125; public void setKey(K key) &#123; this.key = key; &#125; public V getValue() &#123; return value; &#125; public void setValue(V value) &#123; this.value = value; &#125;&#125;public class test &#123; public static void main(String[] args) &#123; Notepad&lt;String, Integer&gt; notepad = new Notepad&lt;&gt;(); notepad.setKey(&quot;tom&quot;); notepad.setValue(20); System.out.println(notepad.getKey()); System.out.println(notepad.getValue()); &#125;&#125; 泛型接口 1234567891011121314151617181920212223242526interface Hello&lt;T&gt;&#123; public T getWord();&#125;class HelloImpl&lt;T&gt; implements Hello&lt;T&gt;&#123; private T word; public HelloImpl(T word) &#123; this.word = word; &#125; public void setWord(T word) &#123; this.word = word; &#125; @Override public T getWord() &#123; return this.word; &#125;&#125;public class test &#123; public static void main(String[] args) &#123; HelloImpl&lt;String&gt; hello = new HelloImpl&lt;String&gt;(&quot;你好,接口泛型&quot;); System.out.println(hello.getWord()); &#125;&#125; 泛型方法 123public &lt;T&gt; T get(T a) &#123; return a;&#125; &lt;T&gt;:声明一个泛型T T:声明方法的返回类型是泛型T get(T a):方法的形参是一个泛型T的变量 上述中，1是必须申明的，2，3可以视情况使用其中之一或者两者一起使用。假如两者都使用，虽然没有问题，但是方法却没有涉及&lt;T&gt;，属于无效代码。 参考：泛型机制详解](https://pdai.tech/md/java/basic/java-basic-x-generic.html)","categories":[{"name":"Java","slug":"Java","permalink":"https://solitaire-12.github.io/categories/Java/"}],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://solitaire-12.github.io/tags/Java%E5%9F%BA%E7%A1%80/"}]},{"title":"算法|排序","slug":"algorithm/sort","date":"2024-03-19T07:33:11.324Z","updated":"2024-03-19T08:13:17.393Z","comments":true,"path":"2024/03/19/algorithm/sort/","permalink":"https://solitaire-12.github.io/2024/03/19/algorithm/sort/","excerpt":"","text":"认识位运算符 获取整数的二进制32位字符 123456public static void print(int num) &#123; for (byte i = 31; i &gt;= 0; i--) &#123; System.out.print((num &amp; (1 &lt;&lt; i)) == 0 ? &quot;0&quot; : &quot;1&quot;); &#125; System.out.println();&#125; 结果： 123456789101112131415byte a = 43;byte b = 120;System.out.print(&quot;a :&quot;);print(a); //a :00101011System.out.print(&quot;b :&quot;);print(b); //b :01111000System.out.print(&quot;a|b:&quot;);print(a | b); //a|b:01111011System.out.print(&quot;a&amp;b:&quot;);print(a &amp; b); //a&amp;b:00101000System.out.print(&quot;a^b:&quot;);print(a ^ b); //a^b:01010011System.out.print(&quot; ~b:&quot;);print(~b); // ~b:10000111System.out.print(&quot;135:&quot;);print(135); //135:10000111System.out.print(&quot;a :&quot;);print(a); //a :00101011System.out.print(&quot;&lt;&lt;3:&quot;);print(a&lt;&lt;3); //&lt;&lt;3:01011000System.out.print(&quot;&gt;&gt;1:&quot;);print(a&gt;&gt;1); //&gt;&gt;1:00010101System.out.print(&quot;&gt;&gt;&gt;1:&quot;);print(a&gt;&gt;&gt;1); //&gt;&gt;&gt;1:00010101System.out.println(~b); //-121print(~b&gt;&gt;7); //11111111 |：或 1234//00101011//01111000//01111011print(43 | 120); //01111011 有1为1 &amp;：与 1234//00101011//01111000//00101000print(43 &amp; 120); //00101000 有0为0 ^：亦或 1234//00101011//01111000//01010011print(43 ^ 120); //01010011 无进位相加 ~：非 取反 1234//01111000//10000111print(~120); //10000111print(135); //10000111 ~120和135的字符是一样的，为什么它是-121不是135呢？ 首先理解，byte 占 8位（-128-127），其中[符号位]占一位，其余数据占用 其次当一个数是负数的时候，符号位以及其左边的位置，均为1。也就是说，-121是//1111....110000111，而135是//0000.....010000111 最后，~操作可以理解成在一条直线上，从[0和-1的中间位置]反方向去找到第n个点 注意：在程序中，0算正数。后文会验证这一结论。 &lt;&lt;：左移 12print(43); //00101011print(43&lt;&lt;3); //01011000 &gt;&gt;：右移 12print(43); //00101011print(43&gt;&gt;1); //00010101 &gt;&gt;&gt;：无符号右移 123print(43&gt;&gt;&gt;1); //00010101print(~120); //10000111print(~120&gt;&gt;7); //11111111 正数&gt;&gt;&gt; 等价 &gt;&gt; 常见使用 判断奇偶性 1(num &amp; 1) == 1 ? &quot;奇数&quot; : &quot;偶数&quot; 两个数做交换 123456789a = a ^ b;b = a ^ b;a = a ^ b;// 注意：在数组操作中时，切记不能进行用位置亦或，不然数据会丢失//可以简化成a ^= b;b ^= a;a ^= b; 取绝对值 12int b = -327int a = (b ^ (b&gt;&gt;31)) - (b&gt;&gt;31); 判断两个数是否异号 123456boolean f = ((x ^ y) &lt; 0);// 同号false，异号true// boolean f = ((0 ^ 1) &lt; 0);// System.out.println(f);// 结果是false 大小写转换 123456// (&#x27;A&#x27; | &#x27; &#x27;) = &#x27;a&#x27;// (&#x27;a&#x27; | &#x27; &#x27;) = &#x27;a&#x27;// (&#x27;A&#x27; &amp; &#x27;_&#x27;) = &#x27;A&#x27;// (&#x27;a&#x27; &amp; &#x27;_&#x27;) = &#x27;A&#x27;// (&#x27;A&#x27; ^ &#x27; &#x27;) = &#x27;a&#x27;// (&#x27;a&#x27; ^ &#x27; &#x27;) = &#x27;A&#x27; +1 12int n = 10;n = -~n; -1 12int n = 10;n = ~-n; 排序 排序动态展示（冒泡排序，选择排序，插入排序，归并排序，快速排序，计数排序，基数排序） - VisuAlgo 打印 1Arrays.stream(arr).forEach(a-&gt; System.out.print(a + &quot; &quot;)); 交换 12345public static void swap(int[] arr, int i, int j) &#123; int tmp = arr[j]; arr[j] = arr[i]; arr[i] = tmp;&#125; 选择排序 12345678910111213public static void selectSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; int N = arr.length; for (int i = 0; i &lt; N; i++) &#123; int minValueIndex = i; for (int j = i + 1; j &lt; N; j++) &#123; minValueIndex = arr[j] &lt; arr[minValueIndex] ? j : minValueIndex; &#125; swap(arr, i, minValueIndex); &#125;&#125; 冒泡排序 12345678910111213public static void bubbleSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; int N = arr.length; for (int end = N - 1; end &gt;= 0; end--) &#123; for (int second = 1; second &lt;= end; second++) &#123; if (arr[second - 1] &gt; arr[second]) &#123; swap(arr, second - 1, second); &#125; &#125; &#125;&#125; 插入排序 12345678910111213141516171819202122232425public static void insertSort1(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; int N = arr.length; for (int end = 1; end &lt; N; end++) &#123; int newNumIndex = end; while (newNumIndex - 1 &gt;= 0 &amp;&amp; arr[newNumIndex - 1] &gt; arr[newNumIndex]) &#123; swap(arr, newNumIndex - 1, newNumIndex); newNumIndex--; &#125; &#125;&#125;public static void insertSort2(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; int N = arr.length; for (int end = 1; end &lt; N; end++) &#123; for (int pre = end - 1; pre &gt;= 0 &amp;&amp; arr[pre] &gt; arr[pre + 1]; pre--) &#123; swap(arr, pre, pre + 1); &#125; &#125;&#125; 归并排序 123456789101112131415161718192021222324252627282930public static void mergeSort(int[] arr, int left, int right) &#123; if (left == right) return; //分成两半 int mid = left + (right-left)/2; //左边排序 mergeSort(arr, left, mid); //右边排序 mergeSort(arr, mid+1, right); merge(arr, left, mid+1, right);&#125;static void merge(int[] arr, int leftPtr, int rightPtr, int rightBound) &#123; int mid = rightPtr - 1; int[] temp = new int[rightBound - leftPtr + 1]; int i = leftPtr; int j = rightPtr; int k = 0; while(i &lt;= mid &amp;&amp; j &lt;= rightBound) &#123; temp[k++] = arr[i] &lt;= arr[j] ? arr[i++] : arr[j++]; &#125; while(i&lt;=mid) temp[k++] = arr[i++]; while(j&lt;=rightBound) temp[k++] = arr[j++]; for(int m=0; m&lt;temp.length; m++) arr[leftPtr +m] = temp[m];&#125; 快速排序 123456789101112131415161718192021222324public static void quickSort(int[] arr, int leftBound, int rightBound) &#123; if(leftBound &gt;= rightBound) return; int mid = partition(arr, leftBound, rightBound); quickSort(arr, leftBound, mid-1); quickSort(arr, mid+1, rightBound);&#125;static int partition(int[] arr, int leftBound, int rightBound) &#123; //adding the following code blocks is the random quick sort &#123; int random = leftBound + (int) (Math.random() * (rightBound - leftBound + 1)); swap(arr, random, rightBound); &#125; int pivot = arr[rightBound]; int left = leftBound; int right = rightBound - 1; while(left &lt;= right) &#123; while(left &lt;= right &amp;&amp; arr[left] &lt;= pivot) left ++; while(left &lt;= right &amp;&amp; arr[right] &gt; pivot) right --; if(left &lt; right) BaseCoding.swap(arr, left, right); &#125; BaseCoding.swap(arr, left, rightBound); return left;&#125; 计数排序 12345678910111213141516171819202122232425/** * 计数排序： * 遍历源数组找到最大值，通过这最大值创建一个计数数组； * 遍历源数组，在计数数组，源数组值位置，++； * 遍历计数数组，将数值大于0位置，以此写入源数组。 * * @param arr 原数组 */public static void countSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; int max = Integer.MIN_VALUE; for (int i = 0; i &lt; arr.length; i++) &#123; max = Math.max(max, arr[i]); &#125; int[] bucket = new int[max + 1]; for (int i = 0; i &lt; arr.length; i++) &#123; bucket[arr[i]]++; &#125; int i = 0; for (int j = 0; j &lt; bucket.length; j++) &#123; while (bucket[j]-- &gt; 0) &#123; arr[i++] = j; &#125; &#125;&#125; 堆排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 堆排序: * * @param arr 源数组 */public static void heapSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) return; // O(N*logN) // for (int i = 0; i &lt; arr.length; i++) &#123; // heapInsert(arr, i); // &#125; // O(N) for (int i = arr.length - 1; i &gt;= 0; i--) &#123; heapify(arr, i, arr.length); &#125; int heapSize = arr.length; swap(arr, 0, --heapSize); // O(N*logN) while (heapSize &gt; 0) &#123; heapify(arr, 0, heapSize); swap(arr, 0, --heapSize); &#125;&#125;private static void heapify(int[] arr, int index, int heapSize) &#123; int left = (index &lt;&lt; 1) + 1; while (left &lt; heapSize) &#123; // 下方还有孩子的时候 // 两个孩子中，谁的值大，把下标给largest // 1）只有左孩子，left -&gt; largest // 2) 同时有左孩子和右孩子，右孩子的值&lt;= 左孩子的值，left -&gt; largest // 3) 同时有左孩子和右孩子并且右孩子的值&gt; 左孩子的值， right -&gt; largest int largest = left + 1 &lt; heapSize &amp;&amp; arr[left + 1] &gt; arr[left] ? left + 1 : left; // 父和较大的孩子之间，谁的值大，把下标给largest largest = arr[largest] &gt; arr[index] ? largest : index; if (largest == index) &#123; break; &#125; swap(arr, largest, index); index = largest; left = (index &lt;&lt; 1) + 1; &#125;&#125;private static void heapInsert(int[] arr, int index) &#123; while (arr[index] &gt; arr[(index - 1) / 2]) &#123; swap(arr, index, (index - 1) / 2); index = (index - 1) / 2; &#125;&#125; 基数排序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 基数排序 * @param arr */public static void radixSort(int[] arr) &#123; if (arr == null || arr.length &lt; 2) &#123; return; &#125; radixSort(arr, 0, arr.length - 1, maxbits(arr));&#125;private static int maxbits(int[] arr) &#123; int max = Integer.MIN_VALUE; for (int i = 0; i &lt; arr.length; i++) &#123; max = Math.max(max, arr[i]); &#125; int res = 0; while (max != 0) &#123; res++; max /= 10; &#125; return res;&#125;private static void radixSort(int[] arr, int L, int R, int digit) &#123; final int radix = 10; int i = 0, j = 0; int[] help = new int[R - L + 1]; for (int d = 1; d &lt;= digit; d++) &#123; int[] count = new int[radix]; // count[0..9] for (i = L; i &lt;= R; i++) &#123; j = getDigit(arr[i], d); count[j]++; &#125; for (i = 1; i &lt; radix; i++) &#123; count[i] = count[i] + count[i - 1]; &#125; for (i = R; i &gt;= L; i--) &#123; j = getDigit(arr[i], d); help[count[j] - 1] = arr[i]; count[j]--; &#125; for (i = L, j = 0; i &lt;= R; i++, j++) &#123; arr[i] = help[j]; &#125; &#125;&#125;public static int getDigit(int x, int d) &#123; return ((x / ((int) Math.pow(10, d - 1))) % 10);&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://solitaire-12.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://solitaire-12.github.io/tags/%E6%8E%92%E5%BA%8F/"}],"author":"织秋"},{"title":"Java|Java8","slug":"Java/Java8","date":"2024-03-18T12:14:55.000Z","updated":"2024-03-19T00:38:25.379Z","comments":true,"path":"2024/03/18/Java/Java8/","permalink":"https://solitaire-12.github.io/2024/03/18/Java/Java8/","excerpt":"","text":"java8实战汇总 Lambda表达式 Lambda表达式有三个部分，参数-&gt;主体 12(parameters) -&gt; expression(parameters) -&gt;&#123; statements; &#125; 123456789() -&gt; &#123;&#125; () -&gt; &quot;Raoul&quot;() -&gt; &#123;return &quot;Mario&quot;; &#125;// return是一个控制流语句。要使用此Lambda有效，需要使用花括号(Integer i) -&gt; return &quot;Alan&quot; + i; (Integer i) -&gt; &#123;return &quot;Alan&quot; + i;&#125;// “Iron Man”是一个表达式，不是一个语句。要使此Lambda有效，可以去除花括号和分号(String s) -&gt; &#123;&quot;Iron Man&quot;; &#125; (String s) -&gt; &quot;Iron Man&quot;; 演示代码 1 stream流的操作 ❏ 元素序列——就像集合一样，流也提供了一个接口，可以访问特定元素类型的一组有序值。因为集合是数据结构，所以它的主要目的是以特定的时间/空间复杂度存储和访问元素（如ArrayList与LinkedList）。但流的目的在于表达计算，比如你前面见到的filter、sorted和map。集合讲的是数据，流讲的是计算。后面几节会详细解释这个思想。 ❏ 源——流会使用一个提供数据的源，比如集合、数组或I/O资源。请注意，从有序集合生成流时会保留原有的顺序。由列表生成的流，其元素顺序与列表一致。 ❏ 数据处理操作——流的数据处理功能支持类似于数据库的操作，以及函数式编程语言中的常用操作，比如filter、map、reduce、find、match、sort等。流操作可以顺序执行，也可以并行执行。 流操作有两个重要的特点。 ❏ 流水线——很多流操作本身会返回一个流，这样多个操作就可以链接起来，构成一个更大的流水线。这使得一些优化成为可能，比如处理延迟和短路。流水线的操作可以看作类似对数据源进行数据库查询。 ❏ 内部迭代——与集合使用迭代器进行显式迭代不同，流的迭代操作是在后台进行的。 示例代码 12345678910List&lt;Dish&gt; menu = Arrays.asList( new Dish(&quot;pork&quot;, false, 800, Dish.Type.MEAT), new Dish(&quot;beef&quot;, false, 700, Dish.Type.MEAT), new Dish(&quot;chicken&quot;, false, 400, Dish.Type.MEAT), new Dish(&quot;french fries&quot;, true, 530, Dish.Type.OTHER), new Dish(&quot;rice&quot;, true, 350, Dish.Type.OTHER), new Dish(&quot;season fruit&quot;, true, 120, Dish.Type.OTHER), new Dish(&quot;pizza&quot;, true, 550, Dish.Type.OTHER), new Dish(&quot;prawns&quot;, false, 300, Dish.Type.FISH), new Dish(&quot;salmon&quot;, false, 450, Dish.Type.FISH) ); 1234567891011121314151617181920212223242526272829public class Dish &#123; private final String name; private final boolean vegetarian; private final int calories; private final Type type; public Dish(String name, boolean vegetarian, int calories, Type type) &#123; this.name = name; this.vegetarian = vegetarian; this.calories = calories; this.type = type; &#125; public String getName() &#123; return name; &#125; public boolean isVegetarian() &#123; return vegetarian; &#125; public int getCalories() &#123; return calories; &#125; public Type getType() &#123; return type; &#125; @Override public String toString() &#123; return name; &#125; public enum Type &#123; MEAT, FISH, OTHER &#125;&#125; 初次使用stream 1234567// toList()是一个静态方法，需要 import static java.util.stream.Collectors.toList;// 要是不想导包，可以把toList() 改成 Collectors.toList()List&lt;String&gt; collect = menu.stream().filter(dish -&gt; dish.getCalories() &gt; 300) .map(Dish::getName) .limit(3) .collect(toList());System.out.println(collect); 上述中Dish::getName相当于Lambda d -&gt; d.getName() 筛选（filter）、提取（map）或截断（limit）功能 流的特性 只能遍历一遍 123Stream&lt;Dish&gt; stream = menu.stream();stream.forEach(System.out::println);stream.forEach(System.out::println); 内部迭代 1234567891011121314151617// 用for-each循环外部迭代ArrayList&lt;String&gt; names = new ArrayList&lt;&gt;();for (Dish dish : menu) &#123; names.add(dish.getName());&#125;/** * for-each结构是一个语法糖，其背后逻辑是一个迭代器，Iterator */ArrayList&lt;String&gt; names = new ArrayList&lt;&gt;();Iterator&lt;Dish&gt; iterator = menu.iterator();while (iterator.hasNext())&#123; names.add(iterator.next().getName());&#125;// stream的内部迭代List&lt;String&gt; names = menu.stream().map(Dish::getName).collect(toList());System.out.println(names); 重构测试 12345678List&lt;String&gt; highCaloricDishes = new ArrayList&lt;&gt;();Iterator&lt;String&gt; iterator = menu.iterator();while(iterator.hasNext()) &#123; Dish dish = iterator.next(); if(dish.getCalories() &gt; 300) &#123; highCaloricDishes.add(d.getName()); &#125;&#125; 1234List&lt;String&gt; highCaloricDishes = menu.stream() .filter(dish -&gt; dish.getCalories() &gt; 300) .collect(toList()); 流的操作 从上述代码中,不难看出，stream()之后跟着filter，map，limit，collect等操作，这是java.util.stream.Stream中的Stream接口定义的操作，它们可以分成两类。 ❏ 中间操作：filter、map和limit可以连成一条流水线； ❏终端操作： collect触发流水线执行并关闭它。 总结： 一个数据源 一个中间操作链，形成流水线 一个终端操作，执行流水线，并生成结果 中间操作 类型 操作 返回类型 操作参数 函数描述 作用 filter Stream&lt;T&gt; Predicate&lt;T&gt; T -&gt; boolean 筛选 map Stream&lt;R&gt; Function&lt;T,R&gt; T -&gt; R 获取 有状态-无界 limit Stream&lt;T&gt; 截选 有状态-无界 skip Stream&lt;T&gt; 跳过 有状态-无界 sorted Stream&lt;T&gt; Comparator&lt;T&gt; (T,T) -&gt; int 有状态-无界 distinct Stream&lt;T&gt; 去重 终端操作 类型 操作 返回类型 作用 forEach void 消费流中的每个元素并对其应用Lambda count long 返回流中元素的个数 collect (generic) 把流整合成一个集合，比如List，Map，甚至是Iterator anyMatch boolean noneMatch boolean allMatch boolean findAny Optinal&lt;T&gt; findFirst Optinal&lt;T&gt; 有状态-有界 reduce Optinal&lt;T&gt; 使用流 筛选 谓词筛选 12List&lt;Dish&gt; collect = menu.stream().filter(Dish::isVegetarian).collect(toList());System.out.println(collect); 筛选各异的元素 流还支持一个叫作distinct的方法，它会返回一个元素各异（根据流所生成元素的hashCode和equals方法实现）的流。例如，以下代码会筛选出列表中所有的偶数，并确保没有重复（使用equals方法进行比较) 12345List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 1, 3, 3, 2, 4);numbers.stream() .filter(i -&gt; i % 2 == 0) .distinct() .forEach(System.out::println); 切片 谓词切片 takeWhile和dropWhile，这两个是java9引入的新方法。 引入原因：倘若数据源已经按照热量从小到大排序了，还要筛选出热量小于320的数据，filter筛选会去遍历所有数据，这样比较消耗性能。所以引入了takeWhile和dropWhile。 takeWhile:通过谓词去选取出符合的数据，遭遇到第一个不符合的元素时，停止处理 dropWhile:和taskWhile是相反的操作，它会删除掉符合要求的数据 截短流 流支持limit(n)方法，该方法会返回另一个不超过给定长度的流。所需的长度作为参数传递给limit。如果流是有序的，则最多会返回前n个元素。 跳过元素 流还支持skip(n)方法，返回一个扔掉了前n个元素的流。如果流中元素不足n个，则返回一个空流。请注意，limit(n)和skip(n)是互补的 映射 流支持map方法，它会接受一个函数作为参数。这个函数会被应用到每个元素上，并将其映射成一个新的元素（使用映射一词，是因为它和转换类似，但其中的细微差别在于它是“创建一个新版本”而不是去“修改”） 流的扁平化 案例 对于一张单词表，如何返回一张列表，列出里面各不相同的字符? 例如，给定单词列表[“Hello”,“World”]，要返回列表[“H”, “e”, “l”, “o”, “W”, “r”, “d”] 尝试1：不行 1234567List&lt;String&gt; words = Arrays.asList(&quot;hello&quot;, &quot;world&quot;);List&lt;String[]&gt; collect = words.stream() .map(word -&gt; word.split(&quot;&quot;)) .distinct() .collect(toList());System.out.println(&quot;words = &quot; + words); 尝试2：不行 12345678List&lt;String&gt; words = Arrays.asList(&quot;hello&quot;, &quot;world&quot;);List&lt;Stream&lt;String&gt;&gt; collect = words.stream() .map(word -&gt; word.split(&quot;&quot;)) .map(Arrays::stream) .distinct() .collect(toList());System.out.println(&quot;words = &quot; + words); 解释Arrays::stream： 1234String[] arrayOfWords = &#123;&quot;Goodbye&quot;, &quot;World&quot;&#125;;List&lt;String&gt; streamOfwords = Arrays .stream(arrayOfWords) // 这是Stream&lt;String&gt;类型的 .collect(toList()); 尝试3：使用flatMap,成功 flatMap 12345678List&lt;String&gt; words = Arrays.asList(&quot;hello&quot;, &quot;world&quot;);List&lt;String&gt; collect = words.stream() .map(word -&gt; word.split(&quot;&quot;)) .flatMap(Arrays::stream) .distinct() .collect(toList());System.out.println(&quot;words = &quot; + words); 查找和匹配 另一个常见的数据处理套路是看看数据集中的某些元素是否匹配一个给定的属性。Stream API通过allMatch、anyMatch、noneMatch、findFirst和findAny方法提供了这样的工具。 anyMatch anyMatch方法可以回答“流中是否有一个元素能匹配给定的谓词” 比如↓，你可以用它来看看菜单里面是否有素食可选择 1234if(menu.stream().anyMatch(Dish::isVegetarian))&#123; System.out.println(&quot;The menu is (somewhat) vegetarian friendly! ! &quot;);&#125;//anyMatch方法返回一个boolean allMatch allMatch方法会看流中的元素是否都能匹配给定的谓词。 比如↓，你可以用它来看看菜品是否有利健康（即所有菜的热量都低于1000卡路里）： 1boolean isHealthy = menu.stream().allMatch(dish -&gt; dish.getCalories() &lt; 1000); noneMatch 它可以确保流中没有任何元素与给定的谓词匹配 比如↓，你可以用noneMatch重写前面的例子 1boolean isHealthy = menu.stream().noneMatch(dish -&gt; dish.getCalories() &gt;= 1000); findAny findAny方法将返回当前流中的任意元素。它可以与其他流操作结合使用 比如↓，你可能想找到一道素食菜肴。可以结合使用filter和findAny方法来实现这个查询 Optional 1Optional&lt;Dish&gt; dish = menu.stream().filter(Dish::isVegetarian).findAny(); findFirst 有些流由一个出现顺序（encounter order）来指定流中项目出现的逻辑顺序（比如由List或排序好的数据列生成的流）。对于这种流，你可能想要找到第一个元素。为此有一个findFirst方法，它的工作方式类似于findAny 123456List&lt;Integer&gt; someNumbers = Arrays.asList(1, 2, 3, 4, 5);Optional&lt;Integer&gt; firstSquareDivisibleByThree = someNumbers.stream() .map(n -&gt; n * n) .filter(n -&gt; n % 3 == 0) .findFirst(); // 9 归约-reduce 此类查询需要将流中所有元素反复结合起来，得到一个值，比如一个Integer。这样的查询可以被归类为归约操作（将流归约成一个值）。用函数式编程语言的术语来说，这称为折叠（fold），因为你可以将这个操作看成把一张长长的纸（你的流）反复折叠成一个小方块，而这就是折叠操作的结果 求和 在研究如何使用reduce方法之前，先来看看如何使用for-each循环来对数字列表中的元素求和 1234int sum = 0;for (int x : numbers) &#123; sum += x;&#125; 从上述代码看，for-each的求和过程并不复杂，那为什么还要引入reduce呢？ for-each代码复用率低，reduce对重复应用的模式做了抽象 12// 引入reduce求和int sum = numbers.stream().reduce(0,(a,b)-&gt;a+b); reduce接收的参数： 初始值，即对应for-each中的sum BinaryOperator&lt;T&gt;用来将两个元素结合起来产生一个新的值。上述代码中是一个lambda表达式 (a,b)-&gt;a+b 所以，当有累乘的业务时，我们可也写成： 1int product = numbers.stream().reduce(1,(a,b)-&gt;a*b); 引入java8静态方法sum进行求和 1int sum = numbers.stream().reduce(0,Integer::sum); 无初始值 reduce还有一个重载的变体，它不接受初始值，但是会返回一个Optional对象 1int sum = numbers.stream().reduce((a,b)-&gt;a+b); 最大值和最小值 12Optional&lt;Integer&gt; max = numbers.stream().reduce(Integer::max);Optional&lt;Integer&gt; min = numbers.stream().reduce(Integer::min); 归约方法的优势与并行化 从for-each循环求和来看，这种迭代求和方式一旦并行，问题时致命的。而reduce提供了更加安全，简便的实现方式。 123// use stream to sum all elements in parallel// 使用流对所有元素并行求和int sum = numbers.parallelStream().reduce(0,Integer::sum); 原始类型流特化 1int calories = menu.stream().map(Dish::getCalories).reduce(0, Integer::sum); 像上述代码，直接写成int calories = menu.stream().map(Dish::getCalories).sum();不是更好？但这却是不可能的，因为map方法生成的是Stream&lt;T&gt;类型的数据，没办法通过sum()方法来实现求和，不过Stream API提供了原始类型流特化，专门处理数值流的方法 IntStream、DoubleStream和LongStream，分别将流中的元素特化为int、long和double，从而避免了暗含的装箱成本 映射到数值流 将流转换为特化版本的常用方法是mapToInt、mapToDouble和mapToLong 这些方法和前面说的map方法的工作方式一样，只是它们返回的是一个特化流，而不是Stream IntStream还支持其他的方便方法，如max、min、average等 1234int sum = menu.stream() //Stream&lt;Dish&gt; .mapToInt(Dish::getCalories) // IntStream .sum(); // 如果IntStream是空的话，返回的是0 转换回对象流 同样，一旦有了数值流，你可能会想把它转换回非特化流。例如，IntStream上的操作只能产生原始整数：IntStream的map操作接受的Lambda必须接受int并返回int（一个IntUnaryOperator）。但是你可能想要生成另一类值，比如Dish 123IntStream intStream = menu.stream() .mapToInt(Dish::getCalories);Stream&lt;Integer&gt; stream = intStream.boxed(); 默认值OptionalInt 求和的那个例子很容易，因为它有一个默认值：0。但是，如果你要计算IntStream中的最大元素，就得换个法子了，因为0是错误的结果 Optional类，这是一个可以表示值存在或不存在的容器。Optional可以用Integer、String等参考类型来参数化。对于三种原始流特化，也分别有一个Optional原始类型特化版本：OptionalInt、OptionalDouble和OptionalLong 123456OptionalInt maxCalories = menu.stream() .mapToInt(Dish::getCalories) .max();System.out.println(&quot;maxCalories = &quot; + maxCalories);int max = maxCalories.orElse(1); 数值范围 123456789IntStream.range(1, 10) .filter(value -&gt; value % 2 == 0) .forEach(System.out::println); // ↑结果：2，4，6，8IntStream.rangeClosed(1, 10) .filter(value -&gt; value % 2 == 0) .forEach(System.out::println); // ↑结果：2，4，6，8，10 // 总结range范围[x,y),rangeClosed范围[x,y] 尝试：获取勾股数 123456789101112131415161718192021Stream&lt;int[]&gt; pythagoreanTriples = IntStream.rangeClosed(1, 100).boxed() .flatMap(a -&gt; IntStream.rangeClosed(a, 100) .filter(b -&gt; Math.sqrt(a*a + b*b) % 1 == 0) .mapToObj( b -&gt; new int[]&#123;a, b, (int)Math.sqrt(a * a + b * b)&#125;) );pythagoreanTriples.limit(5).forEach(t-&gt;System.out.println(t[0]+&quot;,&quot;+ t[1] + &quot;,&quot; + t[2]));Stream&lt;double[]&gt; pythagoreanTriples = IntStream.rangeClosed(1, 100).boxed() .flatMap(a -&gt; IntStream.rangeClosed(a, 100) .mapToObj( b -&gt; new double[]&#123;a, b, Math.sqrt(a * a + b * b)&#125;) ) .filter(t -&gt; t[2] % 1 == 0);pythagoreanTriples.limit(5).forEach(t-&gt;System.out.println(t[0]+&quot;,&quot;+ t[1] + &quot;,&quot; + t[2])); Optional简介 Optional&lt;T&gt;类（java.util.Optional）是一个容器类，代表一个值存在或不存在。 在上述代码中，findAny存在什么元素都没有找到的可能，所以java8引入了Optional&lt;T&gt;,用来避免返回null的问题 测试 测试1：filter 根据示例代码，结合filter使用流筛选出前两个荤菜 12345List&lt;Dish&gt; collect = menu.stream() .filter(dish -&gt; dish.getType().equals(Dish.Type.MEAT)) .limit(2) .collect(toList()); 测试2：map 根据示例代码，结合映射 (1)给定一个数字列表，如何返回一个由每个数的平方构成的列表呢？例如，给定[1, 2, 3, 4, 5]，应该返回[1, 4, 9, 16,25]。 (2) 给定两个数字列表，如何返回所有的数对呢？例如，给定列表[1, 2, 3]和列表[3, 4]，应该返回[(1, 3), (1, 4), (2, 3), (2,4), (3, 3), (3, 4)]。为简单起见，你可以用有两个元素的数组来代表数对 (3) 如何扩展前一个例子，只返回总和能被3整除的数对 12345// (1)给定一个数字列表，如何返回一个由每个数的平方构成的列表呢？例如，给定[1, 2, 3, 4, 5]，应该返回[1, 4, 9, 16,25]。List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4, 5);List&lt;Integer&gt; collect = numbers.stream().map(n -&gt; n * n) .collect(toList());System.out.println(&quot;collect = &quot; + collect); 123456789// (2)给定两个数字列表，如何返回所有的数对呢？例如，给定列表[1, 2, 3]和列表[3, 4]，应该返回[(1, 3), (1, 4), (2, 3), (2,4), (3, 3), (3, 4)]。为简单起见，你可以用有两个元素的数组来代表数对List&lt;Integer&gt; numbers1 = Arrays.asList(1, 2, 3);List&lt;Integer&gt; numbers2 = Arrays.asList(3, 4);List&lt;int[]&gt; pairs = numbers1.stream() .flatMap(i -&gt; numbers2.stream() .map(j -&gt; new int[]&#123;i, j&#125;) ) .collect(toList()); 1234567891011// (3)如何扩展前一个例子，只返回总和能被3整除的数对List&lt;Integer&gt; numbers1 = Arrays.asList(1, 2, 3);List&lt;Integer&gt; numbers2 = Arrays.asList(3, 4);List&lt;int[]&gt; pairs = numbers1.stream() .flatMap(i -&gt; numbers2.stream() .filter(j -&gt; (i + j) % 3 == 0) .map(j -&gt; new int[]&#123;i, j&#125;) ) .collect(toList()); 测试3：reduce 根据示例代码，结合reduce，数一数流中有多少个菜 1int count = menu.stream().map(d -&gt; 1).reduce(0, (a, b) -&gt; a + b); 等价于 1long count = menu.stream().count(); 综合测试 源代码 123456789101112Trader raoul = new Trader(&quot;Raoul&quot;, &quot;Cambridge&quot;);Trader mario = new Trader(&quot;Mario&quot;, &quot;Milan&quot;);Trader alan = new Trader(&quot;Alan&quot;, &quot;Cambridge&quot;);Trader brian = new Trader(&quot;Brian&quot;, &quot;Cambridge&quot;);List&lt;Transaction&gt; transactions = Arrays.asList( new Transaction(brian, 2011, 300), new Transaction(raoul, 2012, 1000), new Transaction(raoul, 2011, 400), new Transaction(mario, 2012, 710), new Transaction(mario, 2012, 700), new Transaction(alan, 2012, 950)); 1234567891011121314151617181920212223242526272829303132333435363738394041public class Trader&#123; private final String name; private final String city; public Trader(String n, String c)&#123; this.name = n; this.city = c; &#125; public String getName()&#123; return this.name; &#125; public String getCity()&#123; return this.city; &#125; public String toString()&#123; return &quot;Trader:&quot;+this.name + &quot; in &quot; + this.city; &#125;&#125;public class Transaction&#123; private final Trader trader; private final int year; private final int value; public Transaction(Trader trader, int year, int value)&#123; this.trader = trader; this.year = year; this.value = value; &#125; public Trader getTrader()&#123; return this.trader; &#125; public int getYear()&#123; return this.year; &#125; public int getValue()&#123; return this.value; &#125; public String toString()&#123; return &quot;&#123;&quot; + this.trader + &quot;, &quot; + &quot;year: &quot;+this.year+&quot;, &quot; + &quot;value:&quot; + this.value +&quot;&#125;&quot;; &#125;&#125; 问题： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// (1) 找出2011年发生的所有交易，并按交易额排序（从低到高）。 List&lt;Transaction&gt; tr2011 = transactions.stream() .filter(transaction -&gt; transaction.getYear() == 2011) .sorted(Comparator.comparing(Transaction::getValue)) .collect(Collectors.toList()); System.out.println(&quot;tr2011 = &quot; + tr2011);// (2) 交易员都在哪些不同的城市工作过？ List&lt;String&gt; cities = transactions.stream() .map(transaction -&gt; transaction.getTrader().getCity()) .distinct() .collect(Collectors.toList());// Set&lt;String&gt; cities = transactions.stream()// .map(transaction -&gt; transaction.getTrader().getCity())// .collect(Collectors.toSet());// System.out.println(&quot;cities = &quot; + cities); System.out.println(&quot;cities = &quot; + cities);// (3) 查找所有来自于剑桥的交易员，并按姓名排序。 List&lt;Trader&gt; cambridgeTrader = transactions.stream() .map(Transaction::getTrader) .filter(trader -&gt; trader.getCity().equals(&quot;Cambridge&quot;)) .distinct() .sorted(Comparator.comparing(Trader::getName)) .collect(Collectors.toList()); System.out.println(&quot;cambridgeTrader = &quot; + cambridgeTrader);// (4) 返回所有交易员的姓名字符串，按字母顺序排序。// String reduce = transactions.stream()// .map(transaction -&gt; transaction.getTrader().getName())// .distinct()// .sorted()// .reduce(&quot;&quot;, (n1, n2) -&gt; n1 + n2); String reduce = transactions.stream() .map(transaction -&gt; transaction.getTrader().getName()) .distinct() .sorted() .collect(Collectors.joining()); System.out.println(&quot;reduce = &quot; + reduce);// (5) 有没有交易员是在米兰工作的？ boolean anyOneInMilan = transactions.stream() .anyMatch(transaction -&gt; transaction.getTrader().getCity().equals(&quot;Milan&quot;)); System.out.println(&quot;anyOneInMilan = &quot; + anyOneInMilan);// (6) 打印生活在剑桥的交易员的所有交易额。 transactions.stream() .filter(transaction -&gt; transaction.getTrader().getCity().equals(&quot;Cambridge&quot;)) .map(Transaction::getValue) .forEach(System.out::println);// (7) 所有交易中，最高的交易额是多少？ Optional&lt;Integer&gt; max = transactions.stream() .map(Transaction::getValue) .reduce(Integer::max); Optional&lt;Transaction&gt; max1 = transactions.stream().max(Comparator.comparing(Transaction::getValue)); System.out.println(&quot;max = &quot; + max); System.out.println(&quot;max1 = &quot; + max1);// (8) 找到交易额最小的交易。 Optional&lt;Integer&gt; min = transactions.stream() .map(Transaction::getValue) .reduce(Integer::min); Optional&lt;Transaction&gt; min1 = transactions.stream().min(Comparator.comparing(Transaction::getValue)); System.out.println(&quot;min = &quot; + min); System.out.println(&quot;min1 = &quot; + min1); 测试树形数据 1234567891011121314151617181920212223242526272829303132333435public static void main(String[] args) &#123; ArrayList&lt;Zz&gt; source = new ArrayList&lt;&gt;(); for (int i = 1; i &lt; 10; i++) &#123; Zz z1 = new Zz(); z1.setId(Long.valueOf(i)); int i1 = i % 3 == 2 ? i - 1 : 0; z1.setPid(Long.valueOf(i1)); source.add(z1); &#125; System.out.println(source); List&lt;Zz&gt; tree = source.stream() .filter(item -&gt; item.getPid().equals(0)) .map(item -&gt; &#123; List&lt;Zz&gt; children = new ArrayList&lt;&gt;(); item.setChildren(children); getChildren(children, item.getId(), source); return item; &#125;) .collect(Collectors.toList()); System.out.println(tree);&#125;private static void getChildren(List&lt;Zz&gt; children, Long id, ArrayList&lt;Zz&gt; source) &#123; source.stream() .filter(item -&gt; item.getPid().equals(id)) .map(item -&gt; &#123; children.add(item); List&lt;Zz&gt; child = new ArrayList&lt;&gt;(); item.setChildren(child); getChildren(children, item.getId(), source); return item; &#125;) .collect(Collectors.toList());&#125; 函数式编程Function 待续","categories":[{"name":"Java","slug":"Java","permalink":"https://solitaire-12.github.io/categories/Java/"}],"tags":[{"name":"Java8","slug":"Java8","permalink":"https://solitaire-12.github.io/tags/Java8/"}]},{"title":"Java|JVM","slug":"Java/JVM","date":"2024-03-18T12:10:55.000Z","updated":"2024-03-19T00:38:37.182Z","comments":true,"path":"2024/03/18/Java/JVM/","permalink":"https://solitaire-12.github.io/2024/03/18/Java/JVM/","excerpt":"","text":"类加载 加载过程 将java文件编译成class字节码文件，通过类加载器写入到JVM虚拟机运行时数据区中。 类加载流程 ​ 类加载需要经历五个阶段，分别是加载，验证，准备，解析，初始化。其中加载，验证，准备，初始化这四个阶段发生的顺序时确定的，而解析阶段时不一定的。它在某些情况下可以在初始化阶段之后开始，这是为了支持java的运行时绑定（也称做动态绑定）。另外，这些阶段都是按顺序开始，而非按顺序进行或者完成。 类加载器 双亲委派机制： ​ 当某个类加载器需要加载某个class文件时，它首先会把这个任务委派给它的父级加载器，递归到顶级加载器。随后进行加载，倘若加载失败，由子类加载器进行加载。可以保证同一个class只被加载一次。 运行时数据区 绿色线程共享，蓝色线程私有 JDK6：有永久代，静态变量存放在永久代上 JDK7：有永久代，但是已经把运行时常量池，常量池存放在方法区中了，逐渐的减少永久代的使用。 JDK8：无永久代，运行是常量池、类常量池都保存在元数据区，也就是元空间 。但是字符串常量池任然放在堆上。需要注意的是：元空间占用的是本地内存。 内存模型 程序计数器 线程私有。Java虚拟机执行多线程时，由于CPU时间片限制，线程执行具有随机性，操作系统会一直切换执行不同指令，我们需要把切换时候线程执行的位置存入到PC寄存器中，等切回来的时候能够回到原来的位置继续执行。 任何时候，每个Java虚拟机都在执行单个方法的代码，即该线程的当前方法。如果该方法不是Native方法，即PC寄存器会记录当前正在执行的java虚拟机指令的地址，如果线程当前执行的方法是本地的，那么java虚拟机的PC寄存器的值就是Undefined。 程序计数器是唯一一个不会出现OOM的区域。 本地方法栈 线程私有。提供虚拟机使用到本地Native方法，本地方法栈和虚拟机栈在有的虚拟机是合在一起的，例如Hot Spot虚拟机。 栈 线程私有，每个线程都有一个与线程同时创建的私有的虚拟机栈。虚拟机栈中存储栈帧，每个被线程调用的方法都会产生一个栈帧，栈帧中保存一个方法的状态信息，例如局部变量(存储方法内的局部变量)、操作数帧(用来计算临时存放变量)、方法出口(记录哪个方法的本方法)、动态链接(存放方法的元信息)等。调用一个方法就是执行一个栈帧的过程，一个方法调用完成，对应的栈帧就会出栈。 如果线程执行所需栈深度大于Java虚拟机栈深度，就会抛出StackOverFlowError，其实方法调用的过程就是入栈和出栈的过程，如果一致入栈不出栈，就容易发生异常（递归调用） 如果Java虚拟机栈可以动态扩展，但是扩展大小的时候无法申请到足够的内存，则会抛出OutOfMemoryError。 堆 线程共享。堆是java虚拟机管理内存最大的一块，在虚拟机启动时创建，所有线程共享，堆中的对象永远不会被显式释放，必须由GC回收，所以GC也主要回收堆中的对象实例，我们平常讨论的垃圾回收就是回收堆内存。堆可以处于物理上不连续的空间，可以固定大小，也可以动态扩展，通过参数-Xms和-Xmx两个参数控制堆的最小值和最大值。这里是**OOM异常的易发区** 分成新生代和老年代。 新生代又分成1个Eden区和2个Survivor区，默认比例8:1:1 绝大多数对象在Eden区生成，**当Eden区填满，或者不满足新建一个对象时，触发Young GC。**将存活的对象放进Survivor。Survivor分成两块，用于每次Young GC时，将存活的对象统统复制进空的幸存区，将有数据的那块清空。 当一个对象在两个survivor区之间交换了一定次数之后，将对象拷贝到老年代。这个数量用-XX:MaxTenuringThreshold配置，默认值是15 方法区 线程共享，在虚拟机启动时创建，存储已经被虚拟机加载的类信息、常量、静态变量，即经编译器之后的数据(运行时常量池[存放class文件元信息描述，编译后的代码数据，引用类型数据，类文件常量池]、属性和方法数据)，以及方法和构造函数的代码，包括在类和实例初始化以及接口初始化使用的特殊方法，方法区在逻辑上是堆的一部分，但是它又有另一个别名叫非堆，目的是与堆区分开。方法区可以是固定大小，也可以根据计算需要进行扩展。如果方法区的内存无法满足分配请求时也会抛出OutOfMemoryError。 永久代 元空间 JDK8使用元空间代替永久代（类元信息，字段，静态属性，方法，常量），同时运行时常量池等都移动到了元空间，字符串常量池在堆中。 元空间在本地内存中，它独立于运行时数据区，也不是Java虚拟机规范中定义的内存区域，它直接从操作系统中分配，因此不受堆大小影响。但是会受到本机总内存大小以及处理器寻址空间的限制，因此它也可能抛出OOM异常。 垃圾回收 找到垃圾 ​ 两种方法判断对象是否需要销毁： 引用计数法 ​ 为对象添加一个引用计数器，每当对象被引用，引用计数器加1。当引用失效，减一。计数器为零，代表该对象可以被回收。 ​ **问题：**循环引用问题 可达性分析 ​ 通过一系列&quot;GC Roots&quot;的根节点开始，沿用引用链进行搜索，凡是在引用链上的，都不会被回收。 ​ 解决了循环依赖问题。 垃圾回收算法-GC 标记-清除算法 首先在堆内存扫描一遍，然后把灰色区域的对象标记 继续扫描，扫描的同时将被标记的对象统一回收回收后： 回收之后，产生大量不连续的内存碎片。 需要进行标记，清除两个过程，效率不高。 复制算法 ​ 将内存区域分成大小一样的两块，每次只是用一块，gc之后将对象复制到另一块，然后一次性清理掉这块内存。 回收前： 回收后： 复制算法的缺点是牺牲了一半的内存空间，有点浪费。复制算法在JVM中的体现就是：java堆内存做了几次划分，Hot Spot虚拟机中Eden和Survivor的比例是Eden:S0:S1=8:1:1，将Survivor分成了两个区域S0和S1来进行赋值，这种做法是为了弥补原始复制算法直接将一半空间作为空闲空间的浪费。IBM公司表示：Young区有98%的对象都是朝生夕死的，生命周期极短，所以说一次GC下来存活的对象很少，所以没必要用一半空间来复制。 标记-整理算法 ​ 标记-整理算法就是为了老年代而设计的算法，标记-整理算法和标记-清除算法的区别在最后一步，标记-整理不会直接对对象清理，而是进行移动，将存活对象移动到一端，然后清理掉边界以外的对象。 回收前： 回收后： 分代收集算法 ​ 目前主流的商业虚拟机都是采用分代收集算法，这种算法就是上面三种算法的结合。新生代采用复制算法，老年代采用标记-整理或标记-清除算法。 垃圾收集器 Serial 和Serial Old 收集器 Serial 单线程，GC时Stop The World（STW），暂停所有用户线程。 Serial Old 单线程，作用与老年代。标记整理算法 ​ 优点：单线程收集效率高 ​ 缺点：需要STW，暂停所有用户线程 ​ 算法： Serial采用复制算法 ；Serial Old采用标记-整理算法 ParNew收集器 ​ ParNew收集器是Serial收集器的多线程版本，实现并行收集， 原理跟Serial一致（并行指的是多个GC线程并行，但是用户线程还是暂停，并发指的是用户线程和GC线程同时执行）。ParNew默认开启和CPU个数相同的线程数进行回收。 ​ 优点：在多CPU时，比Serial的效率高。 ​ 缺点：还是需要STW，单CPU时比Serial效率低 ​ 算法：复制算法 Parallel Scavenge 收集器 新生代收集器，也是复制算法，和ParNew一样并行的多线程收集器，更关注系统的吞吐量[吞吐量=(运行用户代码的时间) / (运行用户代码的时间+ GC 时间) ]Parallel Scavenge 提供了两个参数用于精确控制吞吐量： 123456789-XX:MaxGCPauseMillis //GC最大停顿毫秒数，必须大于0-XX:GCTimeRation //设置吞吐量大小，大于0小于100，默认值为99-XX:+UseAdaptiveSizePolicy //开启自适应策略//你会不会觉得把MaxGCPauseMillis设置小点就会让GC速度变快？//答案是否定的，如果设置时间过小，Parallel Scavenge 会牺牲吞吐量和新生代空间来交换，//比如新生代400Mb需要GC时间为100ms，设置成50ms了，那么就会把新生代调小为200Mb，这样肯定时间就降下来了，//然而这种操作可能会降低吞吐量，原先10s触发一次GC，每次100ms，// 修改时间后变成5s触发一次GC，每次70ms，那么10ms触发两次GC的时间变成了140ms，吞吐量反而降低。 Parallel Old 收集器 是Parallel Scavenge 的老年代版本，使用标记-整理算法，因为Parallel Scavenge 无法和CMS搭配使用，所以只能和Serial Old。自从Parallel Old出现，就有了Parallel Scavenge +Parallel Old的组合，这是JDK8使用的，注重吞吐量的一组收集器。 CMS收集器 这是优化GC停顿时间为目标的收集器，并发回收（仍然需要STW，但是时间很短）。通过-XX:+UseMarkSweepGC启用。CMS基于标记-清除算法实现。整个过程分为四步： 初始标记：需要STW，标记GC Roots对象。 并发标记：这个阶段可以和用户线程一起进行，分为三步： 根据第一步找到的GC Roots开始搜索跟GC Roots相连的对象。 预清理：处理并发标记之后发生变化的对象。 可被终止的预清理：有一个abort 触发条件，该阶段存在的目的是希望能发生一次Young GC，来减少Young区对象数量，降低重新标记的工作量，因为重新标记会扫描整个堆内空间，可以通过参数-XX:+CMSScavangeBeforeRemark 控制在重新标记前发生一次Young GC，默认为false。 重新标记：需要STW，这个阶段是为了修正在阶段2标记之后产生变化的对象。 并发清除：和用户线程同时进行，开始正式清理垃圾，此阶段产生的垃圾留待下次清除。 优点：并发收集，低停顿 缺点：产生大量碎片，并发阶段会降低吞吐量 G1收集器 ​ G1是以优化GC停顿时间为目标的收集器，它尝试以高概率满足GC停顿时间为目标，同时实现高吞吐量。在G1中，将堆的整个内存布局做了修改，在G1中整个堆划分为多个大小相等的独立区域Region，虽然在逻辑上还保留了新生代和老年代，但是物理上已经隔离了，G1的堆内存布局如下图 ： 上图智能柜被划分成一组大小相同的Region，每个Region都是连续的虚拟内存范围，G1可以知道哪个Region区域内大部分是空的，这样就可以在每次允许的收集时间内优先回收价值最大的Region区域（根据回收所获得的空间大小以及回收需要的时间综合考虑）所以这就是G1叫做Garbage-First的原因。G1是JDK8默认的垃圾收集器。G1的工作流程和CMS很相似，区别在最后的步骤。也有四步： 初始标记：需要STW，标记下GC Roots关联的对象，并且修改TAMS（Next Top at Mark Start）的值，使得下一阶段并发运行时，能在正确可用的Region中创建对象。 并发标记：和CMS一样，主要是进行GC Roots的向下搜索，找出存活对象进行标记。 最终标记：需要STW，和CMS一样，这个阶段是修正并发标记期间因用户程序运行而导致变动的对象。 筛选回收：对各个Region的回收价值和成本进行排序，根据用户期望的GC停顿时间制定回收计划。 G1的第一个重点是为运行需要大堆且GC延迟有限的应用程序的用户提供解决方案，这就意味着堆大小约为6G或更大，并且稳定且可预测的暂停时间低于0.5秒。如果应用程序具备以下特性，可以考虑切换到G1收集器： 超过50%的Java堆被实时数据占用 对象分配率或提升率差异很大 当前应用程序GC停顿时间超过0.5秒，又想缩短停顿时间 各收集对比 收集器 串行/并行/并发 新生代/老年代 算法 目标 适用场景 Serial 串行 新生代 复制 响应速度优先 单CPU环境下的Client模式 Serial Old 串行 老年代 标记-整理 响应速度优先 单CPU环境下的Client模式，CMS的后备预案 ParNew 并行 新生代 复制 响应速度优先 多CPU环境时在Server模式下与CMS配合使用 Parallel Scavenge 并行 新生代 复制 吞吐量优先 在后台运算且不需要太多交互的任务 Parallel 并行 老年代 标记-整理 吞吐量优先 在后台运算且不需要太多交互的任务 CMS 并发 老年代 标记-清除 响应速度优先 集中在互联网网站 或 B/S系统服务端中的Java应用 G1 并发 兼顾 标记-整理+复制 响应速度优先 面向服务端应用 串行收集器：Serial和Serial Old单线程收集，适用于内存较小的嵌入式设备。 并行收集器【吞吐量优先】：Parallel Scavenge + Parallel Old，适用于科学计算、后台处理等场景。 并发收集器【GC停顿时间优先】：CMS和G1，适用于对时间有要求的场景，例如Web应用。 调优 所谓调优就是设置一个合理的JVM参数，适配当前系统运行。 参数 可以分成三类：标准参数，-X参数，-XX参数 标准参数 ​ 以&quot;-&quot;开头的参数称为标准参数，是任何一个JDK版本都支持的，比较稳定，不会随版本更新和改变。例如-version,-help,-server。 -X参数 ​ 以-X开头的参数是在特定版本HotSpot支持的命令。JDK版本变化之后，参数可能变化，这个参数用的较少。 -XX参数 -XX是不稳定的参数，也是主要参数，分为Boolean类型和非Boolean类型。 Boolean类型 -XX:[+-]&lt;name&gt;：+或-表示启用或者禁用name属性 123456表示启用CMS垃圾收集器-XX:+UseConcMarkSweepGC表示启用G1垃圾收集器-XX:+UseG1GC表示打印出所有的JVM参数信息-XX:+PrintFlagsFinal 非Boolean类型 -XX&lt;name&gt;=&lt;value&gt;：name表示属性，value表示属性对应的值 12设置最大永久代空间大小为5M-XX:MaxMetaspaceSize=5M 其他 123-Xms1000 等价于 -XX:InitialHeapSize=1000-Xmx1000 等价于 -XX:MaxHeapSize=1000-Xss1000 等价于 -XX:ThreadStackSize=1000 常用参数 设置 说明 -XX:ClCompilerCount=3 最大并行编译数，大于1时可以提高编译速度，但会影响系统稳定性 -XX:InitialHeapSize=100m 初始堆大小，可以简写为-Xms100 -XX:MaxHeapSize 最大堆大小，可以简写为-Xmx100 -XX:NewSize=20m 设置年轻代大小 -XX:MaxNewSize 设置年轻代最大值 -XX:OldSize=50m 设置老年代大小 -XX:MetaspaceSize=50m 设置方法区大小，JDK8才有，用元空间代替方法区 -XX:+UseParallelGC 设置Parallel Scavenge作为新生代收集器系统默认会选择Parallel Old作为老年代收集器 -XX:NewRatio 新生代和老年代的比值比如 -XX:NewRatio=4表示新生代：老年代=1：4 -XX:SurvivorRatio 表示Survivor区和Eden区的比值比如-XX:SurvivorRatio=8表示（S0或S1):Eden=1:8 命令监控工具 待续 参考： Java SE Specifications (oracle.com) JVM万字总结 - 掘金 (juejin.cn) 【JVM系列8】JVM知识点总览 - 掘金 (juejin.cn) JVM8内存模型_jdk8内存模型_余生之君的博客-CSDN博客 JVM内存模型总结，有各版本JDK对比、有元空间OOM监控案例、有Java版虚拟机，综合实践学习！ - 小傅哥 - 博客园 (cnblogs.com) https://zhuanlan.zhihu.com/p/297001119","categories":[{"name":"Java","slug":"Java","permalink":"https://solitaire-12.github.io/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://solitaire-12.github.io/tags/JVM/"}]},{"title":"内网穿透","slug":"Server/内网穿透","date":"2024-03-18T09:05:07.000Z","updated":"2024-03-25T03:57:26.002Z","comments":true,"path":"2024/03/18/Server/内网穿透/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/","excerpt":"","text":"环境： 服务端阿里云： CentOS 8.5.2111 客户端：windows 11 / windows 10 frp 0.42 第一步 官网下载frp软件 笔者使用的版本：链接：https://pan.baidu.com/s/1cjmUiOY8iRIywx6whkhRdg 提取码：gatr 第二步 服务端： 将 frp_0.42.0_linux_amd64.tar.gz压缩包传给linux 进入到文件存放目录 123tar -zxvf frp_0.42.0_linux_amd64.tar.gzcd frp_0.42.0_linux_amd64/vi frps.ini 💬提醒：只要bind_port的7000端口没被占用，一般不建议修改。 12./frps -c frps.ini #临时启动nohup ./frps -c frps.ini &amp; #永久启动 客户端： 解压 frp_0.42.0_windows_amd64.zip ，注意关闭一下杀毒软件。 打开之后，修改frpc.ini文件。 💬解释： 12[ssh]定义一个使用 [type] 协议的url: [local_ip]:[local_port]被[server_addr]:[remote_port]代理了。即：用户访问 [server_addr]:[remote_port] 后，服务器&quot;跳转&quot;到 [local_ip]:[local_port] 启动 12# 进入cmdfrpc.exe -c frpc.ini ⚠️注意：假如你将frpc.ini中的remote_port和frps.ini中的dashboard_port配置成一样的话，启动客户端的时候，就会出现 [ssh] start error: port unavailable。","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"内网穿透","slug":"内网穿透","permalink":"https://solitaire-12.github.io/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"}]},{"title":"linux|项目部署","slug":"Server/项目上线","date":"2024-03-18T08:25:07.000Z","updated":"2024-03-25T03:57:29.375Z","comments":true,"path":"2024/03/18/Server/项目上线/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF/","excerpt":"","text":"环境 系统：Centos 8.4.2105 docker : 24.0.5 nginx: nginx/1.25.1 后端 宿主机安装 后端达成jar包上传到服务器 ps -ef | grep java查看有没有在运行的java项目，要是端口冲突的话，直接kill java -jar没有问题之后，nohup java -jar x.jar &gt; x.log 2&gt;&amp;1 &amp; 就可以。 docker内部 创建Dockerfile vi Dockerfile 1234FROM openjdk:8ADD 18080.jar test.jarEXPOSE 18080ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;test.jar&quot;] 描述： FROM openjdk:8:拉取基础镜像 ADD 18080.jar test.jar：将18080.jar复制到镜像中，重命名成test.jar EXPOSE 18080:声明端口，一般指定的是jar运行端口 ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;test.jar&quot;]：容器启动之后执行的命令 MAINTAINER xx: 添加作者信息 构建镜像 1docker build -t test . 描述： test：镜像名称 .：表示Dockerfile在当前目录下 查看镜像 1docker images 运行进行 1docker run -d --restart=always --name test -p 8080:18080 test 描述： -d：表示后台运行容器 --restart=always：代表容器在停止或者服务器开机之后会自动重启 --name test:命名成test -p 8080:18080：端口映射 [宿主机端口]:[容器端口] test:运行的镜像 更新jar包 1docker cp new.jar [容器id]:/test.jar 随后，重启容器 Docker部署可执行jar包-阿里云开发者社区 (aliyun.com) 前端 项目打包 打包之后将项目放在挂载的html目录下,详情查看docker安装nginx | 线上跳转↗️ 挂载项目： 将项目挂载到文件夹a里边 配置nginx 修改前的配置文件： 修改后的配置文件： 注意事项 要是jar运行在宿主机(安装dokcer的服务器)上面的话，建议server_name修改成172.17.0.1(这个地址是安装docker之后，默认生成的网桥)。用ifconfig -a可以查看Linux 的IP地址 不要在 localtion / &#123;&#125;这块里边取写 proxy_pass，会网页都打不开。 倘若有第二个前端项目的话，同样html目录下面创建挂载文件夹并把文件存放在内。然后在写一个localtion块，修改一下listen端口。例如： 问题排查： 1.运行jar包服务器卡死 环境： 系统：8.5.2111 配置：2核2G 目标服务： Minio，frp，docker，nginx，redis，mysql，前端，后端 场景： 服务器运行着Minio，frp，nginx，docker，redis-docker，mysql-docker，前端。将后端jar包上传服务器，nohup java -jar xx &amp;连接服务器的xshell敲命令执行卡顿，无法重连xshell，随后服务器卡死。 问题排查： 观察到xshell命令还是能敲上的，但是键盘敲击之后，xshell界面很久才能回显，执行也要很久很久，而且重连xshell也是很长时间连接不上。 类似这种反应时间很长，系统出现&quot;死机&quot;的现象，多半是内存不足。 为啥会内存不足呢？ 重启服务器之后，重新配置好需要的环境，不运行jar包。查看top命令： 观察其中的进程，注意到有个mysqld占用了22.4%的存，整台服务器的内存是2G，实际运行起来指定没有2048M的，以及其他一些系统支持的，算亏损10%，空闲内存实际约为1850M。刨去mysqld默认占用的1G，实际占用的450M。要想维系整个系统的开支，可用内存也就1G左右。 将jar在开发环境上运行，观察到其占用了开发电脑1000-1300M的内存资源。 解决方案也就呼之欲出了： 配置java -jarJVM的参数，限定其内存大小。例如： 1java -Xmx512M -jar xxx.jar 从mysqld上解决，参考1 参考2 它是Performance Schema 的大小，默认配置是1G。 此时基本解决内存不足问题。 2.jar包运行，CPU飙升至190% 环境： 系统：8.5.2111 配置：2核2G 目标服务器任务进程： Minio，frp，docker，nginx，redis，mysql，前端，后端 场景： 服务器运行着Minio，frp，nginx，docker，redis-docker，mysql-docker，前端。将后端jar包上传服务器，nohup java -jar xx &amp;后，top指令监测到cpu处在190%附近。 问题排查： 通过 命令，查看该进程下的线程资源占用情况。 1ps -mp pid -o THREAD,tid,time 将其中%CPU高的线程TID进行转码 1print &quot;%x\\n&quot; TID 通过查看cpu占用很高的线程状态 1jstack PID | grep [0xTID:第二部转码出来的tid] 如果线程还处在Runnable状态，那么通过jstack去找到详情。如果问题指向代码，那就去修改。 到此基本解决。 当然，还有个情况就是，正常启动直接拉爆，随后又静默到1%的，查线程高占用的都停止了，这种情况下，倘若后期还有MQ什么的其他服务要用的话，建议将升级到4核cpu。","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"部署","slug":"部署","permalink":"https://solitaire-12.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"linux|docker安装RabbitMQ","slug":"Server/docker安装RabbitMQ","date":"2024-03-18T08:23:07.000Z","updated":"2024-03-25T03:57:07.939Z","comments":true,"path":"2024/03/18/Server/docker安装RabbitMQ/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/docker%E5%AE%89%E8%A3%85RabbitMQ/","excerpt":"","text":"工欲善其事必先利其器：windows下载erlang 下载RabbitMQ 下载RabbitMQ源码 Docker安装RabbitMQ 12345678910111213# 查找rabbitmq镜像docker search rabbitmq# pull镜像docker pull rabbitmq# 运行mq ## -p [暴露的宿主机端口]:[docker内部端口]docker run -d --hostname rabbit-test --name rabbit \\-p 15672:15672 -p 5672:5672 rabbitmq# 进入mq镜像docker exec -it [容器id] /bin/bash# 运行rabbitmq-plugins enable rabbitmq_management# 默认账号密码：guest：guest","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://solitaire-12.github.io/tags/RabbitMQ/"}]},{"title":"linux|docker安装nginx","slug":"Server/docker安装nginx","date":"2024-03-18T08:23:07.000Z","updated":"2024-03-25T03:57:03.767Z","comments":true,"path":"2024/03/18/Server/docker安装nginx/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/docker%E5%AE%89%E8%A3%85nginx/","excerpt":"","text":"centos8.5.2111 拉取镜像 1docker pull nginx 创建Nginx挂载目录 123mkdir -p mytools/nginx/confmkdir -p mytools/nginx/logmkdir -p mytools/nginx/html 将容器中的配置复制到宿主机 12345678# 生成容器docker run --name nginx -p 9001:80 -d nginx# 将容器nginx.conf文件复制到宿主机docker cp nginx:/etc/nginx/nginx.conf /mytools/nginx/conf/nginx.conf# 将容器conf.d文件夹下内容复制到宿主机docker cp nginx:/etc/nginx/conf.d /mytools/nginx/conf/conf.d# 将容器中的html文件夹复制到宿主机docker cp nginx:/usr/share/nginx/html /mytools/nginx/ 正式启动nginx 12345678docker run \\-p 9002:80 \\--name nginx \\-v /mytools/nginx/conf/nginx.conf:/etc/nginx/nginx.conf \\-v /mytools/nginx/conf/conf.d:/etc/nginx/conf.d \\-v /mytools/nginx/log:/var/log/nginx \\-v /mytools/nginx/html:/usr/share/nginx/html \\-d nginx:latest 查看运行状态 1curl localhost:9002 公网IP访问 修改html查看运行状态 查看是否生效 1docker restart nginx","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://solitaire-12.github.io/tags/Nginx/"}]},{"title":"linux|docker安装redis","slug":"Server/docker安装redis","date":"2024-03-18T08:22:07.000Z","updated":"2024-03-25T03:57:11.363Z","comments":true,"path":"2024/03/18/Server/docker安装redis/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/docker%E5%AE%89%E8%A3%85redis/","excerpt":"","text":"centos 8.5.2111 拉取redis镜像 1docker pull redis 准备redis的配置文件 官网： redis 6.0.6 下载 – Redis中国用户组（CRUG） 把redis.conf上传到服务器redis安装的位置 配置redis.conf文件 bind 127.0.0.1 #注释掉这部分，使redis可以外部访问 daemonize no#用守护线程的方式启动 requirepass 你的密码#给redis设置密码 appendonly yes#redis持久化 默认是no tcp-keepalive 300 #防止出现远程主机强迫关闭了一个现有的连接的错误 默认是300 123456789101112#注释掉这部分，使redis可以外部访问bind 127.0.0.1 #用守护线程的方式启动daemonize no#给redis设置密码requirepass 你的密码#redis持久化 默认是noappendonly yes#防止出现远程主机强迫关闭了一个现有的连接的错误 默认是300tcp-keepalive 300 # :/[findword] 在vi命令中寻找文字，最好加个空格去找 创建本地与docker映射目录 12mkdir mytools/redismkdir mytools/redis/data 把上述的redis.conf复制到redis目录下 启动 1sudo docker run -p 6379:6379 --name redis -v /mytools/redis/redis.conf:/etc/redis/redis.conf -v /mytools/redis/data:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes -p 6379:6379:把容器内的6379端口映射到宿主机6379端口-v /data/redis/redis.conf:/etc/redis/redis.conf：把宿主机配置好的redis.conf放到容器内的这个位置中-v /data/redis/data:/data：把redis持久化的数据在宿主机内显示，做数据备份redis-server /etc/redis/redis.conf：这个是关键配置，让redis不是无配置启动，而是按照这个redis.conf的配置启动–appendonly yes：redis启动后数据持久化 查看日志：docker logs redis","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"}]},{"title":"linux|docker安装minio","slug":"Server/minio","date":"2024-03-18T08:22:07.000Z","updated":"2024-03-25T03:57:22.551Z","comments":true,"path":"2024/03/18/Server/minio/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/minio/","excerpt":"","text":"环境 centos 8.5 创建minio文件夹 12mkdir /mytools/miniocd /mytools/minio wget下载minio 1wget https://dl.minio.io/server/minio/release/linux-amd64/minio 创建日志，授权 12touch minio.logchmod 777 minio 启动minio 12# 后面跟着的是静态文件挂载目录./minio server /opt/minio/data **问题原因：**笔者编写文档之前，运行过minio，这里显示端口占用问题，kill一下进程就好了 红字：警告:检测到默认凭据’minioadmin:minioadmin’，我们建议您使用’MINIO_ROOT_USER’和’MINIO_ROOT_PASSWORD’环境变量更改这些值 配置环境变量 12345vim /etc/profile export MINIO_ROOT_USER=fileadmin export MINIO_ROOT_PASSWORD=fileadminsource /etc/profile 指定端口重启minio 12# 有需求自己换成自己的存放目录./minio server /opt/minio/data --console-address &quot;:9050&quot; 设置后台启动 12345vim start.shnohup /mytools/minio/minio server /opt/minio/data --console-address &quot;:9050&quot; &gt; /opt/minio/minio.log 2&gt;&amp;1 &amp;sh start.sh 问题： minio服务部署在云端时，配置服务网站要用公网ip，不然没办法回显","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"minio","slug":"minio","permalink":"https://solitaire-12.github.io/tags/minio/"}]},{"title":"linux|docker安装mysql8","slug":"Server/docker安装mysql8","date":"2024-03-18T08:21:07.000Z","updated":"2024-03-25T03:56:59.934Z","comments":true,"path":"2024/03/18/Server/docker安装mysql8/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/docker%E5%AE%89%E8%A3%85mysql8/","excerpt":"","text":"拉取镜像 1docker pull mysql:8.0.23 检查镜像 1docker images 启动镜像 1234567891011sudo docker run -p 3306:3306 --name mysql \\-v /usr/local/docker/mysql/mysql-files:/var/lib/mysql-files \\-v /usr/local/docker/mysql/conf:/etc/mysql \\-v /usr/local/docker/mysql/logs:/var/log/mysql \\-v /usr/local/docker/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=root \\-d mysql:8.0.23docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql 1docker logs -f mysql #查看启动日志 进入容器 1docker exec -it mysql bash 方法二 下载mysql 1docker pull mysql:8.0.23 创建文件夹做挂载 12345//创建文件夹做挂载mkdir -p /mydata/mysql/confmkdir /mydata/mysql/data//创建my.cnf配置文件vi /mydata/mysql/conf/my.cnf my.cnf添加如下内容 123456789101112131415[mysqld]user=mysqlcharacter-set-server=utf8default_authentication_plugin=mysql_native_passwordsecure_file_priv=/var/lib/mysqlexpire_logs_days=7sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTIONmax_connections=1000lower_case_table_names=1 [client]default-character-set=utf8 [mysql]default-character-set=utf8 注意： 你的数据库要是从本地windows上迁移到linux的话，lower_case_table_names=1,这个一定要加上，不然的话要重新部署，很麻烦 创建容器 12345678910docker run \\--restart=always \\--privileged=true \\-p 3306:3306 --name mysql \\-v /mydata/mysql/log:/var/log/mysql \\-v /mydata/mysql/data:/var/lib/mysql \\-v /mydata/mysql/conf/my.cnf:/etc/mysql/my.cnf \\-v /mydata/mysql/conf/conf.d:/etc/mysql/conf.d \\-e MYSQL_ROOT_PASSWORD=root \\-d mysql:8.0 进入容器 1docker exec -it mysql bash 进入mysql 123mysql -u root -p# 查看数据库show databases","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/tags/MySQL/"}]},{"title":"linux|安装Git/Maven","slug":"Server/GitMaven","date":"2024-03-18T08:20:07.000Z","updated":"2024-03-25T03:57:15.190Z","comments":true,"path":"2024/03/18/Server/GitMaven/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/GitMaven/","excerpt":"","text":"安装Git 1yum install -y git 校验安装结果 1git --version 安装maven 创建存放maven的文件夹 1mkdir -p /mytools/maven 将maven.gz上传到该文件夹 解压 1tar -xvf apache-maven-3.9.4-bin.tar.gz 配置环境变量 1234vim /etc/profile export MAVEN_HOME=/mydata/maven/apache-maven-3.9.4 export PATH=$&#123;MAVEN_HOME&#125;/bin:$PATHsource /etc/profile 验证 1mvn -v 配置阿里镜像(看情况)","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"环境","slug":"环境","permalink":"https://solitaire-12.github.io/tags/%E7%8E%AF%E5%A2%83/"}]},{"title":"linux|安装Docker","slug":"Server/docker","date":"2024-03-18T08:20:07.000Z","updated":"2024-03-25T03:56:56.334Z","comments":true,"path":"2024/03/18/Server/docker/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/docker/","excerpt":"","text":"环境 ： centos 8.4.2105 检查是否存在docker 1docker -v 卸载docker 12345678910yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 123yum remove docker-ce \\ docker-ce-cli \\ containerd 设置仓库 1sudo yum install -y yum-utils device-mapper-persistent-data lvm2 稳定仓库 12# 阿里云sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 1234# 清华镜像sudo yum-config-manager \\ --add-repo \\ https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 安装 Docker Engine-Community 1sudo yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin 问题： 原因：CentOS-8于2021年12月31日停止了源的服务 解决步骤： 1234567# 1cd /etc/yum.repos.d# 2mkdir bakcp * bak/# 3vi root_.repo #把enabled=1，改成enable=0 指定版本安装 123456789$ yum list docker-ce --showduplicates | sort -rdocker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io 启动docker 123sudo systemctl start docker# 运行hello-world镜像，查看是否安装成功sudo docker run hello-world 卸载docker 1234# 删除安装包yum remove docker-ce# 删除镜像、容器、配置文件等内容rm -rf /var/lib/docker centos 8.2安装问题 设置仓库时发生问题： 解决：更新一下yum 1yum -y update 出现Failed to download metadata for repo ‘appstream’: Cannot prepare internal mirrorlist: No URLs in mirrorlist 问题原因： 因为centos8项目官方已于2021年底停止维护，相关源已无法使用，所以网上22年前的换源教程都已无法使用。 解决： 12345678#进入配置文件目录cd /etc/yum.repos.d/#删除旧的配置文件rm -rf *.repo# 下载可用的.repo文件wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo# 运行 yum makecache 生成缓存yum makecache","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://solitaire-12.github.io/tags/Docker/"}]},{"title":"linux|安装JDK","slug":"Server/JDK","date":"2024-03-18T08:14:07.000Z","updated":"2024-03-25T03:57:18.985Z","comments":true,"path":"2024/03/18/Server/JDK/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/JDK/","excerpt":"","text":"清华云下载jdk ： https://repo.huaweicloud.com/java/ 12345678910111213141516171819202122# 查看是否安装过jdkrpm -qa | grep java# ↑这里要是存在内容，执行删除rpm -qa | grep java | xargs rpm -e --nodeps# 查看java版本java -version# 创建存放java的文件mkdir /usr/java# cd进入这一个文件夹# 把下载好的jdk上传到这个文件夹下# 解压tar -xvf jdk1.8.0_202ar.gz# 配置环境vi /etc/profile# 在文件末尾添加 【注意=附近不要有空格】export JAVA_HOME=/usr/java/jdk1.8.0_202export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib/ export PATH=$PATH:$JAVA_HOME/bin# 初始化配置 - 执行这条语句，可以让环境立即生效，不需要重启source /etc/profile# 查看版本 (有信息的话，就安装成功了)java -version 如果上述中没有添加到环境中，尝试终端直接运行以下代码 1234export JAVA_HOME=/mytools/java/jdk1.8.0_202export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"环境","slug":"环境","permalink":"https://solitaire-12.github.io/tags/%E7%8E%AF%E5%A2%83/"}]},{"title":"Hexo|图片相关","slug":"Hexo/Hexo-图片相关","date":"2024-03-18T07:14:55.000Z","updated":"2024-03-18T07:55:33.857Z","comments":true,"path":"2024/03/18/Hexo/Hexo-图片相关/","permalink":"https://solitaire-12.github.io/2024/03/18/Hexo/Hexo-%E5%9B%BE%E7%89%87%E7%9B%B8%E5%85%B3/","excerpt":"","text":"1.相对路径挂载图片【文章显示】 在 _config.yml文件中配置 post_asset_folder 1234post_asset_folder: truemarked: prependRoot: true postAsset: true 💬配置之后，当执行 hexo new titleName之后，会在该titleName.md文件夹的同级目录下生成一个同名文件夹用来存放图片资源。 .md文件中使用 1![](photoName.jpg) 💬hexo-renderer-marked默认渲染引擎，通过hexo g之后，会将该图片路径渲染成绝对路径。在文章列表中，无法加载图片。 2.绝对路径挂载图片【主页显示】 在source文件夹下，创建一个img文件夹用来存放图片 .md文件中使用 1![](/img/photoName.jpg) 💬/img代表根目录下的img文件夹，其中 /指代根目录【public文件夹】 3.同时显示 安装 hexo-asset-image 1npm hexo-asset-image --save 安装之后，发现图片路径都不显示了，F12查看图片路径变成了/.io//xxx.xx。 修复上述bug。进入到node_modules找到hexo-asset-image插件，用下述代码替换掉index.js原有代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&quot;use strict&quot;;var cheerio = require(&quot;cheerio&quot;);// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-stringfunction getPosition(str, m, i) &#123; return str.split(m, i).join(m).length;&#125;var version = String(hexo.version).split(&quot;.&quot;);hexo.extend.filter.register(&quot;after_post_render&quot;, function (data) &#123; var config = hexo.config; if (config.post_asset_folder) &#123; var link = data.permalink; if (version.length &gt; 0 &amp;&amp; Number(version[0]) == 3) var beginPos = getPosition(link, &quot;/&quot;, 1) + 1; else var beginPos = getPosition(link, &quot;/&quot;, 3) + 1; // In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;. var endPos = link.lastIndexOf(&quot;/&quot;) + 1; link = link.substring(beginPos, endPos); var toprocess = [&quot;excerpt&quot;, &quot;more&quot;, &quot;content&quot;]; for (var i = 0; i &lt; toprocess.length; i++) &#123; var key = toprocess[i]; var $ = cheerio.load(data[key], &#123; ignoreWhitespace: false, xmlMode: false, lowerCaseTags: false, decodeEntities: false, &#125;); $(&quot;img&quot;).each(function () &#123; if ($(this).attr(&quot;src&quot;)) &#123; // For windows style path, we replace &#x27;\\&#x27; to &#x27;/&#x27;. var src = $(this).attr(&quot;src&quot;).replace(&quot;\\\\&quot;, &quot;/&quot;); if (!/http[s]*.*|\\/\\/.*/.test(src) &amp;&amp; !/^\\s*\\//.test(src)) &#123; // For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed. // In addition, to support multi-level local directory. var linkArray = link.split(&quot;/&quot;).filter(function (elem) &#123; return elem != &quot;&quot;; &#125;); var srcArray = src.split(&quot;/&quot;).filter(function (elem) &#123; return elem != &quot;&quot; &amp;&amp; elem != &quot;.&quot;; &#125;); if (srcArray.length &gt; 1) srcArray.shift(); src = srcArray.join(&quot;/&quot;); $(this).attr(&quot;src&quot;, config.root + link + src); console.info &amp;&amp; console.info(&quot;update link as:--&gt;&quot; + config.root + link + src); &#125; &#125; else &#123; console.info &amp;&amp; console.info(&quot;no src attr, skipped...&quot;); console.info &amp;&amp; console.info($(this)); &#125; &#125;); data[key] = $.html(); &#125; &#125;&#125;); 替换之后，重新hexo g即可。 4.支持emoji hexo-renderer-marked默认渲染引擎并不支持emoji，却换到可以支持的渲染引擎。笔者使用的是 hexo-renderer-markdown-it。 12npm un hexo-renderer-marked --savenpm i hexo-renderer-markdown-it --save 在 _config.yml文件中配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243# hexo-renderer-markdown-itmarkdown: preset: &quot;default&quot; render: html: true xhtmlOut: false langPrefix: &quot;language-&quot; breaks: true linkify: true typographer: true quotes: &quot;“”‘’&quot; enable_rules: disable_rules: plugins: - markdown-it-abbr - markdown-it-cjk-breaks - markdown-it-deflist - markdown-it-emoji - markdown-it-footnote - markdown-it-ins - markdown-it-mark - markdown-it-sub - markdown-it-sup # - markdown-it-checkbox # - markdown-it-imsize # - markdown-it-expandable - name: markdown-it-container options: success - name: markdown-it-container options: tips - name: markdown-it-container options: warning - name: markdown-it-container options: danger anchors: level: 2 collisionSuffix: &quot;&quot; permalink: false permalinkClass: &quot;header-anchor&quot; permalinkSide: &quot;left&quot; permalinkSymbol: &quot;¶&quot; case: 0 separator: &quot;-&quot; 参考： Hexo图片相关 - 文章显示、主页显示、修改fancybox_hexo 文章缩略-CSDN博客 【Hexo】选择更高级的Markdown渲染器_hexo-renderer-marked-CSDN博客","categories":[{"name":"博客","slug":"博客","permalink":"https://solitaire-12.github.io/categories/%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://solitaire-12.github.io/tags/Hexo/"}]},{"title":"数据管理DMS","slug":"Server/dms","date":"2024-03-18T05:40:07.000Z","updated":"2024-03-25T03:56:51.512Z","comments":true,"path":"2024/03/18/Server/dms/","permalink":"https://solitaire-12.github.io/2024/03/18/Server/dms/","excerpt":"","text":"下载：数据管理DMS–下载中心 (aliyun.com) 本地部署： 安装好之后，登录实例即可。 问题：内网IP链接失败 解决方法：【本地数据库是MySQL】 登录数据库，切换到mysql 1select user,host from user; # 查看信息 ⚠️上述信息是指该用户名只能通过localhost这个地址进行访问。 修改root用户的host 1update user set host =&quot;%&quot; where user = &quot;root&quot;; 刷新权限 1flush privileges;","categories":[{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"tags":[{"name":"DMS","slug":"DMS","permalink":"https://solitaire-12.github.io/tags/DMS/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-03-14T05:00:15.235Z","updated":"2024-03-25T02:10:12.947Z","comments":true,"path":"2024/03/14/hello-world/","permalink":"https://solitaire-12.github.io/2024/03/14/hello-world/","excerpt":"","text":"$log_{}n$ $$ log_{}n $$ 😄 $$ \\frac{m}{2}\\leq k \\leq m $$ $log_{2}n$ Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"初始","slug":"初始","permalink":"https://solitaire-12.github.io/categories/%E5%88%9D%E5%A7%8B/"}],"tags":[]}],"categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://solitaire-12.github.io/categories/Nginx/"},{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/categories/MySQL/"},{"name":"Spring","slug":"Spring","permalink":"https://solitaire-12.github.io/categories/Spring/"},{"name":"Redis","slug":"Redis","permalink":"https://solitaire-12.github.io/categories/Redis/"},{"name":"算法","slug":"算法","permalink":"https://solitaire-12.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"Java","slug":"Java","permalink":"https://solitaire-12.github.io/categories/Java/"},{"name":"服务器","slug":"服务器","permalink":"https://solitaire-12.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"博客","slug":"博客","permalink":"https://solitaire-12.github.io/categories/%E5%8D%9A%E5%AE%A2/"},{"name":"初始","slug":"初始","permalink":"https://solitaire-12.github.io/categories/%E5%88%9D%E5%A7%8B/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://solitaire-12.github.io/tags/Nginx/"},{"name":"MySQL","slug":"MySQL","permalink":"https://solitaire-12.github.io/tags/MySQL/"},{"name":"spring","slug":"spring","permalink":"https://solitaire-12.github.io/tags/spring/"},{"name":"AOP","slug":"AOP","permalink":"https://solitaire-12.github.io/tags/AOP/"},{"name":"IOC","slug":"IOC","permalink":"https://solitaire-12.github.io/tags/IOC/"},{"name":"redis","slug":"redis","permalink":"https://solitaire-12.github.io/tags/redis/"},{"name":"高可用","slug":"高可用","permalink":"https://solitaire-12.github.io/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"持久化","slug":"持久化","permalink":"https://solitaire-12.github.io/tags/%E6%8C%81%E4%B9%85%E5%8C%96/"},{"name":"数据结构","slug":"数据结构","permalink":"https://solitaire-12.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Java基础","slug":"Java基础","permalink":"https://solitaire-12.github.io/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"排序","slug":"排序","permalink":"https://solitaire-12.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"Java8","slug":"Java8","permalink":"https://solitaire-12.github.io/tags/Java8/"},{"name":"JVM","slug":"JVM","permalink":"https://solitaire-12.github.io/tags/JVM/"},{"name":"内网穿透","slug":"内网穿透","permalink":"https://solitaire-12.github.io/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"name":"部署","slug":"部署","permalink":"https://solitaire-12.github.io/tags/%E9%83%A8%E7%BD%B2/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://solitaire-12.github.io/tags/RabbitMQ/"},{"name":"minio","slug":"minio","permalink":"https://solitaire-12.github.io/tags/minio/"},{"name":"环境","slug":"环境","permalink":"https://solitaire-12.github.io/tags/%E7%8E%AF%E5%A2%83/"},{"name":"Docker","slug":"Docker","permalink":"https://solitaire-12.github.io/tags/Docker/"},{"name":"Hexo","slug":"Hexo","permalink":"https://solitaire-12.github.io/tags/Hexo/"},{"name":"DMS","slug":"DMS","permalink":"https://solitaire-12.github.io/tags/DMS/"}]}